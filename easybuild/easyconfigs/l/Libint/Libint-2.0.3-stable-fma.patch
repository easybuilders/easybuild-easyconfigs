Remove some problematic (and not very useful) FMA header code that does not compile with GCC.
This is done with a patch, because this problem also affects anything that links against Libint.
This way, FMA compiler optimization can remain enabled.
Author: Toon Verstraelen (UGent, toon.verstraelen@ugent.be)
--- libint-2.0.3-stable.orig/include/vector_x86.h	2019-02-22 09:16:11.718287697 +0100
+++ libint-2.0.3-stable/include/vector_x86.h	2019-02-22 09:17:37.628678533 +0100
@@ -141,30 +141,6 @@
     return c;
   }
 
-#if defined(__FMA__)
-  inline VectorSSEDouble fma_plus(VectorSSEDouble a, VectorSSEDouble b, VectorSSEDouble c) {
-    VectorSSEDouble d;
-    d.d = _mm_fmadd_pd(a.d, b.d, c.d);
-    return d;
-  }
-  inline VectorSSEDouble fma_minus(VectorSSEDouble a, VectorSSEDouble b, VectorSSEDouble c) {
-    VectorSSEDouble d;
-    d.d = _mm_fmsub_pd(a.d, b.d, c.d);
-    return d;
-  }
-#elif defined(__FMA4__)
-  inline VectorSSEDouble fma_plus(VectorSSEDouble a, VectorSSEDouble b, VectorSSEDouble c) {
-    VectorSSEDouble d;
-    d.d = _mm_macc_pd(a.d, b.d, c.d);
-    return d;
-  }
-  inline VectorSSEDouble fma_minus(VectorSSEDouble a, VectorSSEDouble b, VectorSSEDouble c) {
-    VectorSSEDouble d;
-    d.d = _mm_msub_pd(a.d, b.d, c.d);
-    return d;
-  }
-#endif
-
   //@}
 
   //@{ standard functions
@@ -365,30 +341,6 @@
     return c;
   }
 
-#if defined(__FMA__)
-  inline VectorSSEFloat fma_plus(VectorSSEFloat a, VectorSSEFloat b, VectorSSEFloat c) {
-    VectorSSEFloat d;
-    d.d = _mm_fmadd_ps(a.d, b.d, c.d);
-    return d;
-  }
-  inline VectorSSEFloat fma_minus(VectorSSEFloat a, VectorSSEFloat b, VectorSSEFloat c) {
-    VectorSSEFloat d;
-    d.d = _mm_fmsub_ps(a.d, b.d, c.d);
-    return d;
-  }
-#elif defined(__FMA4__)
-  inline VectorSSEFloat fma_plus(VectorSSEFloat a, VectorSSEFloat b, VectorSSEFloat c) {
-    VectorSSEFloat d;
-    d.d = _mm_macc_ps(a.d, b.d, c.d);
-    return d;
-  }
-  inline VectorSSEFloat fma_minus(VectorSSEFloat a, VectorSSEFloat b, VectorSSEFloat c) {
-    VectorSSEFloat d;
-    d.d = _mm_msub_ps(a.d, b.d, c.d);
-    return d;
-  }
-#endif
-
   //@}
 
   //@{ standard functions
@@ -591,30 +543,6 @@
     return c;
   }
 
-#if defined(__FMA__)
-  inline VectorAVXDouble fma_plus(VectorAVXDouble a, VectorAVXDouble b, VectorAVXDouble c) {
-    VectorAVXDouble d;
-    d.d = _mm256_fmadd_pd(a.d, b.d, c.d);
-    return d;
-  }
-  inline VectorAVXDouble fma_minus(VectorAVXDouble a, VectorAVXDouble b, VectorAVXDouble c) {
-    VectorAVXDouble d;
-    d.d = _mm256_fmsub_pd(a.d, b.d, c.d);
-    return d;
-  }
-#elif defined(__FMA4__)
-  inline VectorAVXDouble fma_plus(VectorAVXDouble a, VectorAVXDouble b, VectorAVXDouble c) {
-    VectorAVXDouble d;
-    d.d = _mm256_facc_pd(a.d, b.d, c.d);
-    return d;
-  }
-  inline VectorAVXDouble fma_minus(VectorAVXDouble a, VectorAVXDouble b, VectorAVXDouble c) {
-    VectorAVXDouble d;
-    d.d = _mm256_fsub_pd(a.d, b.d, c.d);
-    return d;
-  }
-#endif
-
   //@}
 
   //@{ standard functions
