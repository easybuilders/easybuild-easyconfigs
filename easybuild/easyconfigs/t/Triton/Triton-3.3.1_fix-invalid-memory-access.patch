This fixes an error in Pytorch tests:
> $ python inductor/test_flex_attention.py TestFlexAttention.test_builtin_score_mods_different_block_size_float16_score_mod7_BLOCK_SIZE2
> RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


From b5eaa251164dc842bf80de2c2840f4af7ad80f33 Mon Sep 17 00:00:00 2001
From: Thomas Raoux <thomas.raoux@openai.com>
Date: Wed, 26 Mar 2025 05:55:30 -0700
Subject: [PATCH] Fix indeterminism bug in LLVM datalayout (#6303)

The data layout depends on the flag `nvptx-short-ptr` but this flag was
being set after we set the data layout. Since this is a global this was
only affecting the first kernel compiled.
---
 third_party/nvidia/backend/compiler.py |  3 ++-
 third_party/nvidia/triton_nvidia.cc    | 10 ++++++++++
 2 files changed, 12 insertions(+), 1 deletion(-)
 
diff --git a/third_party/nvidia/backend/compiler.py b/third_party/nvidia/backend/compiler.py
index 6db76a352..9ec48b8da 100644
--- a/third_party/nvidia/backend/compiler.py
+++ b/third_party/nvidia/backend/compiler.py
@@ -349,6 +349,7 @@ class CUDABackend(BaseBackend):
         proc = sm_arch_from_capability(capability)
         features = get_features(options, self.target.arch)
         triple = 'nvptx64-nvidia-cuda'
+        nvidia.set_short_ptr()
         llvm.attach_datalayout(llvm_mod, triple, proc, features)
         nvidia.set_nvvm_reflect_ftz(llvm_mod)
 
@@ -384,7 +385,7 @@ class CUDABackend(BaseBackend):
         triple = 'nvptx64-nvidia-cuda'
         proc = sm_arch_from_capability(capability)
         features = get_features(opt, self.target.arch)
-        ret = llvm.translate_to_asm(src, triple, proc, features, ['nvptx-short-ptr'], opt.enable_fp_fusion, False)
+        ret = llvm.translate_to_asm(src, triple, proc, features, [], opt.enable_fp_fusion, False)
         # Find kernel names (there should only be one)
         names = re.findall(r".visible .entry ([a-zA-Z_][a-zA-Z0-9_]*)", ret)
         assert len(names) == 1
diff --git a/third_party/nvidia/triton_nvidia.cc b/third_party/nvidia/triton_nvidia.cc
index 0b280f789..5773fdc66 100644
--- a/third_party/nvidia/triton_nvidia.cc
+++ b/third_party/nvidia/triton_nvidia.cc
@@ -77,6 +77,16 @@ void init_triton_nvidia(py::module &&m) {
     context.loadAllAvailableDialects();
   });
 
+  // Set short point option, this needs to be set before setting the data
+  // layout.
+  m.def("set_short_ptr", []() {
+    auto options = llvm::cl::getRegisteredOptions();
+    const char *flag = "nvptx-short-ptr";
+    auto *shortPtr = static_cast<llvm::cl::opt<bool> *>(options[flag]);
+    assert(shortPtr);
+    shortPtr->setValue(true);
+  });
+
   // TODO: could be done in python if we had a generic interface to set metadata
   m.def("set_nvvm_reflect_ftz", [](llvm::Module *mod) {
     // please check https://llvm.org/docs/NVPTXUsage.html#reflection-parameters
