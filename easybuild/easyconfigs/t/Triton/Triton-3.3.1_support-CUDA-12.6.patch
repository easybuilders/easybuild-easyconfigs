Triton 3.3.1 requires ptxas 12.4 and causes wrong results when using a newer one, e.g. our 12.6 version at least on H100 GPUs.
This manifests e.g. in PyTorch tests as:
> $ python inductor/test_flex_attention.py TestFlexAttention.test_builtin_score_mods_different_block_size_float16_score_mod7_BLOCK_SIZE2
> AssertionError: False is not true : Grad_Query Compiled error 0.22260109744821538 is greater than ref error 8.437808984711331e-05 by more than 1.1X.

See https://github.com/pytorch/pytorch/issues/159940

Backport https://github.com/triton-lang/triton/commit/1c5ea2297c1bc5c0760b07c26c8232e76705f5c6 "Update ptxas to version 12.8.93"
which removes the workaround which seems to cause that.

Author: Alexander Grund (TU Dresden)

diff --git a/.gitignore b/.gitignore
index ec8a00351..fd642b747 100644
--- a/.gitignore
+++ b/.gitignore
@@ -57,7 +57,6 @@ cmake-build-*
 cuobjdump
 nvdisasm
 ptxas
-ptxas-blackwell
 
 # Third-party include
 third_party/nvidia/backend/include
diff --git a/cmake/nvidia-toolchain-version.json b/cmake/nvidia-toolchain-version.json
index 0436299a0..5d7358ae5 100644
--- a/cmake/nvidia-toolchain-version.json
+++ b/cmake/nvidia-toolchain-version.json
@@ -1,9 +1,8 @@
 {
-  "ptxas-blackwell": "12.8.61",
-  "ptxas": "12.4.99",
+  "ptxas": "12.8.93",
   "cuobjdump": "12.8.55",
   "nvdisasm": "12.8.55",
   "cudacrt": "12.8.61",
   "cudart": "12.8.57",
-  "cupti": "12.8.57"
+  "cupti": "12.8.90"
 }
diff --git a/python/setup.py b/python/setup.py
index 2a026458d..493cc8330 100644
--- a/python/setup.py
+++ b/python/setup.py
@@ -528,16 +528,6 @@ download_and_copy(
     url_func=lambda system, arch, version:
     f"https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/{system}-{arch}/cuda_nvcc-{system}-{arch}-{version}-archive.tar.xz",
 )
-# We download a separate ptxas for blackwell, since there are some bugs when using it for hopper
-download_and_copy(
-    name="nvcc",
-    src_func=lambda system, arch, version: f"cuda_nvcc-{system}-{arch}-{version}-archive/bin/ptxas{exe_extension}",
-    dst_path="bin/ptxas-blackwell",
-    variable="TRITON_PTXAS_PATH",
-    version=NVIDIA_TOOLCHAIN_VERSION["ptxas-blackwell"],
-    url_func=lambda system, arch, version:
-    f"https://developer.download.nvidia.com/compute/cuda/redist/cuda_nvcc/{system}-{arch}/cuda_nvcc-{system}-{arch}-{version}-archive.tar.xz",
-)
 download_and_copy(
     name="cuobjdump",
     src_func=lambda system, arch, version:
diff --git a/third_party/nvidia/backend/compiler.py b/third_party/nvidia/backend/compiler.py
index 6db76a352..666d87294 100644
--- a/third_party/nvidia/backend/compiler.py
+++ b/third_party/nvidia/backend/compiler.py
@@ -49,17 +49,17 @@ def _path_to_binary(binary: str):
 
 
 @functools.lru_cache()
-def get_ptxas(arch: int):
-    name = "ptxas-blackwell" if arch >= 100 else "ptxas"
+def get_ptxas():
+    name = "ptxas"
     return _path_to_binary(name)
 
 
 @functools.lru_cache()
-def get_ptxas_version(arch: int):
+def get_ptxas_version():
     mock_ver = os.environ.get('TRITON_MOCK_PTX_VERSION')
     if mock_ver is not None:
         return mock_ver  # This is not really a version of ptxas, but it is good enough for testing
-    version = subprocess.check_output([get_ptxas(arch)[0], "--version"]).decode("utf-8")
+    version = subprocess.check_output([get_ptxas()[0], "--version"]).decode("utf-8")
     return version
 
 
@@ -85,7 +85,7 @@ def ptx_get_version(cuda_version) -> int:
 def get_ptx_version_from_options(options, arch: int):
     ptx_version = options.ptx_version
     if ptx_version is None:
-        _, cuda_version = get_ptxas(arch)
+        _, cuda_version = get_ptxas()
         ptx_version = ptx_get_version(cuda_version)
     return ptx_version
 
@@ -266,7 +266,6 @@ class CUDABackend(BaseBackend):
             passes.ttgpuir.add_fuse_nested_loops(pm)
             passes.common.add_canonicalizer(pm)
             passes.common.add_licm(pm)
-            passes.ttgpuir.add_optimize_accumulator_init(pm)
             passes.common.add_canonicalizer(pm)
             passes.ttgpuir.add_combine_tensor_select_and_if(pm)
             passes.ttgpuir.add_ws_task_partition(pm, opt.num_consumer_groups)
@@ -401,7 +400,7 @@ class CUDABackend(BaseBackend):
         return ret
 
     def make_cubin(self, src, metadata, opt, capability):
-        ptxas, _ = get_ptxas(self.target.arch)
+        ptxas, _ = get_ptxas()
         with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.ptx') as fsrc, \
             tempfile.NamedTemporaryFile(delete=False, mode='r', suffix='.log') as flog:
             fsrc.write(src)
@@ -453,5 +452,5 @@ class CUDABackend(BaseBackend):
 
     @functools.lru_cache()
     def hash(self):
-        version = get_ptxas_version(self.target.arch)
+        version = get_ptxas_version()
         return f'{version}-{self.target.arch}'
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
index 5b99ef061..ef48a21ad 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
@@ -175,11 +175,8 @@ DotOpMmaV3SmemLoader loadA(const LLVMTypeConverter *typeConverter,
 
   // The descriptor should be calculated based on the first warp of the
   // warpgroup.
-  Value warp = b.and_(b.udiv(thread, b.i32_val(32)), b.i32_val(0xFFFFFFFC));
-  // Workaround for a bug in ptxas 12.3 that cause a failure in
-  // test_core.py::test_dot. The shuffle will force the compiler to treat the
-  // value as uniform and prevent wrong optimizations.
-  warp = mlir::LLVM::NVIDIA::shuffleIdx(loc, rewriter, warp, 0);
+  Value warp =
+      b.and_(rewriter.create<nvgpu::WarpIdOp>(loc), b.i32_val(0xFFFFFFFC));
   Value warpM = b.urem(warp, b.i32_val(wpt[0]));
   Value warpId = b.urem(warpM, b.i32_val(shapePerCTA[0] / instrShape[0]));
 
@@ -208,7 +205,8 @@ DotOpMmaV3SmemLoader loadB(const LLVMTypeConverter *typeConverter,
   bool transB = !bSharedLayout.getTransposed();
   auto shapePerCTA = triton::gpu::getShapePerCTA(bTy);
 
-  Value warp = b.and_(b.udiv(thread, b.i32_val(32)), b.i32_val(0xFFFFFFFC));
+  Value warp =
+      b.and_(rewriter.create<nvgpu::WarpIdOp>(loc), b.i32_val(0xFFFFFFFC));
   Value warpMN = b.udiv(warp, b.i32_val(wpt[0]));
   Value warpN = b.urem(warpMN, b.i32_val(wpt[1]));
   Value warpId = b.urem(warpN, b.i32_val(shapePerCTA[1] / instrShape[1]));
