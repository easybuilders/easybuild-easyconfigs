Add support for flatbuffers 2.0.6 on top of 2.0
Taken from https://github.com/tensorflow/tensorflow/commit/3ef396642bb88580b527077cf2d6dc12325085be

Backported to 2.9.1 by
Author: Alexander Grund (TU Dresden)

diff --git a/tensorflow/lite/CMakeLists.txt b/tensorflow/lite/CMakeLists.txt
index 40f9485b5d6..5ac317b9c13 100644
--- a/tensorflow/lite/CMakeLists.txt
+++ b/tensorflow/lite/CMakeLists.txt
@@ -306,6 +306,11 @@ if(TFLITE_ENABLE_GPU)
     ${TFLITE_SOURCE_DIR}/tools/versioning/gpu_compatibility.cc
     ${TFLITE_SOURCE_DIR}/tools/versioning/op_signature.cc
   )
+  include_directories(
+    AFTER
+    ${TFLITE_SOURCE_DIR}/delegates/gpu/common
+    ${TFLITE_SOURCE_DIR}/delegates/gpu/common/task
+  )
   if(TFLITE_ENABLE_METAL AND "${CMAKE_SYSTEM_NAME}" STREQUAL "Darwin")
     #
     # libmetal_delegate library
diff --git a/tensorflow/lite/delegates/gpu/cl/compiled_program_cache_generated.h b/tensorflow/lite/delegates/gpu/cl/compiled_program_cache_generated.h
index 8a12bf2a9db..ec0f176b45a 100644
--- a/tensorflow/lite/delegates/gpu/cl/compiled_program_cache_generated.h
+++ b/tensorflow/lite/delegates/gpu/cl/compiled_program_cache_generated.h
@@ -26,10 +26,13 @@ namespace cl {
 namespace data {
 
 struct Program;
+struct ProgramBuilder;
 
 struct CompiledCache;
+struct CompiledCacheBuilder;
 
 struct Program FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef ProgramBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_FINGERPRINT = 4,
     VT_BINARY = 6
@@ -42,7 +45,7 @@ struct Program FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint64_t>(verifier, VT_FINGERPRINT) &&
+           VerifyField<uint64_t>(verifier, VT_FINGERPRINT, 8) &&
            VerifyOffset(verifier, VT_BINARY) &&
            verifier.VerifyVector(binary()) &&
            verifier.EndTable();
@@ -50,6 +53,7 @@ struct Program FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct ProgramBuilder {
+  typedef Program Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_fingerprint(uint64_t fingerprint) {
@@ -62,7 +66,6 @@ struct ProgramBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  ProgramBuilder &operator=(const ProgramBuilder &);
   flatbuffers::Offset<Program> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<Program>(end);
@@ -92,6 +95,7 @@ inline flatbuffers::Offset<Program> CreateProgramDirect(
 }
 
 struct CompiledCache FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
+  typedef CompiledCacheBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_DRIVER_VERSION = 4,
     VT_PROGRAMS = 6
@@ -99,8 +103,8 @@ struct CompiledCache FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::String *driver_version() const {
     return GetPointer<const flatbuffers::String *>(VT_DRIVER_VERSION);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<Program>> *programs() const {
-    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<Program>> *>(VT_PROGRAMS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::Program>> *programs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::Program>> *>(VT_PROGRAMS);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
@@ -114,19 +118,19 @@ struct CompiledCache FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct CompiledCacheBuilder {
+  typedef CompiledCache Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_driver_version(flatbuffers::Offset<flatbuffers::String> driver_version) {
     fbb_.AddOffset(CompiledCache::VT_DRIVER_VERSION, driver_version);
   }
-  void add_programs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Program>>> programs) {
+  void add_programs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::Program>>> programs) {
     fbb_.AddOffset(CompiledCache::VT_PROGRAMS, programs);
   }
   explicit CompiledCacheBuilder(flatbuffers::FlatBufferBuilder &_fbb)
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  CompiledCacheBuilder &operator=(const CompiledCacheBuilder &);
   flatbuffers::Offset<CompiledCache> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<CompiledCache>(end);
@@ -137,7 +141,7 @@ struct CompiledCacheBuilder {
 inline flatbuffers::Offset<CompiledCache> CreateCompiledCache(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<flatbuffers::String> driver_version = 0,
-    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<Program>>> programs = 0) {
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::Program>>> programs = 0) {
   CompiledCacheBuilder builder_(_fbb);
   builder_.add_programs(programs);
   builder_.add_driver_version(driver_version);
@@ -147,9 +151,9 @@ inline flatbuffers::Offset<CompiledCache> CreateCompiledCache(
 inline flatbuffers::Offset<CompiledCache> CreateCompiledCacheDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     const char *driver_version = nullptr,
-    const std::vector<flatbuffers::Offset<Program>> *programs = nullptr) {
+    const std::vector<flatbuffers::Offset<tflite::gpu::cl::data::Program>> *programs = nullptr) {
   auto driver_version__ = driver_version ? _fbb.CreateString(driver_version) : 0;
-  auto programs__ = programs ? _fbb.CreateVector<flatbuffers::Offset<Program>>(*programs) : 0;
+  auto programs__ = programs ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::cl::data::Program>>(*programs) : 0;
   return tflite::gpu::cl::data::CreateCompiledCache(
       _fbb,
       driver_version__,
@@ -173,6 +177,11 @@ inline bool CompiledCacheBufferHasIdentifier(const void *buf) {
       buf, CompiledCacheIdentifier());
 }
 
+inline bool SizePrefixedCompiledCacheBufferHasIdentifier(const void *buf) {
+  return flatbuffers::BufferHasIdentifier(
+      buf, CompiledCacheIdentifier(), true);
+}
+
 inline bool VerifyCompiledCacheBuffer(
     flatbuffers::Verifier &verifier) {
   return verifier.VerifyBuffer<tflite::gpu::cl::data::CompiledCache>(CompiledCacheIdentifier());
diff --git a/tensorflow/lite/delegates/gpu/cl/serialization_generated.h b/tensorflow/lite/delegates/gpu/cl/serialization_generated.h
index 26691adb44a..aa4affb7ed4 100644
--- a/tensorflow/lite/delegates/gpu/cl/serialization_generated.h
+++ b/tensorflow/lite/delegates/gpu/cl/serialization_generated.h
@@ -19,8 +19,9 @@ limitations under the License.
 #define FLATBUFFERS_GENERATED_SERIALIZATION_TFLITE_GPU_CL_DATA_H_
 
 #include "flatbuffers/flatbuffers.h"
-#include "tensorflow/lite/delegates/gpu/common/gpu_model_generated.h"
-#include "tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h"
+
+#include "serialization_base_generated.h"
+#include "gpu_model_generated.h"
 
 namespace tflite {
 namespace gpu {
@@ -39,15 +40,18 @@ struct BinaryProgram FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_FINGERPRINT = 4,
     VT_BINARY = 6
   };
-  uint64_t fingerprint() const { return GetField<uint64_t>(VT_FINGERPRINT, 0); }
+  uint64_t fingerprint() const {
+    return GetField<uint64_t>(VT_FINGERPRINT, 0);
+  }
   const flatbuffers::Vector<uint8_t> *binary() const {
     return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_BINARY);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint64_t>(verifier, VT_FINGERPRINT) &&
+           VerifyField<uint64_t>(verifier, VT_FINGERPRINT, 8) &&
            VerifyOffset(verifier, VT_BINARY) &&
-           verifier.VerifyVector(binary()) && verifier.EndTable();
+           verifier.VerifyVector(binary()) &&
+           verifier.EndTable();
   }
 };
 
@@ -62,7 +66,7 @@ struct BinaryProgramBuilder {
     fbb_.AddOffset(BinaryProgram::VT_BINARY, binary);
   }
   explicit BinaryProgramBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-      : fbb_(_fbb) {
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<BinaryProgram> Finish() {
@@ -73,7 +77,8 @@ struct BinaryProgramBuilder {
 };
 
 inline flatbuffers::Offset<BinaryProgram> CreateBinaryProgram(
-    flatbuffers::FlatBufferBuilder &_fbb, uint64_t fingerprint = 0,
+    flatbuffers::FlatBufferBuilder &_fbb,
+    uint64_t fingerprint = 0,
     flatbuffers::Offset<flatbuffers::Vector<uint8_t>> binary = 0) {
   BinaryProgramBuilder builder_(_fbb);
   builder_.add_fingerprint(fingerprint);
@@ -82,11 +87,14 @@ inline flatbuffers::Offset<BinaryProgram> CreateBinaryProgram(
 }
 
 inline flatbuffers::Offset<BinaryProgram> CreateBinaryProgramDirect(
-    flatbuffers::FlatBufferBuilder &_fbb, uint64_t fingerprint = 0,
+    flatbuffers::FlatBufferBuilder &_fbb,
+    uint64_t fingerprint = 0,
     const std::vector<uint8_t> *binary = nullptr) {
   auto binary__ = binary ? _fbb.CreateVector<uint8_t>(*binary) : 0;
-  return tflite::gpu::cl::data::CreateBinaryProgram(_fbb, fingerprint,
-                                                    binary__);
+  return tflite::gpu::cl::data::CreateBinaryProgram(
+      _fbb,
+      fingerprint,
+      binary__);
 }
 
 struct InferenceContext FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -104,25 +112,18 @@ struct InferenceContext FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::String *driver_version() const {
     return GetPointer<const flatbuffers::String *>(VT_DRIVER_VERSION);
   }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>
-      *binary_programs() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>> *>(
-        VT_BINARY_PROGRAMS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>> *binary_programs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>> *>(VT_BINARY_PROGRAMS);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>>
-      *tuned_work_group_sizes_per_node() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::Int3>> *>(
-        VT_TUNED_WORK_GROUP_SIZES_PER_NODE);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>> *tuned_work_group_sizes_per_node() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>> *>(VT_TUNED_WORK_GROUP_SIZES_PER_NODE);
   }
   const flatbuffers::Vector<uint64_t> *fingerprints_per_node() const {
-    return GetPointer<const flatbuffers::Vector<uint64_t> *>(
-        VT_FINGERPRINTS_PER_NODE);
+    return GetPointer<const flatbuffers::Vector<uint64_t> *>(VT_FINGERPRINTS_PER_NODE);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) && VerifyOffset(verifier, VT_GPU_MODEL) &&
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_GPU_MODEL) &&
            verifier.VerifyTable(gpu_model()) &&
            VerifyOffset(verifier, VT_DRIVER_VERSION) &&
            verifier.VerifyString(driver_version()) &&
@@ -142,32 +143,20 @@ struct InferenceContextBuilder {
   typedef InferenceContext Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_gpu_model(
-      flatbuffers::Offset<tflite::gpu::data::GpuModel> gpu_model) {
+  void add_gpu_model(flatbuffers::Offset<tflite::gpu::data::GpuModel> gpu_model) {
     fbb_.AddOffset(InferenceContext::VT_GPU_MODEL, gpu_model);
   }
-  void add_driver_version(
-      flatbuffers::Offset<flatbuffers::String> driver_version) {
+  void add_driver_version(flatbuffers::Offset<flatbuffers::String> driver_version) {
     fbb_.AddOffset(InferenceContext::VT_DRIVER_VERSION, driver_version);
   }
-  void add_binary_programs(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>>
-          binary_programs) {
+  void add_binary_programs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>> binary_programs) {
     fbb_.AddOffset(InferenceContext::VT_BINARY_PROGRAMS, binary_programs);
   }
-  void add_tuned_work_group_sizes_per_node(
-      flatbuffers::Offset<
-          flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>>>
-          tuned_work_group_sizes_per_node) {
-    fbb_.AddOffset(InferenceContext::VT_TUNED_WORK_GROUP_SIZES_PER_NODE,
-                   tuned_work_group_sizes_per_node);
+  void add_tuned_work_group_sizes_per_node(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>>> tuned_work_group_sizes_per_node) {
+    fbb_.AddOffset(InferenceContext::VT_TUNED_WORK_GROUP_SIZES_PER_NODE, tuned_work_group_sizes_per_node);
   }
-  void add_fingerprints_per_node(
-      flatbuffers::Offset<flatbuffers::Vector<uint64_t>>
-          fingerprints_per_node) {
-    fbb_.AddOffset(InferenceContext::VT_FINGERPRINTS_PER_NODE,
-                   fingerprints_per_node);
+  void add_fingerprints_per_node(flatbuffers::Offset<flatbuffers::Vector<uint64_t>> fingerprints_per_node) {
+    fbb_.AddOffset(InferenceContext::VT_FINGERPRINTS_PER_NODE, fingerprints_per_node);
   }
   explicit InferenceContextBuilder(flatbuffers::FlatBufferBuilder &_fbb)
         : fbb_(_fbb) {
@@ -184,14 +173,9 @@ inline flatbuffers::Offset<InferenceContext> CreateInferenceContext(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<tflite::gpu::data::GpuModel> gpu_model = 0,
     flatbuffers::Offset<flatbuffers::String> driver_version = 0,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>>
-        binary_programs = 0,
-    flatbuffers::Offset<
-        flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>>>
-        tuned_work_group_sizes_per_node = 0,
-    flatbuffers::Offset<flatbuffers::Vector<uint64_t>> fingerprints_per_node =
-        0) {
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>> binary_programs = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::Int3>>> tuned_work_group_sizes_per_node = 0,
+    flatbuffers::Offset<flatbuffers::Vector<uint64_t>> fingerprints_per_node = 0) {
   InferenceContextBuilder builder_(_fbb);
   builder_.add_fingerprints_per_node(fingerprints_per_node);
   builder_.add_tuned_work_group_sizes_per_node(tuned_work_group_sizes_per_node);
@@ -205,31 +189,20 @@ inline flatbuffers::Offset<InferenceContext> CreateInferenceContextDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<tflite::gpu::data::GpuModel> gpu_model = 0,
     const char *driver_version = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>
-        *binary_programs = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::Int3>>
-        *tuned_work_group_sizes_per_node = nullptr,
+    const std::vector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>> *binary_programs = nullptr,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::Int3>> *tuned_work_group_sizes_per_node = nullptr,
     const std::vector<uint64_t> *fingerprints_per_node = nullptr) {
-  auto driver_version__ =
-      driver_version ? _fbb.CreateString(driver_version) : 0;
-  auto binary_programs__ =
-      binary_programs
-          ? _fbb.CreateVector<
-                flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>(
-                *binary_programs)
-          : 0;
-  auto tuned_work_group_sizes_per_node__ =
-      tuned_work_group_sizes_per_node
-          ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::Int3>>(
-                *tuned_work_group_sizes_per_node)
-          : 0;
-  auto fingerprints_per_node__ =
-      fingerprints_per_node
-          ? _fbb.CreateVector<uint64_t>(*fingerprints_per_node)
-          : 0;
+  auto driver_version__ = driver_version ? _fbb.CreateString(driver_version) : 0;
+  auto binary_programs__ = binary_programs ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::cl::data::BinaryProgram>>(*binary_programs) : 0;
+  auto tuned_work_group_sizes_per_node__ = tuned_work_group_sizes_per_node ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::Int3>>(*tuned_work_group_sizes_per_node) : 0;
+  auto fingerprints_per_node__ = fingerprints_per_node ? _fbb.CreateVector<uint64_t>(*fingerprints_per_node) : 0;
   return tflite::gpu::cl::data::CreateInferenceContext(
-      _fbb, gpu_model, driver_version__, binary_programs__,
-      tuned_work_group_sizes_per_node__, fingerprints_per_node__);
+      _fbb,
+      gpu_model,
+      driver_version__,
+      binary_programs__,
+      tuned_work_group_sizes_per_node__,
+      fingerprints_per_node__);
 }
 
 inline const tflite::gpu::cl::data::InferenceContext *GetInferenceContext(const void *buf) {
diff --git a/tensorflow/lite/delegates/gpu/common/gpu_model_generated.h b/tensorflow/lite/delegates/gpu/common/gpu_model_generated.h
index 0f0873b1e44..8cc0d6c75c9 100644
--- a/tensorflow/lite/delegates/gpu/common/gpu_model_generated.h
+++ b/tensorflow/lite/delegates/gpu/common/gpu_model_generated.h
@@ -18,7 +18,8 @@ limitations under the License.
 #define FLATBUFFERS_GENERATED_GPUMODEL_TFLITE_GPU_DATA_H_
 
 #include "flatbuffers/flatbuffers.h"
-#include "tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h"
+
+#include "serialization_base_generated.h"
 
 namespace tflite {
 namespace gpu {
@@ -45,11 +46,15 @@ struct TensorDescWithId FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const tflite::gpu::data::TensorDescriptor *desc() const {
     return GetPointer<const tflite::gpu::data::TensorDescriptor *>(VT_DESC);
   }
-  int32_t id() const { return GetField<int32_t>(VT_ID, 0); }
+  int32_t id() const {
+    return GetField<int32_t>(VT_ID, 0);
+  }
   bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) && VerifyOffset(verifier, VT_DESC) &&
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_DESC) &&
            verifier.VerifyTable(desc()) &&
-           VerifyField<int32_t>(verifier, VT_ID) && verifier.EndTable();
+           VerifyField<int32_t>(verifier, VT_ID, 4) &&
+           verifier.EndTable();
   }
 };
 
@@ -64,7 +69,7 @@ struct TensorDescWithIdBuilder {
     fbb_.AddElement<int32_t>(TensorDescWithId::VT_ID, id, 0);
   }
   explicit TensorDescWithIdBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-      : fbb_(_fbb) {
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<TensorDescWithId> Finish() {
@@ -90,12 +95,17 @@ struct PairOfValueIds FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_FIRST = 4,
     VT_SECOND = 6
   };
-  int32_t first() const { return GetField<int32_t>(VT_FIRST, 0); }
-  int32_t second() const { return GetField<int32_t>(VT_SECOND, 0); }
+  int32_t first() const {
+    return GetField<int32_t>(VT_FIRST, 0);
+  }
+  int32_t second() const {
+    return GetField<int32_t>(VT_SECOND, 0);
+  }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_FIRST) &&
-           VerifyField<int32_t>(verifier, VT_SECOND) && verifier.EndTable();
+           VerifyField<int32_t>(verifier, VT_FIRST, 4) &&
+           VerifyField<int32_t>(verifier, VT_SECOND, 4) &&
+           verifier.EndTable();
   }
 };
 
@@ -110,7 +120,7 @@ struct PairOfValueIdsBuilder {
     fbb_.AddElement<int32_t>(PairOfValueIds::VT_SECOND, second, 0);
   }
   explicit PairOfValueIdsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-      : fbb_(_fbb) {
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<PairOfValueIds> Finish() {
@@ -121,7 +131,8 @@ struct PairOfValueIdsBuilder {
 };
 
 inline flatbuffers::Offset<PairOfValueIds> CreatePairOfValueIds(
-    flatbuffers::FlatBufferBuilder &_fbb, int32_t first = 0,
+    flatbuffers::FlatBufferBuilder &_fbb,
+    int32_t first = 0,
     int32_t second = 0) {
   PairOfValueIdsBuilder builder_(_fbb);
   builder_.add_second(second);
@@ -150,13 +161,15 @@ struct GpuNode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return GetPointer<const flatbuffers::String *>(VT_NAME);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) && VerifyOffset(verifier, VT_GPU_OP) &&
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_GPU_OP) &&
            verifier.VerifyTable(gpu_op()) &&
            VerifyOffset(verifier, VT_INPUT_IDS) &&
            verifier.VerifyVector(input_ids()) &&
            VerifyOffset(verifier, VT_OUTPUT_IDS) &&
            verifier.VerifyVector(output_ids()) &&
-           VerifyOffset(verifier, VT_NAME) && verifier.VerifyString(name()) &&
+           VerifyOffset(verifier, VT_NAME) &&
+           verifier.VerifyString(name()) &&
            verifier.EndTable();
   }
 };
@@ -168,18 +181,17 @@ struct GpuNodeBuilder {
   void add_gpu_op(flatbuffers::Offset<tflite::gpu::data::GPUOperation> gpu_op) {
     fbb_.AddOffset(GpuNode::VT_GPU_OP, gpu_op);
   }
-  void add_input_ids(
-      flatbuffers::Offset<flatbuffers::Vector<int32_t>> input_ids) {
+  void add_input_ids(flatbuffers::Offset<flatbuffers::Vector<int32_t>> input_ids) {
     fbb_.AddOffset(GpuNode::VT_INPUT_IDS, input_ids);
   }
-  void add_output_ids(
-      flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_ids) {
+  void add_output_ids(flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_ids) {
     fbb_.AddOffset(GpuNode::VT_OUTPUT_IDS, output_ids);
   }
   void add_name(flatbuffers::Offset<flatbuffers::String> name) {
     fbb_.AddOffset(GpuNode::VT_NAME, name);
   }
-  explicit GpuNodeBuilder(flatbuffers::FlatBufferBuilder &_fbb) : fbb_(_fbb) {
+  explicit GpuNodeBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<GpuNode> Finish() {
@@ -212,8 +224,12 @@ inline flatbuffers::Offset<GpuNode> CreateGpuNodeDirect(
   auto input_ids__ = input_ids ? _fbb.CreateVector<int32_t>(*input_ids) : 0;
   auto output_ids__ = output_ids ? _fbb.CreateVector<int32_t>(*output_ids) : 0;
   auto name__ = name ? _fbb.CreateString(name) : 0;
-  return tflite::gpu::data::CreateGpuNode(_fbb, gpu_op, input_ids__,
-                                          output_ids__, name__);
+  return tflite::gpu::data::CreateGpuNode(
+      _fbb,
+      gpu_op,
+      input_ids__,
+      output_ids__,
+      name__);
 }
 
 struct GpuModel FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -228,24 +244,14 @@ struct GpuModel FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_OUTPUT_REFS = 16,
     VT_VARIABLE_IDS_AND_REFS = 18
   };
-  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>
-      *nodes() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::GpuNode>> *>(VT_NODES);
-  }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>
-      *tensors() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *>(
-        VT_TENSORS);
-  }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>
-      *const_tensors() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *>(
-        VT_CONST_TENSORS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>> *nodes() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>> *>(VT_NODES);
+  }
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *tensors() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *>(VT_TENSORS);
+  }
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *const_tensors() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *>(VT_CONST_TENSORS);
   }
   const flatbuffers::Vector<int32_t> *input_ids() const {
     return GetPointer<const flatbuffers::Vector<int32_t> *>(VT_INPUT_IDS);
@@ -259,15 +265,12 @@ struct GpuModel FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const flatbuffers::Vector<int64_t> *output_refs() const {
     return GetPointer<const flatbuffers::Vector<int64_t> *>(VT_OUTPUT_REFS);
   }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>
-      *variable_ids_and_refs() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>> *>(
-        VT_VARIABLE_IDS_AND_REFS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>> *variable_ids_and_refs() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>> *>(VT_VARIABLE_IDS_AND_REFS);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) && VerifyOffset(verifier, VT_NODES) &&
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_NODES) &&
            verifier.VerifyVector(nodes()) &&
            verifier.VerifyVectorOfTables(nodes()) &&
            VerifyOffset(verifier, VT_TENSORS) &&
@@ -295,47 +298,32 @@ struct GpuModelBuilder {
   typedef GpuModel Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_nodes(
-      flatbuffers::Offset<
-          flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>>
-          nodes) {
+  void add_nodes(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>> nodes) {
     fbb_.AddOffset(GpuModel::VT_NODES, nodes);
   }
-  void add_tensors(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>>
-          tensors) {
+  void add_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>> tensors) {
     fbb_.AddOffset(GpuModel::VT_TENSORS, tensors);
   }
-  void add_const_tensors(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>>
-          const_tensors) {
+  void add_const_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>> const_tensors) {
     fbb_.AddOffset(GpuModel::VT_CONST_TENSORS, const_tensors);
   }
-  void add_input_ids(
-      flatbuffers::Offset<flatbuffers::Vector<int32_t>> input_ids) {
+  void add_input_ids(flatbuffers::Offset<flatbuffers::Vector<int32_t>> input_ids) {
     fbb_.AddOffset(GpuModel::VT_INPUT_IDS, input_ids);
   }
-  void add_output_ids(
-      flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_ids) {
+  void add_output_ids(flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_ids) {
     fbb_.AddOffset(GpuModel::VT_OUTPUT_IDS, output_ids);
   }
-  void add_input_refs(
-      flatbuffers::Offset<flatbuffers::Vector<int64_t>> input_refs) {
+  void add_input_refs(flatbuffers::Offset<flatbuffers::Vector<int64_t>> input_refs) {
     fbb_.AddOffset(GpuModel::VT_INPUT_REFS, input_refs);
   }
-  void add_output_refs(
-      flatbuffers::Offset<flatbuffers::Vector<int64_t>> output_refs) {
+  void add_output_refs(flatbuffers::Offset<flatbuffers::Vector<int64_t>> output_refs) {
     fbb_.AddOffset(GpuModel::VT_OUTPUT_REFS, output_refs);
   }
-  void add_variable_ids_and_refs(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>>
-          variable_ids_and_refs) {
+  void add_variable_ids_and_refs(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>> variable_ids_and_refs) {
     fbb_.AddOffset(GpuModel::VT_VARIABLE_IDS_AND_REFS, variable_ids_and_refs);
   }
-  explicit GpuModelBuilder(flatbuffers::FlatBufferBuilder &_fbb) : fbb_(_fbb) {
+  explicit GpuModelBuilder(flatbuffers::FlatBufferBuilder &_fbb)
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<GpuModel> Finish() {
@@ -347,22 +335,14 @@ struct GpuModelBuilder {
 
 inline flatbuffers::Offset<GpuModel> CreateGpuModel(
     flatbuffers::FlatBufferBuilder &_fbb,
-    flatbuffers::Offset<
-        flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>>
-        nodes = 0,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>>
-        tensors = 0,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>>
-        const_tensors = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>> nodes = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>> tensors = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>> const_tensors = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> input_ids = 0,
     flatbuffers::Offset<flatbuffers::Vector<int32_t>> output_ids = 0,
     flatbuffers::Offset<flatbuffers::Vector<int64_t>> input_refs = 0,
     flatbuffers::Offset<flatbuffers::Vector<int64_t>> output_refs = 0,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>>
-        variable_ids_and_refs = 0) {
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>> variable_ids_and_refs = 0) {
   GpuModelBuilder builder_(_fbb);
   builder_.add_variable_ids_and_refs(variable_ids_and_refs);
   builder_.add_output_refs(output_refs);
@@ -377,48 +357,32 @@ inline flatbuffers::Offset<GpuModel> CreateGpuModel(
 
 inline flatbuffers::Offset<GpuModel> CreateGpuModelDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>> *nodes =
-        nullptr,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>
-        *tensors = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>
-        *const_tensors = nullptr,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::GpuNode>> *nodes = nullptr,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *tensors = nullptr,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>> *const_tensors = nullptr,
     const std::vector<int32_t> *input_ids = nullptr,
     const std::vector<int32_t> *output_ids = nullptr,
     const std::vector<int64_t> *input_refs = nullptr,
     const std::vector<int64_t> *output_refs = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>
-        *variable_ids_and_refs = nullptr) {
-  auto nodes__ =
-      nodes
-          ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>(
-                *nodes)
-          : 0;
-  auto tensors__ =
-      tensors ? _fbb.CreateVector<
-                    flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>(
-                    *tensors)
-              : 0;
-  auto const_tensors__ =
-      const_tensors
-          ? _fbb.CreateVector<
-                flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>(
-                *const_tensors)
-          : 0;
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>> *variable_ids_and_refs = nullptr) {
+  auto nodes__ = nodes ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::GpuNode>>(*nodes) : 0;
+  auto tensors__ = tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>(*tensors) : 0;
+  auto const_tensors__ = const_tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::TensorDescWithId>>(*const_tensors) : 0;
   auto input_ids__ = input_ids ? _fbb.CreateVector<int32_t>(*input_ids) : 0;
   auto output_ids__ = output_ids ? _fbb.CreateVector<int32_t>(*output_ids) : 0;
   auto input_refs__ = input_refs ? _fbb.CreateVector<int64_t>(*input_refs) : 0;
-  auto output_refs__ =
-      output_refs ? _fbb.CreateVector<int64_t>(*output_refs) : 0;
-  auto variable_ids_and_refs__ =
-      variable_ids_and_refs
-          ? _fbb.CreateVector<
-                flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>(
-                *variable_ids_and_refs)
-          : 0;
+  auto output_refs__ = output_refs ? _fbb.CreateVector<int64_t>(*output_refs) : 0;
+  auto variable_ids_and_refs__ = variable_ids_and_refs ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::PairOfValueIds>>(*variable_ids_and_refs) : 0;
   return tflite::gpu::data::CreateGpuModel(
-      _fbb, nodes__, tensors__, const_tensors__, input_ids__, output_ids__,
-      input_refs__, output_refs__, variable_ids_and_refs__);
+      _fbb,
+      nodes__,
+      tensors__,
+      const_tensors__,
+      input_ids__,
+      output_ids__,
+      input_refs__,
+      output_refs__,
+      variable_ids_and_refs__);
 }
 
 }  // namespace data
diff --git a/tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h b/tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h
index a8f7b714e80..c2bd14c3c1a 100644
--- a/tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h
+++ b/tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h
@@ -139,17 +139,38 @@ enum class DataType : int8_t {
 
 inline const DataType (&EnumValuesDataType())[12] {
   static const DataType values[] = {
-      DataType::UNKNOWN, DataType::FLOAT16, DataType::FLOAT32,
-      DataType::FLOAT64, DataType::UINT8,   DataType::INT8,
-      DataType::UINT16,  DataType::INT16,   DataType::UINT32,
-      DataType::INT32,   DataType::UINT64,  DataType::INT64};
+    DataType::UNKNOWN,
+    DataType::FLOAT16,
+    DataType::FLOAT32,
+    DataType::FLOAT64,
+    DataType::UINT8,
+    DataType::INT8,
+    DataType::UINT16,
+    DataType::INT16,
+    DataType::UINT32,
+    DataType::INT32,
+    DataType::UINT64,
+    DataType::INT64
+  };
   return values;
 }
 
 inline const char * const *EnumNamesDataType() {
-  static const char *const names[13] = {
-      "UNKNOWN", "FLOAT16", "FLOAT32", "FLOAT64", "UINT8", "INT8", "UINT16",
-      "INT16",   "UINT32",  "INT32",   "UINT64",  "INT64", nullptr};
+  static const char * const names[13] = {
+    "UNKNOWN",
+    "FLOAT16",
+    "FLOAT32",
+    "FLOAT64",
+    "UINT8",
+    "INT8",
+    "UINT16",
+    "INT16",
+    "UINT32",
+    "INT32",
+    "UINT64",
+    "INT64",
+    nullptr
+  };
   return names;
 }
 
@@ -315,21 +336,26 @@ enum class CalculationsPrecision : int8_t {
 };
 
 inline const CalculationsPrecision (&EnumValuesCalculationsPrecision())[3] {
-  static const CalculationsPrecision values[] = {CalculationsPrecision::F32,
-                                                 CalculationsPrecision::F32_F16,
-                                                 CalculationsPrecision::F16};
+  static const CalculationsPrecision values[] = {
+    CalculationsPrecision::F32,
+    CalculationsPrecision::F32_F16,
+    CalculationsPrecision::F16
+  };
   return values;
 }
 
-inline const char *const *EnumNamesCalculationsPrecision() {
-  static const char *const names[4] = {"F32", "F32_F16", "F16", nullptr};
+inline const char * const *EnumNamesCalculationsPrecision() {
+  static const char * const names[4] = {
+    "F32",
+    "F32_F16",
+    "F16",
+    nullptr
+  };
   return names;
 }
 
 inline const char *EnumNameCalculationsPrecision(CalculationsPrecision e) {
-  if (flatbuffers::IsOutRange(e, CalculationsPrecision::F32,
-                              CalculationsPrecision::F16))
-    return "";
+  if (flatbuffers::IsOutRange(e, CalculationsPrecision::F32, CalculationsPrecision::F16)) return "";
   const size_t index = static_cast<size_t>(e);
   return EnumNamesCalculationsPrecision()[index];
 }
@@ -346,26 +372,29 @@ enum class TensorToGrid : int8_t {
 
 inline const TensorToGrid (&EnumValuesTensorToGrid())[5] {
   static const TensorToGrid values[] = {
-      TensorToGrid::CUSTOM, TensorToGrid::WB_TO_X_HD_TO_Y_S_TO_Z,
-      TensorToGrid::WB_TO_X_HD_TO_Y_Z_IS_1, TensorToGrid::WB_TO_X_H_TO_Y_D_TO_Z,
-      TensorToGrid::B_TO_X_Y_IS_1_Z_IS_1};
+    TensorToGrid::CUSTOM,
+    TensorToGrid::WB_TO_X_HD_TO_Y_S_TO_Z,
+    TensorToGrid::WB_TO_X_HD_TO_Y_Z_IS_1,
+    TensorToGrid::WB_TO_X_H_TO_Y_D_TO_Z,
+    TensorToGrid::B_TO_X_Y_IS_1_Z_IS_1
+  };
   return values;
 }
 
-inline const char *const *EnumNamesTensorToGrid() {
-  static const char *const names[6] = {"CUSTOM",
-                                       "WB_TO_X_HD_TO_Y_S_TO_Z",
-                                       "WB_TO_X_HD_TO_Y_Z_IS_1",
-                                       "WB_TO_X_H_TO_Y_D_TO_Z",
-                                       "B_TO_X_Y_IS_1_Z_IS_1",
-                                       nullptr};
+inline const char * const *EnumNamesTensorToGrid() {
+  static const char * const names[6] = {
+    "CUSTOM",
+    "WB_TO_X_HD_TO_Y_S_TO_Z",
+    "WB_TO_X_HD_TO_Y_Z_IS_1",
+    "WB_TO_X_H_TO_Y_D_TO_Z",
+    "B_TO_X_Y_IS_1_Z_IS_1",
+    nullptr
+  };
   return names;
 }
 
 inline const char *EnumNameTensorToGrid(TensorToGrid e) {
-  if (flatbuffers::IsOutRange(e, TensorToGrid::CUSTOM,
-                              TensorToGrid::B_TO_X_Y_IS_1_Z_IS_1))
-    return "";
+  if (flatbuffers::IsOutRange(e, TensorToGrid::CUSTOM, TensorToGrid::B_TO_X_Y_IS_1_Z_IS_1)) return "";
   const size_t index = static_cast<size_t>(e);
   return EnumNamesTensorToGrid()[index];
 }
@@ -383,30 +412,31 @@ enum class CompilerOptions : int8_t {
 
 inline const CompilerOptions (&EnumValuesCompilerOptions())[6] {
   static const CompilerOptions values[] = {
-      CompilerOptions::ADRENO_FULL_SIMD_LINE,
-      CompilerOptions::ADRENO_MORE_WAVES,
-      CompilerOptions::CL_FAST_RELAXED_MATH,
-      CompilerOptions::CL_OPT_DISABLE,
-      CompilerOptions::CL_2_0,
-      CompilerOptions::CL_3_0};
+    CompilerOptions::ADRENO_FULL_SIMD_LINE,
+    CompilerOptions::ADRENO_MORE_WAVES,
+    CompilerOptions::CL_FAST_RELAXED_MATH,
+    CompilerOptions::CL_OPT_DISABLE,
+    CompilerOptions::CL_2_0,
+    CompilerOptions::CL_3_0
+  };
   return values;
 }
 
-inline const char *const *EnumNamesCompilerOptions() {
-  static const char *const names[7] = {"ADRENO_FULL_SIMD_LINE",
-                                       "ADRENO_MORE_WAVES",
-                                       "CL_FAST_RELAXED_MATH",
-                                       "CL_OPT_DISABLE",
-                                       "CL_2_0",
-                                       "CL_3_0",
-                                       nullptr};
+inline const char * const *EnumNamesCompilerOptions() {
+  static const char * const names[7] = {
+    "ADRENO_FULL_SIMD_LINE",
+    "ADRENO_MORE_WAVES",
+    "CL_FAST_RELAXED_MATH",
+    "CL_OPT_DISABLE",
+    "CL_2_0",
+    "CL_3_0",
+    nullptr
+  };
   return names;
 }
 
 inline const char *EnumNameCompilerOptions(CompilerOptions e) {
-  if (flatbuffers::IsOutRange(e, CompilerOptions::ADRENO_FULL_SIMD_LINE,
-                              CompilerOptions::CL_3_0))
-    return "";
+  if (flatbuffers::IsOutRange(e, CompilerOptions::ADRENO_FULL_SIMD_LINE, CompilerOptions::CL_3_0)) return "";
   const size_t index = static_cast<size_t>(e);
   return EnumNamesCompilerOptions()[index];
 }
@@ -433,10 +463,10 @@ struct Int4 FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_X) &&
-           VerifyField<int32_t>(verifier, VT_Y) &&
-           VerifyField<int32_t>(verifier, VT_Z) &&
-           VerifyField<int32_t>(verifier, VT_W) &&
+           VerifyField<int32_t>(verifier, VT_X, 4) &&
+           VerifyField<int32_t>(verifier, VT_Y, 4) &&
+           VerifyField<int32_t>(verifier, VT_Z, 4) &&
+           VerifyField<int32_t>(verifier, VT_W, 4) &&
            verifier.EndTable();
   }
 };
@@ -500,9 +530,9 @@ struct Int3 FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_X) &&
-           VerifyField<int32_t>(verifier, VT_Y) &&
-           VerifyField<int32_t>(verifier, VT_Z) &&
+           VerifyField<int32_t>(verifier, VT_X, 4) &&
+           VerifyField<int32_t>(verifier, VT_Y, 4) &&
+           VerifyField<int32_t>(verifier, VT_Z, 4) &&
            verifier.EndTable();
   }
 };
@@ -557,8 +587,8 @@ struct Int2 FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_X) &&
-           VerifyField<int32_t>(verifier, VT_Y) &&
+           VerifyField<int32_t>(verifier, VT_X, 4) &&
+           VerifyField<int32_t>(verifier, VT_Y, 4) &&
            verifier.EndTable();
   }
 };
@@ -676,7 +706,7 @@ struct GPUObjectDescriptor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
            VerifyOffset(verifier, VT_STATE_VARS) &&
            verifier.VerifyVector(state_vars()) &&
            verifier.VerifyVectorOfTables(state_vars()) &&
-           VerifyField<int8_t>(verifier, VT_ACCESS_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_ACCESS_TYPE, 1) &&
            verifier.EndTable();
   }
 };
@@ -743,8 +773,8 @@ struct IntValue FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_NAME) &&
            verifier.VerifyString(name()) &&
-           VerifyField<int32_t>(verifier, VT_VALUE) &&
-           VerifyField<uint8_t>(verifier, VT_ACTIVE) &&
+           VerifyField<int32_t>(verifier, VT_VALUE, 4) &&
+           VerifyField<uint8_t>(verifier, VT_ACTIVE, 1) &&
            verifier.EndTable();
   }
 };
@@ -818,8 +848,8 @@ struct FloatValue FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_NAME) &&
            verifier.VerifyString(name()) &&
-           VerifyField<float>(verifier, VT_VALUE) &&
-           VerifyField<uint8_t>(verifier, VT_ACTIVE) &&
+           VerifyField<float>(verifier, VT_VALUE, 4) &&
+           VerifyField<uint8_t>(verifier, VT_ACTIVE, 1) &&
            verifier.EndTable();
   }
 };
@@ -893,8 +923,8 @@ struct HalfValue FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_NAME) &&
            verifier.VerifyString(name()) &&
-           VerifyField<float>(verifier, VT_VALUE) &&
-           VerifyField<uint8_t>(verifier, VT_ACTIVE) &&
+           VerifyField<float>(verifier, VT_VALUE, 4) &&
+           VerifyField<uint8_t>(verifier, VT_ACTIVE, 1) &&
            verifier.EndTable();
   }
 };
@@ -984,13 +1014,13 @@ struct BufferDescriptor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_BASE_OBJ) &&
            verifier.VerifyTable(base_obj()) &&
-           VerifyField<int8_t>(verifier, VT_ELEMENT_TYPE) &&
-           VerifyField<int32_t>(verifier, VT_ELEMENT_SIZE) &&
-           VerifyField<int8_t>(verifier, VT_MEMORY_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_ELEMENT_TYPE, 1) &&
+           VerifyField<int32_t>(verifier, VT_ELEMENT_SIZE, 4) &&
+           VerifyField<int8_t>(verifier, VT_MEMORY_TYPE, 1) &&
            VerifyOffset(verifier, VT_ATTRIBUTES) &&
            verifier.VerifyVector(attributes()) &&
            verifier.VerifyVectorOfStrings(attributes()) &&
-           VerifyField<int32_t>(verifier, VT_SIZE) &&
+           VerifyField<int32_t>(verifier, VT_SIZE, 4) &&
            VerifyOffset(verifier, VT_DATA) &&
            verifier.VerifyVector(data()) &&
            verifier.EndTable();
@@ -1107,9 +1137,9 @@ struct Texture2DDescriptor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_BASE_OBJ) &&
            verifier.VerifyTable(base_obj()) &&
-           VerifyField<int8_t>(verifier, VT_ELEMENT_TYPE) &&
-           VerifyField<uint8_t>(verifier, VT_NORMALIZED) &&
-           VerifyField<int8_t>(verifier, VT_NORMALIZED_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_ELEMENT_TYPE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_NORMALIZED, 1) &&
+           VerifyField<int8_t>(verifier, VT_NORMALIZED_TYPE, 1) &&
            VerifyOffset(verifier, VT_SIZE) &&
            verifier.VerifyTable(size()) &&
            VerifyOffset(verifier, VT_DATA) &&
@@ -1220,10 +1250,10 @@ struct TensorLinearDescriptor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_BASE_OBJ) &&
            verifier.VerifyTable(base_obj()) &&
-           VerifyField<int8_t>(verifier, VT_STORAGE_TYPE) &&
-           VerifyField<int8_t>(verifier, VT_ELEMENT_TYPE) &&
-           VerifyField<int8_t>(verifier, VT_MEMORY_TYPE) &&
-           VerifyField<int32_t>(verifier, VT_SIZE) &&
+           VerifyField<int8_t>(verifier, VT_STORAGE_TYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_ELEMENT_TYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_MEMORY_TYPE, 1) &&
+           VerifyField<int32_t>(verifier, VT_SIZE, 4) &&
            VerifyOffset(verifier, VT_DATA) &&
            verifier.VerifyVector(data()) &&
            verifier.EndTable();
@@ -1326,11 +1356,11 @@ struct BHWDC FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_B) &&
-           VerifyField<int32_t>(verifier, VT_H) &&
-           VerifyField<int32_t>(verifier, VT_W) &&
-           VerifyField<int32_t>(verifier, VT_D) &&
-           VerifyField<int32_t>(verifier, VT_C) &&
+           VerifyField<int32_t>(verifier, VT_B, 4) &&
+           VerifyField<int32_t>(verifier, VT_H, 4) &&
+           VerifyField<int32_t>(verifier, VT_W, 4) &&
+           VerifyField<int32_t>(verifier, VT_D, 4) &&
+           VerifyField<int32_t>(verifier, VT_C, 4) &&
            verifier.EndTable();
   }
 };
@@ -1418,17 +1448,18 @@ struct TensorDescriptor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return GetField<uint8_t>(VT_USE_BUFFER_FOR_WRITE_ONLY_IMAGE_BUFFER, 0) != 0;
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) && VerifyOffset(verifier, VT_BASE_OBJ) &&
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_BASE_OBJ) &&
            verifier.VerifyTable(base_obj()) &&
-           VerifyField<int8_t>(verifier, VT_DATA_TYPE) &&
-           VerifyField<int8_t>(verifier, VT_STORAGE_TYPE) &&
-           VerifyField<int8_t>(verifier, VT_LAYOUT) &&
-           VerifyOffset(verifier, VT_SHAPE) && verifier.VerifyTable(shape()) &&
-           VerifyOffset(verifier, VT_DATA) && verifier.VerifyVector(data()) &&
-           VerifyField<uint8_t>(verifier,
-                                VT_USE_BUFFER_FOR_WRITE_ONLY_2D_TEXTURE) &&
-           VerifyField<uint8_t>(verifier,
-                                VT_USE_BUFFER_FOR_WRITE_ONLY_IMAGE_BUFFER) &&
+           VerifyField<int8_t>(verifier, VT_DATA_TYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_STORAGE_TYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_LAYOUT, 1) &&
+           VerifyOffset(verifier, VT_SHAPE) &&
+           verifier.VerifyTable(shape()) &&
+           VerifyOffset(verifier, VT_DATA) &&
+           verifier.VerifyVector(data()) &&
+           VerifyField<uint8_t>(verifier, VT_USE_BUFFER_FOR_WRITE_ONLY_2D_TEXTURE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_USE_BUFFER_FOR_WRITE_ONLY_IMAGE_BUFFER, 1) &&
            verifier.EndTable();
   }
 };
@@ -1455,17 +1486,11 @@ struct TensorDescriptorBuilder {
   void add_data(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> data) {
     fbb_.AddOffset(TensorDescriptor::VT_DATA, data);
   }
-  void add_use_buffer_for_write_only_2d_texture(
-      bool use_buffer_for_write_only_2d_texture) {
-    fbb_.AddElement<uint8_t>(
-        TensorDescriptor::VT_USE_BUFFER_FOR_WRITE_ONLY_2D_TEXTURE,
-        static_cast<uint8_t>(use_buffer_for_write_only_2d_texture), 0);
+  void add_use_buffer_for_write_only_2d_texture(bool use_buffer_for_write_only_2d_texture) {
+    fbb_.AddElement<uint8_t>(TensorDescriptor::VT_USE_BUFFER_FOR_WRITE_ONLY_2D_TEXTURE, static_cast<uint8_t>(use_buffer_for_write_only_2d_texture), 0);
   }
-  void add_use_buffer_for_write_only_image_buffer(
-      bool use_buffer_for_write_only_image_buffer) {
-    fbb_.AddElement<uint8_t>(
-        TensorDescriptor::VT_USE_BUFFER_FOR_WRITE_ONLY_IMAGE_BUFFER,
-        static_cast<uint8_t>(use_buffer_for_write_only_image_buffer), 0);
+  void add_use_buffer_for_write_only_image_buffer(bool use_buffer_for_write_only_image_buffer) {
+    fbb_.AddElement<uint8_t>(TensorDescriptor::VT_USE_BUFFER_FOR_WRITE_ONLY_IMAGE_BUFFER, static_cast<uint8_t>(use_buffer_for_write_only_image_buffer), 0);
   }
   explicit TensorDescriptorBuilder(flatbuffers::FlatBufferBuilder &_fbb)
         : fbb_(_fbb) {
@@ -1481,10 +1506,8 @@ struct TensorDescriptorBuilder {
 inline flatbuffers::Offset<TensorDescriptor> CreateTensorDescriptor(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<tflite::gpu::data::GPUObjectDescriptor> base_obj = 0,
-    tflite::gpu::data::DataType data_type =
-        tflite::gpu::data::DataType::UNKNOWN,
-    tflite::gpu::data::TensorStorageType storage_type =
-        tflite::gpu::data::TensorStorageType::UNKNOWN,
+    tflite::gpu::data::DataType data_type = tflite::gpu::data::DataType::UNKNOWN,
+    tflite::gpu::data::TensorStorageType storage_type = tflite::gpu::data::TensorStorageType::UNKNOWN,
     tflite::gpu::data::Layout layout = tflite::gpu::data::Layout::UNKNOWN,
     flatbuffers::Offset<tflite::gpu::data::BHWDC> shape = 0,
     flatbuffers::Offset<flatbuffers::Vector<uint8_t>> data = 0,
@@ -1494,10 +1517,8 @@ inline flatbuffers::Offset<TensorDescriptor> CreateTensorDescriptor(
   builder_.add_data(data);
   builder_.add_shape(shape);
   builder_.add_base_obj(base_obj);
-  builder_.add_use_buffer_for_write_only_image_buffer(
-      use_buffer_for_write_only_image_buffer);
-  builder_.add_use_buffer_for_write_only_2d_texture(
-      use_buffer_for_write_only_2d_texture);
+  builder_.add_use_buffer_for_write_only_image_buffer(use_buffer_for_write_only_image_buffer);
+  builder_.add_use_buffer_for_write_only_2d_texture(use_buffer_for_write_only_2d_texture);
   builder_.add_layout(layout);
   builder_.add_storage_type(storage_type);
   builder_.add_data_type(data_type);
@@ -1507,10 +1528,8 @@ inline flatbuffers::Offset<TensorDescriptor> CreateTensorDescriptor(
 inline flatbuffers::Offset<TensorDescriptor> CreateTensorDescriptorDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
     flatbuffers::Offset<tflite::gpu::data::GPUObjectDescriptor> base_obj = 0,
-    tflite::gpu::data::DataType data_type =
-        tflite::gpu::data::DataType::UNKNOWN,
-    tflite::gpu::data::TensorStorageType storage_type =
-        tflite::gpu::data::TensorStorageType::UNKNOWN,
+    tflite::gpu::data::DataType data_type = tflite::gpu::data::DataType::UNKNOWN,
+    tflite::gpu::data::TensorStorageType storage_type = tflite::gpu::data::TensorStorageType::UNKNOWN,
     tflite::gpu::data::Layout layout = tflite::gpu::data::Layout::UNKNOWN,
     flatbuffers::Offset<tflite::gpu::data::BHWDC> shape = 0,
     const std::vector<uint8_t> *data = nullptr,
@@ -1518,7 +1537,13 @@ inline flatbuffers::Offset<TensorDescriptor> CreateTensorDescriptorDirect(
     bool use_buffer_for_write_only_image_buffer = false) {
   auto data__ = data ? _fbb.CreateVector<uint8_t>(*data) : 0;
   return tflite::gpu::data::CreateTensorDescriptor(
-      _fbb, base_obj, data_type, storage_type, layout, shape, data__,
+      _fbb,
+      base_obj,
+      data_type,
+      storage_type,
+      layout,
+      shape,
+      data__,
       use_buffer_for_write_only_2d_texture,
       use_buffer_for_write_only_image_buffer);
 }
@@ -1989,32 +2014,24 @@ struct OperationDef FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_DST_TENSORS = 8
   };
   tflite::gpu::data::CalculationsPrecision precision() const {
-    return static_cast<tflite::gpu::data::CalculationsPrecision>(
-        GetField<int8_t>(VT_PRECISION, 0));
-  }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>
-      *src_tensors() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *>(
-        VT_SRC_TENSORS);
-  }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>
-      *dst_tensors() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *>(
-        VT_DST_TENSORS);
+    return static_cast<tflite::gpu::data::CalculationsPrecision>(GetField<int8_t>(VT_PRECISION, 0));
+  }
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *src_tensors() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *>(VT_SRC_TENSORS);
+  }
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *dst_tensors() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *>(VT_DST_TENSORS);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_PRECISION) &&
+           VerifyField<int8_t>(verifier, VT_PRECISION, 1) &&
            VerifyOffset(verifier, VT_SRC_TENSORS) &&
            verifier.VerifyVector(src_tensors()) &&
            verifier.VerifyVectorOfTables(src_tensors()) &&
            VerifyOffset(verifier, VT_DST_TENSORS) &&
            verifier.VerifyVector(dst_tensors()) &&
-           verifier.VerifyVectorOfTables(dst_tensors()) && verifier.EndTable();
+           verifier.VerifyVectorOfTables(dst_tensors()) &&
+           verifier.EndTable();
   }
 };
 
@@ -2023,23 +2040,16 @@ struct OperationDefBuilder {
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_precision(tflite::gpu::data::CalculationsPrecision precision) {
-    fbb_.AddElement<int8_t>(OperationDef::VT_PRECISION,
-                            static_cast<int8_t>(precision), 0);
+    fbb_.AddElement<int8_t>(OperationDef::VT_PRECISION, static_cast<int8_t>(precision), 0);
   }
-  void add_src_tensors(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>>
-          src_tensors) {
+  void add_src_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>> src_tensors) {
     fbb_.AddOffset(OperationDef::VT_SRC_TENSORS, src_tensors);
   }
-  void add_dst_tensors(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>>
-          dst_tensors) {
+  void add_dst_tensors(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>> dst_tensors) {
     fbb_.AddOffset(OperationDef::VT_DST_TENSORS, dst_tensors);
   }
   explicit OperationDefBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-      : fbb_(_fbb) {
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<OperationDef> Finish() {
@@ -2051,14 +2061,9 @@ struct OperationDefBuilder {
 
 inline flatbuffers::Offset<OperationDef> CreateOperationDef(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::gpu::data::CalculationsPrecision precision =
-        tflite::gpu::data::CalculationsPrecision::F32,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>>
-        src_tensors = 0,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>>
-        dst_tensors = 0) {
+    tflite::gpu::data::CalculationsPrecision precision = tflite::gpu::data::CalculationsPrecision::F32,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>> src_tensors = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>> dst_tensors = 0) {
   OperationDefBuilder builder_(_fbb);
   builder_.add_dst_tensors(dst_tensors);
   builder_.add_src_tensors(src_tensors);
@@ -2068,26 +2073,16 @@ inline flatbuffers::Offset<OperationDef> CreateOperationDef(
 
 inline flatbuffers::Offset<OperationDef> CreateOperationDefDirect(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::gpu::data::CalculationsPrecision precision =
-        tflite::gpu::data::CalculationsPrecision::F32,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>
-        *src_tensors = nullptr,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>
-        *dst_tensors = nullptr) {
-  auto src_tensors__ =
-      src_tensors
-          ? _fbb.CreateVector<
-                flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>(
-                *src_tensors)
-          : 0;
-  auto dst_tensors__ =
-      dst_tensors
-          ? _fbb.CreateVector<
-                flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>(
-                *dst_tensors)
-          : 0;
-  return tflite::gpu::data::CreateOperationDef(_fbb, precision, src_tensors__,
-                                               dst_tensors__);
+    tflite::gpu::data::CalculationsPrecision precision = tflite::gpu::data::CalculationsPrecision::F32,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *src_tensors = nullptr,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>> *dst_tensors = nullptr) {
+  auto src_tensors__ = src_tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>(*src_tensors) : 0;
+  auto dst_tensors__ = dst_tensors ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::TensorDescriptor>>(*dst_tensors) : 0;
+  return tflite::gpu::data::CreateOperationDef(
+      _fbb,
+      precision,
+      src_tensors__,
+      dst_tensors__);
 }
 
 struct CompilerOption FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -2096,12 +2091,12 @@ struct CompilerOption FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     VT_OPTION = 4
   };
   tflite::gpu::data::CompilerOptions option() const {
-    return static_cast<tflite::gpu::data::CompilerOptions>(
-        GetField<int8_t>(VT_OPTION, 0));
+    return static_cast<tflite::gpu::data::CompilerOptions>(GetField<int8_t>(VT_OPTION, 0));
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_OPTION) && verifier.EndTable();
+           VerifyField<int8_t>(verifier, VT_OPTION, 1) &&
+           verifier.EndTable();
   }
 };
 
@@ -2110,11 +2105,10 @@ struct CompilerOptionBuilder {
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_option(tflite::gpu::data::CompilerOptions option) {
-    fbb_.AddElement<int8_t>(CompilerOption::VT_OPTION,
-                            static_cast<int8_t>(option), 0);
+    fbb_.AddElement<int8_t>(CompilerOption::VT_OPTION, static_cast<int8_t>(option), 0);
   }
   explicit CompilerOptionBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-      : fbb_(_fbb) {
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<CompilerOption> Finish() {
@@ -2126,8 +2120,7 @@ struct CompilerOptionBuilder {
 
 inline flatbuffers::Offset<CompilerOption> CreateCompilerOption(
     flatbuffers::FlatBufferBuilder &_fbb,
-    tflite::gpu::data::CompilerOptions option =
-        tflite::gpu::data::CompilerOptions::ADRENO_FULL_SIMD_LINE) {
+    tflite::gpu::data::CompilerOptions option = tflite::gpu::data::CompilerOptions::ADRENO_FULL_SIMD_LINE) {
   CompilerOptionBuilder builder_(_fbb);
   builder_.add_option(option);
   return builder_.Finish();
@@ -2164,23 +2157,24 @@ struct GPUOperation FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   const tflite::gpu::data::Int3 *work_group_size() const {
     return GetPointer<const tflite::gpu::data::Int3 *>(VT_WORK_GROUP_SIZE);
   }
-  const flatbuffers::Vector<
-      flatbuffers::Offset<tflite::gpu::data::CompilerOption>>
-      *compiler_options() const {
-    return GetPointer<const flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::CompilerOption>> *>(
-        VT_COMPILER_OPTIONS);
+  const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>> *compiler_options() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>> *>(VT_COMPILER_OPTIONS);
   }
   tflite::gpu::data::TensorToGrid tensor_to_grid() const {
-    return static_cast<tflite::gpu::data::TensorToGrid>(
-        GetField<int8_t>(VT_TENSOR_TO_GRID, 0));
+    return static_cast<tflite::gpu::data::TensorToGrid>(GetField<int8_t>(VT_TENSOR_TO_GRID, 0));
+  }
+  bool elementwise() const {
+    return GetField<uint8_t>(VT_ELEMENTWISE, 0) != 0;
+  }
+  bool linkable() const {
+    return GetField<uint8_t>(VT_LINKABLE, 0) != 0;
   }
-  bool elementwise() const { return GetField<uint8_t>(VT_ELEMENTWISE, 0) != 0; }
-  bool linkable() const { return GetField<uint8_t>(VT_LINKABLE, 0) != 0; }
   bool check_src_channels_size() const {
     return GetField<uint8_t>(VT_CHECK_SRC_CHANNELS_SIZE, 0) != 0;
   }
-  uint64_t flops() const { return GetField<uint64_t>(VT_FLOPS, 0); }
+  uint64_t flops() const {
+    return GetField<uint64_t>(VT_FLOPS, 0);
+  }
   const tflite::gpu::data::OperationDef *definition() const {
     return GetPointer<const tflite::gpu::data::OperationDef *>(VT_DEFINITION);
   }
@@ -2188,23 +2182,16 @@ struct GPUOperation FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return GetField<int32_t>(VT_GRID_DIMENSION, 0);
   }
   const tflite::gpu::data::Int3 *work_group_launch_order() const {
-    return GetPointer<const tflite::gpu::data::Int3 *>(
-        VT_WORK_GROUP_LAUNCH_ORDER);
+    return GetPointer<const tflite::gpu::data::Int3 *>(VT_WORK_GROUP_LAUNCH_ORDER);
   }
   const tflite::gpu::data::Int3 *grid_size() const {
     return GetPointer<const tflite::gpu::data::Int3 *>(VT_GRID_SIZE);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>
-      *src_tensors_names() const {
-    return GetPointer<
-        const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>> *>(
-        VT_SRC_TENSORS_NAMES);
+  const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>> *src_tensors_names() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>> *>(VT_SRC_TENSORS_NAMES);
   }
-  const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>
-      *dst_tensors_names() const {
-    return GetPointer<
-        const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>> *>(
-        VT_DST_TENSORS_NAMES);
+  const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>> *dst_tensors_names() const {
+    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>> *>(VT_DST_TENSORS_NAMES);
   }
   const tflite::gpu::data::Int3 *work_groups_count() const {
     return GetPointer<const tflite::gpu::data::Int3 *>(VT_WORK_GROUPS_COUNT);
@@ -2216,22 +2203,24 @@ struct GPUOperation FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return GetPointer<const flatbuffers::String *>(VT_ELEMENTWISE_CODE);
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
-    return VerifyTableStart(verifier) && VerifyOffset(verifier, VT_ARGUMENTS) &&
+    return VerifyTableStart(verifier) &&
+           VerifyOffset(verifier, VT_ARGUMENTS) &&
            verifier.VerifyTable(arguments()) &&
-           VerifyOffset(verifier, VT_CODE) && verifier.VerifyString(code()) &&
+           VerifyOffset(verifier, VT_CODE) &&
+           verifier.VerifyString(code()) &&
            VerifyOffset(verifier, VT_WORK_GROUP_SIZE) &&
            verifier.VerifyTable(work_group_size()) &&
            VerifyOffset(verifier, VT_COMPILER_OPTIONS) &&
            verifier.VerifyVector(compiler_options()) &&
            verifier.VerifyVectorOfTables(compiler_options()) &&
-           VerifyField<int8_t>(verifier, VT_TENSOR_TO_GRID) &&
-           VerifyField<uint8_t>(verifier, VT_ELEMENTWISE) &&
-           VerifyField<uint8_t>(verifier, VT_LINKABLE) &&
-           VerifyField<uint8_t>(verifier, VT_CHECK_SRC_CHANNELS_SIZE) &&
-           VerifyField<uint64_t>(verifier, VT_FLOPS) &&
+           VerifyField<int8_t>(verifier, VT_TENSOR_TO_GRID, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ELEMENTWISE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_LINKABLE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_CHECK_SRC_CHANNELS_SIZE, 1) &&
+           VerifyField<uint64_t>(verifier, VT_FLOPS, 8) &&
            VerifyOffset(verifier, VT_DEFINITION) &&
            verifier.VerifyTable(definition()) &&
-           VerifyField<int32_t>(verifier, VT_GRID_DIMENSION) &&
+           VerifyField<int32_t>(verifier, VT_GRID_DIMENSION, 4) &&
            VerifyOffset(verifier, VT_WORK_GROUP_LAUNCH_ORDER) &&
            verifier.VerifyTable(work_group_launch_order()) &&
            VerifyOffset(verifier, VT_GRID_SIZE) &&
@@ -2244,9 +2233,10 @@ struct GPUOperation FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
            verifier.VerifyVectorOfStrings(dst_tensors_names()) &&
            VerifyOffset(verifier, VT_WORK_GROUPS_COUNT) &&
            verifier.VerifyTable(work_groups_count()) &&
-           VerifyField<int32_t>(verifier, VT_LINKABLE_COUNT) &&
+           VerifyField<int32_t>(verifier, VT_LINKABLE_COUNT, 4) &&
            VerifyOffset(verifier, VT_ELEMENTWISE_CODE) &&
-           verifier.VerifyString(elementwise_code()) && verifier.EndTable();
+           verifier.VerifyString(elementwise_code()) &&
+           verifier.EndTable();
   }
 };
 
@@ -2254,84 +2244,62 @@ struct GPUOperationBuilder {
   typedef GPUOperation Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
-  void add_arguments(
-      flatbuffers::Offset<tflite::gpu::data::Arguments> arguments) {
+  void add_arguments(flatbuffers::Offset<tflite::gpu::data::Arguments> arguments) {
     fbb_.AddOffset(GPUOperation::VT_ARGUMENTS, arguments);
   }
   void add_code(flatbuffers::Offset<flatbuffers::String> code) {
     fbb_.AddOffset(GPUOperation::VT_CODE, code);
   }
-  void add_work_group_size(
-      flatbuffers::Offset<tflite::gpu::data::Int3> work_group_size) {
+  void add_work_group_size(flatbuffers::Offset<tflite::gpu::data::Int3> work_group_size) {
     fbb_.AddOffset(GPUOperation::VT_WORK_GROUP_SIZE, work_group_size);
   }
-  void add_compiler_options(
-      flatbuffers::Offset<flatbuffers::Vector<
-          flatbuffers::Offset<tflite::gpu::data::CompilerOption>>>
-          compiler_options) {
+  void add_compiler_options(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>>> compiler_options) {
     fbb_.AddOffset(GPUOperation::VT_COMPILER_OPTIONS, compiler_options);
   }
   void add_tensor_to_grid(tflite::gpu::data::TensorToGrid tensor_to_grid) {
-    fbb_.AddElement<int8_t>(GPUOperation::VT_TENSOR_TO_GRID,
-                            static_cast<int8_t>(tensor_to_grid), 0);
+    fbb_.AddElement<int8_t>(GPUOperation::VT_TENSOR_TO_GRID, static_cast<int8_t>(tensor_to_grid), 0);
   }
   void add_elementwise(bool elementwise) {
-    fbb_.AddElement<uint8_t>(GPUOperation::VT_ELEMENTWISE,
-                             static_cast<uint8_t>(elementwise), 0);
+    fbb_.AddElement<uint8_t>(GPUOperation::VT_ELEMENTWISE, static_cast<uint8_t>(elementwise), 0);
   }
   void add_linkable(bool linkable) {
-    fbb_.AddElement<uint8_t>(GPUOperation::VT_LINKABLE,
-                             static_cast<uint8_t>(linkable), 0);
+    fbb_.AddElement<uint8_t>(GPUOperation::VT_LINKABLE, static_cast<uint8_t>(linkable), 0);
   }
   void add_check_src_channels_size(bool check_src_channels_size) {
-    fbb_.AddElement<uint8_t>(GPUOperation::VT_CHECK_SRC_CHANNELS_SIZE,
-                             static_cast<uint8_t>(check_src_channels_size), 0);
+    fbb_.AddElement<uint8_t>(GPUOperation::VT_CHECK_SRC_CHANNELS_SIZE, static_cast<uint8_t>(check_src_channels_size), 0);
   }
   void add_flops(uint64_t flops) {
     fbb_.AddElement<uint64_t>(GPUOperation::VT_FLOPS, flops, 0);
   }
-  void add_definition(
-      flatbuffers::Offset<tflite::gpu::data::OperationDef> definition) {
+  void add_definition(flatbuffers::Offset<tflite::gpu::data::OperationDef> definition) {
     fbb_.AddOffset(GPUOperation::VT_DEFINITION, definition);
   }
   void add_grid_dimension(int32_t grid_dimension) {
-    fbb_.AddElement<int32_t>(GPUOperation::VT_GRID_DIMENSION, grid_dimension,
-                             0);
+    fbb_.AddElement<int32_t>(GPUOperation::VT_GRID_DIMENSION, grid_dimension, 0);
   }
-  void add_work_group_launch_order(
-      flatbuffers::Offset<tflite::gpu::data::Int3> work_group_launch_order) {
-    fbb_.AddOffset(GPUOperation::VT_WORK_GROUP_LAUNCH_ORDER,
-                   work_group_launch_order);
+  void add_work_group_launch_order(flatbuffers::Offset<tflite::gpu::data::Int3> work_group_launch_order) {
+    fbb_.AddOffset(GPUOperation::VT_WORK_GROUP_LAUNCH_ORDER, work_group_launch_order);
   }
   void add_grid_size(flatbuffers::Offset<tflite::gpu::data::Int3> grid_size) {
     fbb_.AddOffset(GPUOperation::VT_GRID_SIZE, grid_size);
   }
-  void add_src_tensors_names(
-      flatbuffers::Offset<
-          flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>>
-          src_tensors_names) {
+  void add_src_tensors_names(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>> src_tensors_names) {
     fbb_.AddOffset(GPUOperation::VT_SRC_TENSORS_NAMES, src_tensors_names);
   }
-  void add_dst_tensors_names(
-      flatbuffers::Offset<
-          flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>>
-          dst_tensors_names) {
+  void add_dst_tensors_names(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>> dst_tensors_names) {
     fbb_.AddOffset(GPUOperation::VT_DST_TENSORS_NAMES, dst_tensors_names);
   }
-  void add_work_groups_count(
-      flatbuffers::Offset<tflite::gpu::data::Int3> work_groups_count) {
+  void add_work_groups_count(flatbuffers::Offset<tflite::gpu::data::Int3> work_groups_count) {
     fbb_.AddOffset(GPUOperation::VT_WORK_GROUPS_COUNT, work_groups_count);
   }
   void add_linkable_count(int32_t linkable_count) {
-    fbb_.AddElement<int32_t>(GPUOperation::VT_LINKABLE_COUNT, linkable_count,
-                             0);
+    fbb_.AddElement<int32_t>(GPUOperation::VT_LINKABLE_COUNT, linkable_count, 0);
   }
-  void add_elementwise_code(
-      flatbuffers::Offset<flatbuffers::String> elementwise_code) {
+  void add_elementwise_code(flatbuffers::Offset<flatbuffers::String> elementwise_code) {
     fbb_.AddOffset(GPUOperation::VT_ELEMENTWISE_CODE, elementwise_code);
   }
   explicit GPUOperationBuilder(flatbuffers::FlatBufferBuilder &_fbb)
-      : fbb_(_fbb) {
+        : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
   flatbuffers::Offset<GPUOperation> Finish() {
@@ -2346,23 +2314,18 @@ inline flatbuffers::Offset<GPUOperation> CreateGPUOperation(
     flatbuffers::Offset<tflite::gpu::data::Arguments> arguments = 0,
     flatbuffers::Offset<flatbuffers::String> code = 0,
     flatbuffers::Offset<tflite::gpu::data::Int3> work_group_size = 0,
-    flatbuffers::Offset<flatbuffers::Vector<
-        flatbuffers::Offset<tflite::gpu::data::CompilerOption>>>
-        compiler_options = 0,
-    tflite::gpu::data::TensorToGrid tensor_to_grid =
-        tflite::gpu::data::TensorToGrid::CUSTOM,
-    bool elementwise = false, bool linkable = false,
-    bool check_src_channels_size = false, uint64_t flops = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>>> compiler_options = 0,
+    tflite::gpu::data::TensorToGrid tensor_to_grid = tflite::gpu::data::TensorToGrid::CUSTOM,
+    bool elementwise = false,
+    bool linkable = false,
+    bool check_src_channels_size = false,
+    uint64_t flops = 0,
     flatbuffers::Offset<tflite::gpu::data::OperationDef> definition = 0,
     int32_t grid_dimension = 0,
     flatbuffers::Offset<tflite::gpu::data::Int3> work_group_launch_order = 0,
     flatbuffers::Offset<tflite::gpu::data::Int3> grid_size = 0,
-    flatbuffers::Offset<
-        flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>>
-        src_tensors_names = 0,
-    flatbuffers::Offset<
-        flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>>
-        dst_tensors_names = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>> src_tensors_names = 0,
+    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>> dst_tensors_names = 0,
     flatbuffers::Offset<tflite::gpu::data::Int3> work_groups_count = 0,
     int32_t linkable_count = 0,
     flatbuffers::Offset<flatbuffers::String> elementwise_code = 0) {
@@ -2393,47 +2356,46 @@ inline flatbuffers::Offset<GPUOperation> CreateGPUOperationDirect(
     flatbuffers::Offset<tflite::gpu::data::Arguments> arguments = 0,
     const char *code = nullptr,
     flatbuffers::Offset<tflite::gpu::data::Int3> work_group_size = 0,
-    const std::vector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>>
-        *compiler_options = nullptr,
-    tflite::gpu::data::TensorToGrid tensor_to_grid =
-        tflite::gpu::data::TensorToGrid::CUSTOM,
-    bool elementwise = false, bool linkable = false,
-    bool check_src_channels_size = false, uint64_t flops = 0,
+    const std::vector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>> *compiler_options = nullptr,
+    tflite::gpu::data::TensorToGrid tensor_to_grid = tflite::gpu::data::TensorToGrid::CUSTOM,
+    bool elementwise = false,
+    bool linkable = false,
+    bool check_src_channels_size = false,
+    uint64_t flops = 0,
     flatbuffers::Offset<tflite::gpu::data::OperationDef> definition = 0,
     int32_t grid_dimension = 0,
     flatbuffers::Offset<tflite::gpu::data::Int3> work_group_launch_order = 0,
     flatbuffers::Offset<tflite::gpu::data::Int3> grid_size = 0,
-    const std::vector<flatbuffers::Offset<flatbuffers::String>>
-        *src_tensors_names = nullptr,
-    const std::vector<flatbuffers::Offset<flatbuffers::String>>
-        *dst_tensors_names = nullptr,
+    const std::vector<flatbuffers::Offset<flatbuffers::String>> *src_tensors_names = nullptr,
+    const std::vector<flatbuffers::Offset<flatbuffers::String>> *dst_tensors_names = nullptr,
     flatbuffers::Offset<tflite::gpu::data::Int3> work_groups_count = 0,
-    int32_t linkable_count = 0, const char *elementwise_code = nullptr) {
+    int32_t linkable_count = 0,
+    const char *elementwise_code = nullptr) {
   auto code__ = code ? _fbb.CreateString(code) : 0;
-  auto compiler_options__ =
-      compiler_options
-          ? _fbb.CreateVector<
-                flatbuffers::Offset<tflite::gpu::data::CompilerOption>>(
-                *compiler_options)
-          : 0;
-  auto src_tensors_names__ =
-      src_tensors_names
-          ? _fbb.CreateVector<flatbuffers::Offset<flatbuffers::String>>(
-                *src_tensors_names)
-          : 0;
-  auto dst_tensors_names__ =
-      dst_tensors_names
-          ? _fbb.CreateVector<flatbuffers::Offset<flatbuffers::String>>(
-                *dst_tensors_names)
-          : 0;
-  auto elementwise_code__ =
-      elementwise_code ? _fbb.CreateString(elementwise_code) : 0;
+  auto compiler_options__ = compiler_options ? _fbb.CreateVector<flatbuffers::Offset<tflite::gpu::data::CompilerOption>>(*compiler_options) : 0;
+  auto src_tensors_names__ = src_tensors_names ? _fbb.CreateVector<flatbuffers::Offset<flatbuffers::String>>(*src_tensors_names) : 0;
+  auto dst_tensors_names__ = dst_tensors_names ? _fbb.CreateVector<flatbuffers::Offset<flatbuffers::String>>(*dst_tensors_names) : 0;
+  auto elementwise_code__ = elementwise_code ? _fbb.CreateString(elementwise_code) : 0;
   return tflite::gpu::data::CreateGPUOperation(
-      _fbb, arguments, code__, work_group_size, compiler_options__,
-      tensor_to_grid, elementwise, linkable, check_src_channels_size, flops,
-      definition, grid_dimension, work_group_launch_order, grid_size,
-      src_tensors_names__, dst_tensors_names__, work_groups_count,
-      linkable_count, elementwise_code__);
+      _fbb,
+      arguments,
+      code__,
+      work_group_size,
+      compiler_options__,
+      tensor_to_grid,
+      elementwise,
+      linkable,
+      check_src_channels_size,
+      flops,
+      definition,
+      grid_dimension,
+      work_group_launch_order,
+      grid_size,
+      src_tensors_names__,
+      dst_tensors_names__,
+      work_groups_count,
+      linkable_count,
+      elementwise_code__);
 }
 
 }  // namespace data
diff --git a/tensorflow/lite/experimental/acceleration/configuration/BUILD b/tensorflow/lite/experimental/acceleration/configuration/BUILD
index 4aa77261555..54fa181e5c2 100644
--- a/tensorflow/lite/experimental/acceleration/configuration/BUILD
+++ b/tensorflow/lite/experimental/acceleration/configuration/BUILD
@@ -83,31 +83,11 @@ tf_proto_library_py(
 )
 # copybara:comment_end
 
-# TODO(b/191428000): The automatic generation is temporarily disabled until
-# https://github.com/google/flatbuffers/pull/6486 is landed and tensorflow
-# flatbuffer version updated.
-# Until that time you need to build configuration_for_generation_generated.h
-# and copy the result to configuration_generated.h.
-genrule(
-    name = "copy_configuration_fbs",
-    srcs = ["configuration.fbs"],
-    outs = ["configuration_for_generation.fbs"],
-    cmd = "cp $(<) $(@)",
-)
-
 flatbuffer_cc_library(
-    name = "configuration_fbs_for_generation",
-    srcs = [":configuration_for_generation.fbs"],
-    flatc_args = DEFAULT_FLATC_ARGS + ["--gen-compare"],
-    visibility = ["//visibility:private"],
-)
-
-cc_library(
     name = "configuration_fbs",
-    hdrs = ["configuration_generated.h"],
+    srcs = [":configuration.fbs"],
     compatible_with = get_compatible_with_portable(),
-    linkstatic = True,
-    deps = ["@flatbuffers//:runtime_cc"],
+    flatc_args = DEFAULT_FLATC_ARGS + ["--gen-compare"],
 )
 
 flatbuffer_java_library(
diff --git a/tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h b/tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h
old mode 100644
new mode 100755
index 40b0f0322bc..1491cb9ff83
--- a/tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h
+++ b/tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h
@@ -12,86 +12,109 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
-// clang-format off
 // automatically generated by the FlatBuffers compiler, do not modify
 
-#ifndef FLATBUFFERS_GENERATED_CONFIGURATIONFORGENERATION_TFLITE_H_
-#define FLATBUFFERS_GENERATED_CONFIGURATIONFORGENERATION_TFLITE_H_
+#ifndef FLATBUFFERS_GENERATED_CONFIGURATION_TFLITE_H_
+#define FLATBUFFERS_GENERATED_CONFIGURATION_TFLITE_H_
 
 #include "flatbuffers/flatbuffers.h"
 
 namespace tflite {
 
 struct ComputeSettings;
+struct ComputeSettingsBuilder;
 struct ComputeSettingsT;
 
 struct NNAPISettings;
+struct NNAPISettingsBuilder;
 struct NNAPISettingsT;
 
 struct GPUSettings;
+struct GPUSettingsBuilder;
 struct GPUSettingsT;
 
 struct HexagonSettings;
+struct HexagonSettingsBuilder;
 struct HexagonSettingsT;
 
 struct XNNPackSettings;
+struct XNNPackSettingsBuilder;
 struct XNNPackSettingsT;
 
 struct CoreMLSettings;
+struct CoreMLSettingsBuilder;
 struct CoreMLSettingsT;
 
 struct EdgeTpuDeviceSpec;
+struct EdgeTpuDeviceSpecBuilder;
 struct EdgeTpuDeviceSpecT;
 
 struct EdgeTpuInactivePowerConfig;
+struct EdgeTpuInactivePowerConfigBuilder;
 struct EdgeTpuInactivePowerConfigT;
 
 struct EdgeTpuSettings;
+struct EdgeTpuSettingsBuilder;
 struct EdgeTpuSettingsT;
 
 struct CoralSettings;
+struct CoralSettingsBuilder;
 struct CoralSettingsT;
 
 struct CPUSettings;
+struct CPUSettingsBuilder;
 struct CPUSettingsT;
 
 struct TFLiteSettings;
+struct TFLiteSettingsBuilder;
 struct TFLiteSettingsT;
 
 struct FallbackSettings;
+struct FallbackSettingsBuilder;
 struct FallbackSettingsT;
 
 struct BenchmarkMetric;
+struct BenchmarkMetricBuilder;
 struct BenchmarkMetricT;
 
 struct BenchmarkResult;
+struct BenchmarkResultBuilder;
 struct BenchmarkResultT;
 
 struct ErrorCode;
+struct ErrorCodeBuilder;
 struct ErrorCodeT;
 
 struct BenchmarkError;
+struct BenchmarkErrorBuilder;
 struct BenchmarkErrorT;
 
 struct BenchmarkEvent;
+struct BenchmarkEventBuilder;
 struct BenchmarkEventT;
 
 struct BestAccelerationDecision;
+struct BestAccelerationDecisionBuilder;
 struct BestAccelerationDecisionT;
 
 struct BenchmarkInitializationFailure;
+struct BenchmarkInitializationFailureBuilder;
 struct BenchmarkInitializationFailureT;
 
 struct MiniBenchmarkEvent;
+struct MiniBenchmarkEventBuilder;
 struct MiniBenchmarkEventT;
 
 struct ModelFile;
+struct ModelFileBuilder;
 struct ModelFileT;
 
 struct BenchmarkStoragePaths;
+struct BenchmarkStoragePathsBuilder;
 struct BenchmarkStoragePathsT;
 
 struct MinibenchmarkSettings;
+struct MinibenchmarkSettingsBuilder;
 struct MinibenchmarkSettingsT;
 
 bool operator==(const ComputeSettingsT &lhs, const ComputeSettingsT &rhs);
@@ -143,7 +166,7 @@ bool operator!=(const BenchmarkStoragePathsT &lhs, const BenchmarkStoragePathsT
 bool operator==(const MinibenchmarkSettingsT &lhs, const MinibenchmarkSettingsT &rhs);
 bool operator!=(const MinibenchmarkSettingsT &lhs, const MinibenchmarkSettingsT &rhs);
 
-enum ExecutionPreference {
+enum ExecutionPreference : int32_t {
   ExecutionPreference_ANY = 0,
   ExecutionPreference_LOW_LATENCY = 1,
   ExecutionPreference_LOW_POWER = 2,
@@ -179,7 +202,7 @@ inline const char *EnumNameExecutionPreference(ExecutionPreference e) {
   return EnumNamesExecutionPreference()[index];
 }
 
-enum Delegate {
+enum Delegate : int32_t {
   Delegate_NONE = 0,
   Delegate_NNAPI = 1,
   Delegate_GPU = 2,
@@ -227,7 +250,7 @@ inline const char *EnumNameDelegate(Delegate e) {
   return EnumNamesDelegate()[index];
 }
 
-enum NNAPIExecutionPreference {
+enum NNAPIExecutionPreference : int32_t {
   NNAPIExecutionPreference_UNDEFINED = 0,
   NNAPIExecutionPreference_NNAPI_LOW_POWER = 1,
   NNAPIExecutionPreference_NNAPI_FAST_SINGLE_ANSWER = 2,
@@ -263,7 +286,7 @@ inline const char *EnumNameNNAPIExecutionPreference(NNAPIExecutionPreference e)
   return EnumNamesNNAPIExecutionPreference()[index];
 }
 
-enum NNAPIExecutionPriority {
+enum NNAPIExecutionPriority : int32_t {
   NNAPIExecutionPriority_NNAPI_PRIORITY_UNDEFINED = 0,
   NNAPIExecutionPriority_NNAPI_PRIORITY_LOW = 1,
   NNAPIExecutionPriority_NNAPI_PRIORITY_MEDIUM = 2,
@@ -299,7 +322,7 @@ inline const char *EnumNameNNAPIExecutionPriority(NNAPIExecutionPriority e) {
   return EnumNamesNNAPIExecutionPriority()[index];
 }
 
-enum GPUBackend {
+enum GPUBackend : int32_t {
   GPUBackend_UNSET = 0,
   GPUBackend_OPENCL = 1,
   GPUBackend_OPENGL = 2,
@@ -332,7 +355,7 @@ inline const char *EnumNameGPUBackend(GPUBackend e) {
   return EnumNamesGPUBackend()[index];
 }
 
-enum GPUInferencePriority {
+enum GPUInferencePriority : int32_t {
   GPUInferencePriority_GPU_PRIORITY_AUTO = 0,
   GPUInferencePriority_GPU_PRIORITY_MAX_PRECISION = 1,
   GPUInferencePriority_GPU_PRIORITY_MIN_LATENCY = 2,
@@ -368,7 +391,7 @@ inline const char *EnumNameGPUInferencePriority(GPUInferencePriority e) {
   return EnumNamesGPUInferencePriority()[index];
 }
 
-enum GPUInferenceUsage {
+enum GPUInferenceUsage : int32_t {
   GPUInferenceUsage_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER = 0,
   GPUInferenceUsage_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED = 1,
   GPUInferenceUsage_MIN = GPUInferenceUsage_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,
@@ -400,7 +423,7 @@ inline const char *EnumNameGPUInferenceUsage(GPUInferenceUsage e) {
 
 namespace CoreMLSettings_ {
 
-enum EnabledDevices {
+enum EnabledDevices : int32_t {
   EnabledDevices_DEVICES_ALL = 0,
   EnabledDevices_DEVICES_WITH_NEURAL_ENGINE = 1,
   EnabledDevices_MIN = EnabledDevices_DEVICES_ALL,
@@ -434,7 +457,7 @@ inline const char *EnumNameEnabledDevices(EnabledDevices e) {
 
 namespace EdgeTpuDeviceSpec_ {
 
-enum PlatformType {
+enum PlatformType : int32_t {
   PlatformType_MMIO = 0,
   PlatformType_REFERENCE = 1,
   PlatformType_SIMULATOR = 2,
@@ -472,7 +495,7 @@ inline const char *EnumNamePlatformType(PlatformType e) {
 
 }  // namespace EdgeTpuDeviceSpec_
 
-enum EdgeTpuPowerState {
+enum EdgeTpuPowerState : int32_t {
   EdgeTpuPowerState_UNDEFINED_POWERSTATE = 0,
   EdgeTpuPowerState_TPU_CORE_OFF = 1,
   EdgeTpuPowerState_READY = 2,
@@ -522,7 +545,7 @@ inline const char *EnumNameEdgeTpuPowerState(EdgeTpuPowerState e) {
 
 namespace EdgeTpuSettings_ {
 
-enum FloatTruncationType {
+enum FloatTruncationType : int32_t {
   FloatTruncationType_UNSPECIFIED = 0,
   FloatTruncationType_NO_TRUNCATION = 1,
   FloatTruncationType_BFLOAT16 = 2,
@@ -558,7 +581,7 @@ inline const char *EnumNameFloatTruncationType(FloatTruncationType e) {
   return EnumNamesFloatTruncationType()[index];
 }
 
-enum QosClass {
+enum QosClass : int32_t {
   QosClass_QOS_UNDEFINED = 0,
   QosClass_BEST_EFFORT = 1,
   QosClass_REALTIME = 2,
@@ -595,7 +618,7 @@ inline const char *EnumNameQosClass(QosClass e) {
 
 namespace CoralSettings_ {
 
-enum Performance {
+enum Performance : int32_t {
   Performance_UNDEFINED = 0,
   Performance_MAXIMUM = 1,
   Performance_HIGH = 2,
@@ -636,7 +659,7 @@ inline const char *EnumNamePerformance(Performance e) {
 
 }  // namespace CoralSettings_
 
-enum BenchmarkEventType {
+enum BenchmarkEventType : int32_t {
   BenchmarkEventType_UNDEFINED_BENCHMARK_EVENT_TYPE = 0,
   BenchmarkEventType_START = 1,
   BenchmarkEventType_END = 2,
@@ -678,7 +701,7 @@ inline const char *EnumNameBenchmarkEventType(BenchmarkEventType e) {
   return EnumNamesBenchmarkEventType()[index];
 }
 
-enum BenchmarkStage {
+enum BenchmarkStage : int32_t {
   BenchmarkStage_UNKNOWN = 0,
   BenchmarkStage_INITIALIZATION = 1,
   BenchmarkStage_INFERENCE = 2,
@@ -713,18 +736,20 @@ inline const char *EnumNameBenchmarkStage(BenchmarkStage e) {
 
 struct ComputeSettingsT : public flatbuffers::NativeTable {
   typedef ComputeSettings TableType;
-  tflite::ExecutionPreference preference;
-  std::unique_ptr<tflite::TFLiteSettingsT> tflite_settings;
-  std::string model_namespace_for_statistics;
-  std::string model_identifier_for_statistics;
-  std::unique_ptr<tflite::MinibenchmarkSettingsT> settings_to_test_locally;
-  ComputeSettingsT()
-      : preference(tflite::ExecutionPreference_ANY) {
-  }
+  tflite::ExecutionPreference preference = tflite::ExecutionPreference_ANY;
+  std::unique_ptr<tflite::TFLiteSettingsT> tflite_settings{};
+  std::string model_namespace_for_statistics{};
+  std::string model_identifier_for_statistics{};
+  std::unique_ptr<tflite::MinibenchmarkSettingsT> settings_to_test_locally{};
+  ComputeSettingsT() = default;
+  ComputeSettingsT(const ComputeSettingsT &o);
+  ComputeSettingsT(ComputeSettingsT&&) FLATBUFFERS_NOEXCEPT = default;
+  ComputeSettingsT &operator=(ComputeSettingsT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct ComputeSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef ComputeSettingsT NativeTableType;
+  typedef ComputeSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_PREFERENCE = 4,
     VT_TFLITE_SETTINGS = 6,
@@ -749,7 +774,7 @@ struct ComputeSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_PREFERENCE) &&
+           VerifyField<int32_t>(verifier, VT_PREFERENCE, 4) &&
            VerifyOffset(verifier, VT_TFLITE_SETTINGS) &&
            verifier.VerifyTable(tflite_settings()) &&
            VerifyOffset(verifier, VT_MODEL_NAMESPACE_FOR_STATISTICS) &&
@@ -766,6 +791,7 @@ struct ComputeSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct ComputeSettingsBuilder {
+  typedef ComputeSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_preference(tflite::ExecutionPreference preference) {
@@ -787,7 +813,6 @@ struct ComputeSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  ComputeSettingsBuilder &operator=(const ComputeSettingsBuilder &);
   flatbuffers::Offset<ComputeSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<ComputeSettings>(end);
@@ -833,32 +858,27 @@ flatbuffers::Offset<ComputeSettings> CreateComputeSettings(flatbuffers::FlatBuff
 
 struct NNAPISettingsT : public flatbuffers::NativeTable {
   typedef NNAPISettings TableType;
-  std::string accelerator_name;
-  std::string cache_directory;
-  std::string model_token;
-  tflite::NNAPIExecutionPreference execution_preference;
-  int32_t no_of_nnapi_instances_to_cache;
-  std::unique_ptr<tflite::FallbackSettingsT> fallback_settings;
-  bool allow_nnapi_cpu_on_android_10_plus;
-  tflite::NNAPIExecutionPriority execution_priority;
-  bool allow_dynamic_dimensions;
-  bool allow_fp16_precision_for_fp32;
-  bool use_burst_computation;
-  int64_t support_library_handle;
-  NNAPISettingsT()
-      : execution_preference(tflite::NNAPIExecutionPreference_UNDEFINED),
-        no_of_nnapi_instances_to_cache(0),
-        allow_nnapi_cpu_on_android_10_plus(false),
-        execution_priority(tflite::NNAPIExecutionPriority_NNAPI_PRIORITY_UNDEFINED),
-        allow_dynamic_dimensions(false),
-        allow_fp16_precision_for_fp32(false),
-        use_burst_computation(false),
-        support_library_handle(0) {
-  }
+  std::string accelerator_name{};
+  std::string cache_directory{};
+  std::string model_token{};
+  tflite::NNAPIExecutionPreference execution_preference = tflite::NNAPIExecutionPreference_UNDEFINED;
+  int32_t no_of_nnapi_instances_to_cache = 0;
+  std::unique_ptr<tflite::FallbackSettingsT> fallback_settings{};
+  bool allow_nnapi_cpu_on_android_10_plus = false;
+  tflite::NNAPIExecutionPriority execution_priority = tflite::NNAPIExecutionPriority_NNAPI_PRIORITY_UNDEFINED;
+  bool allow_dynamic_dimensions = false;
+  bool allow_fp16_precision_for_fp32 = false;
+  bool use_burst_computation = false;
+  int64_t support_library_handle = 0;
+  NNAPISettingsT() = default;
+  NNAPISettingsT(const NNAPISettingsT &o);
+  NNAPISettingsT(NNAPISettingsT&&) FLATBUFFERS_NOEXCEPT = default;
+  NNAPISettingsT &operator=(NNAPISettingsT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct NNAPISettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef NNAPISettingsT NativeTableType;
+  typedef NNAPISettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_ACCELERATOR_NAME = 4,
     VT_CACHE_DIRECTORY = 6,
@@ -917,16 +937,16 @@ struct NNAPISettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
            verifier.VerifyString(cache_directory()) &&
            VerifyOffset(verifier, VT_MODEL_TOKEN) &&
            verifier.VerifyString(model_token()) &&
-           VerifyField<int32_t>(verifier, VT_EXECUTION_PREFERENCE) &&
-           VerifyField<int32_t>(verifier, VT_NO_OF_NNAPI_INSTANCES_TO_CACHE) &&
+           VerifyField<int32_t>(verifier, VT_EXECUTION_PREFERENCE, 4) &&
+           VerifyField<int32_t>(verifier, VT_NO_OF_NNAPI_INSTANCES_TO_CACHE, 4) &&
            VerifyOffset(verifier, VT_FALLBACK_SETTINGS) &&
            verifier.VerifyTable(fallback_settings()) &&
-           VerifyField<uint8_t>(verifier, VT_ALLOW_NNAPI_CPU_ON_ANDROID_10_PLUS) &&
-           VerifyField<int32_t>(verifier, VT_EXECUTION_PRIORITY) &&
-           VerifyField<uint8_t>(verifier, VT_ALLOW_DYNAMIC_DIMENSIONS) &&
-           VerifyField<uint8_t>(verifier, VT_ALLOW_FP16_PRECISION_FOR_FP32) &&
-           VerifyField<uint8_t>(verifier, VT_USE_BURST_COMPUTATION) &&
-           VerifyField<int64_t>(verifier, VT_SUPPORT_LIBRARY_HANDLE) &&
+           VerifyField<uint8_t>(verifier, VT_ALLOW_NNAPI_CPU_ON_ANDROID_10_PLUS, 1) &&
+           VerifyField<int32_t>(verifier, VT_EXECUTION_PRIORITY, 4) &&
+           VerifyField<uint8_t>(verifier, VT_ALLOW_DYNAMIC_DIMENSIONS, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ALLOW_FP16_PRECISION_FOR_FP32, 1) &&
+           VerifyField<uint8_t>(verifier, VT_USE_BURST_COMPUTATION, 1) &&
+           VerifyField<int64_t>(verifier, VT_SUPPORT_LIBRARY_HANDLE, 8) &&
            verifier.EndTable();
   }
   NNAPISettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -935,6 +955,7 @@ struct NNAPISettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct NNAPISettingsBuilder {
+  typedef NNAPISettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_accelerator_name(flatbuffers::Offset<flatbuffers::String> accelerator_name) {
@@ -977,7 +998,6 @@ struct NNAPISettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  NNAPISettingsBuilder &operator=(const NNAPISettingsBuilder &);
   flatbuffers::Offset<NNAPISettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<NNAPISettings>(end);
@@ -1052,28 +1072,20 @@ flatbuffers::Offset<NNAPISettings> CreateNNAPISettings(flatbuffers::FlatBufferBu
 
 struct GPUSettingsT : public flatbuffers::NativeTable {
   typedef GPUSettings TableType;
-  bool is_precision_loss_allowed;
-  bool enable_quantized_inference;
-  tflite::GPUBackend force_backend;
-  tflite::GPUInferencePriority inference_priority1;
-  tflite::GPUInferencePriority inference_priority2;
-  tflite::GPUInferencePriority inference_priority3;
-  tflite::GPUInferenceUsage inference_preference;
-  std::string cache_directory;
-  std::string model_token;
-  GPUSettingsT()
-      : is_precision_loss_allowed(false),
-        enable_quantized_inference(true),
-        force_backend(tflite::GPUBackend_UNSET),
-        inference_priority1(tflite::GPUInferencePriority_GPU_PRIORITY_AUTO),
-        inference_priority2(tflite::GPUInferencePriority_GPU_PRIORITY_AUTO),
-        inference_priority3(tflite::GPUInferencePriority_GPU_PRIORITY_AUTO),
-        inference_preference(tflite::GPUInferenceUsage_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER) {
-  }
+  bool is_precision_loss_allowed = false;
+  bool enable_quantized_inference = true;
+  tflite::GPUBackend force_backend = tflite::GPUBackend_UNSET;
+  tflite::GPUInferencePriority inference_priority1 = tflite::GPUInferencePriority_GPU_PRIORITY_AUTO;
+  tflite::GPUInferencePriority inference_priority2 = tflite::GPUInferencePriority_GPU_PRIORITY_AUTO;
+  tflite::GPUInferencePriority inference_priority3 = tflite::GPUInferencePriority_GPU_PRIORITY_AUTO;
+  tflite::GPUInferenceUsage inference_preference = tflite::GPUInferenceUsage_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER;
+  std::string cache_directory{};
+  std::string model_token{};
 };
 
 struct GPUSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef GPUSettingsT NativeTableType;
+  typedef GPUSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_IS_PRECISION_LOSS_ALLOWED = 4,
     VT_ENABLE_QUANTIZED_INFERENCE = 6,
@@ -1114,13 +1126,13 @@ struct GPUSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_IS_PRECISION_LOSS_ALLOWED) &&
-           VerifyField<uint8_t>(verifier, VT_ENABLE_QUANTIZED_INFERENCE) &&
-           VerifyField<int32_t>(verifier, VT_FORCE_BACKEND) &&
-           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY1) &&
-           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY2) &&
-           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY3) &&
-           VerifyField<int32_t>(verifier, VT_INFERENCE_PREFERENCE) &&
+           VerifyField<uint8_t>(verifier, VT_IS_PRECISION_LOSS_ALLOWED, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ENABLE_QUANTIZED_INFERENCE, 1) &&
+           VerifyField<int32_t>(verifier, VT_FORCE_BACKEND, 4) &&
+           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY1, 4) &&
+           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY2, 4) &&
+           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY3, 4) &&
+           VerifyField<int32_t>(verifier, VT_INFERENCE_PREFERENCE, 4) &&
            VerifyOffset(verifier, VT_CACHE_DIRECTORY) &&
            verifier.VerifyString(cache_directory()) &&
            VerifyOffset(verifier, VT_MODEL_TOKEN) &&
@@ -1133,6 +1145,7 @@ struct GPUSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct GPUSettingsBuilder {
+  typedef GPUSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_is_precision_loss_allowed(bool is_precision_loss_allowed) {
@@ -1166,7 +1179,6 @@ struct GPUSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  GPUSettingsBuilder &operator=(const GPUSettingsBuilder &);
   flatbuffers::Offset<GPUSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<GPUSettings>(end);
@@ -1228,20 +1240,15 @@ flatbuffers::Offset<GPUSettings> CreateGPUSettings(flatbuffers::FlatBufferBuilde
 
 struct HexagonSettingsT : public flatbuffers::NativeTable {
   typedef HexagonSettings TableType;
-  int32_t debug_level;
-  int32_t powersave_level;
-  bool print_graph_profile;
-  bool print_graph_debug;
-  HexagonSettingsT()
-      : debug_level(0),
-        powersave_level(0),
-        print_graph_profile(false),
-        print_graph_debug(false) {
-  }
+  int32_t debug_level = 0;
+  int32_t powersave_level = 0;
+  bool print_graph_profile = false;
+  bool print_graph_debug = false;
 };
 
 struct HexagonSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef HexagonSettingsT NativeTableType;
+  typedef HexagonSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_DEBUG_LEVEL = 4,
     VT_POWERSAVE_LEVEL = 6,
@@ -1262,10 +1269,10 @@ struct HexagonSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_DEBUG_LEVEL) &&
-           VerifyField<int32_t>(verifier, VT_POWERSAVE_LEVEL) &&
-           VerifyField<uint8_t>(verifier, VT_PRINT_GRAPH_PROFILE) &&
-           VerifyField<uint8_t>(verifier, VT_PRINT_GRAPH_DEBUG) &&
+           VerifyField<int32_t>(verifier, VT_DEBUG_LEVEL, 4) &&
+           VerifyField<int32_t>(verifier, VT_POWERSAVE_LEVEL, 4) &&
+           VerifyField<uint8_t>(verifier, VT_PRINT_GRAPH_PROFILE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_PRINT_GRAPH_DEBUG, 1) &&
            verifier.EndTable();
   }
   HexagonSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1274,6 +1281,7 @@ struct HexagonSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct HexagonSettingsBuilder {
+  typedef HexagonSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_debug_level(int32_t debug_level) {
@@ -1292,7 +1300,6 @@ struct HexagonSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  HexagonSettingsBuilder &operator=(const HexagonSettingsBuilder &);
   flatbuffers::Offset<HexagonSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<HexagonSettings>(end);
@@ -1318,14 +1325,12 @@ flatbuffers::Offset<HexagonSettings> CreateHexagonSettings(flatbuffers::FlatBuff
 
 struct XNNPackSettingsT : public flatbuffers::NativeTable {
   typedef XNNPackSettings TableType;
-  int32_t num_threads;
-  XNNPackSettingsT()
-      : num_threads(0) {
-  }
+  int32_t num_threads = 0;
 };
 
 struct XNNPackSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef XNNPackSettingsT NativeTableType;
+  typedef XNNPackSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_NUM_THREADS = 4
   };
@@ -1334,7 +1339,7 @@ struct XNNPackSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUM_THREADS) &&
+           VerifyField<int32_t>(verifier, VT_NUM_THREADS, 4) &&
            verifier.EndTable();
   }
   XNNPackSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1343,6 +1348,7 @@ struct XNNPackSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct XNNPackSettingsBuilder {
+  typedef XNNPackSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_num_threads(int32_t num_threads) {
@@ -1352,7 +1358,6 @@ struct XNNPackSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  XNNPackSettingsBuilder &operator=(const XNNPackSettingsBuilder &);
   flatbuffers::Offset<XNNPackSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<XNNPackSettings>(end);
@@ -1372,20 +1377,15 @@ flatbuffers::Offset<XNNPackSettings> CreateXNNPackSettings(flatbuffers::FlatBuff
 
 struct CoreMLSettingsT : public flatbuffers::NativeTable {
   typedef CoreMLSettings TableType;
-  tflite::CoreMLSettings_::EnabledDevices enabled_devices;
-  int32_t coreml_version;
-  int32_t max_delegated_partitions;
-  int32_t min_nodes_per_partition;
-  CoreMLSettingsT()
-      : enabled_devices(tflite::CoreMLSettings_::EnabledDevices_DEVICES_ALL),
-        coreml_version(0),
-        max_delegated_partitions(0),
-        min_nodes_per_partition(2) {
-  }
+  tflite::CoreMLSettings_::EnabledDevices enabled_devices = tflite::CoreMLSettings_::EnabledDevices_DEVICES_ALL;
+  int32_t coreml_version = 0;
+  int32_t max_delegated_partitions = 0;
+  int32_t min_nodes_per_partition = 2;
 };
 
 struct CoreMLSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef CoreMLSettingsT NativeTableType;
+  typedef CoreMLSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_ENABLED_DEVICES = 4,
     VT_COREML_VERSION = 6,
@@ -1406,10 +1406,10 @@ struct CoreMLSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_ENABLED_DEVICES) &&
-           VerifyField<int32_t>(verifier, VT_COREML_VERSION) &&
-           VerifyField<int32_t>(verifier, VT_MAX_DELEGATED_PARTITIONS) &&
-           VerifyField<int32_t>(verifier, VT_MIN_NODES_PER_PARTITION) &&
+           VerifyField<int32_t>(verifier, VT_ENABLED_DEVICES, 4) &&
+           VerifyField<int32_t>(verifier, VT_COREML_VERSION, 4) &&
+           VerifyField<int32_t>(verifier, VT_MAX_DELEGATED_PARTITIONS, 4) &&
+           VerifyField<int32_t>(verifier, VT_MIN_NODES_PER_PARTITION, 4) &&
            verifier.EndTable();
   }
   CoreMLSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1418,6 +1418,7 @@ struct CoreMLSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct CoreMLSettingsBuilder {
+  typedef CoreMLSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_enabled_devices(tflite::CoreMLSettings_::EnabledDevices enabled_devices) {
@@ -1436,7 +1437,6 @@ struct CoreMLSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  CoreMLSettingsBuilder &operator=(const CoreMLSettingsBuilder &);
   flatbuffers::Offset<CoreMLSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<CoreMLSettings>(end);
@@ -1462,19 +1462,15 @@ flatbuffers::Offset<CoreMLSettings> CreateCoreMLSettings(flatbuffers::FlatBuffer
 
 struct EdgeTpuDeviceSpecT : public flatbuffers::NativeTable {
   typedef EdgeTpuDeviceSpec TableType;
-  tflite::EdgeTpuDeviceSpec_::PlatformType platform_type;
-  int32_t num_chips;
-  std::vector<std::string> device_paths;
-  int32_t chip_family;
-  EdgeTpuDeviceSpecT()
-      : platform_type(tflite::EdgeTpuDeviceSpec_::PlatformType_MMIO),
-        num_chips(0),
-        chip_family(0) {
-  }
+  tflite::EdgeTpuDeviceSpec_::PlatformType platform_type = tflite::EdgeTpuDeviceSpec_::PlatformType_MMIO;
+  int32_t num_chips = 0;
+  std::vector<std::string> device_paths{};
+  int32_t chip_family = 0;
 };
 
 struct EdgeTpuDeviceSpec FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef EdgeTpuDeviceSpecT NativeTableType;
+  typedef EdgeTpuDeviceSpecBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_PLATFORM_TYPE = 4,
     VT_NUM_CHIPS = 6,
@@ -1495,12 +1491,12 @@ struct EdgeTpuDeviceSpec FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_PLATFORM_TYPE) &&
-           VerifyField<int32_t>(verifier, VT_NUM_CHIPS) &&
+           VerifyField<int32_t>(verifier, VT_PLATFORM_TYPE, 4) &&
+           VerifyField<int32_t>(verifier, VT_NUM_CHIPS, 4) &&
            VerifyOffset(verifier, VT_DEVICE_PATHS) &&
            verifier.VerifyVector(device_paths()) &&
            verifier.VerifyVectorOfStrings(device_paths()) &&
-           VerifyField<int32_t>(verifier, VT_CHIP_FAMILY) &&
+           VerifyField<int32_t>(verifier, VT_CHIP_FAMILY, 4) &&
            verifier.EndTable();
   }
   EdgeTpuDeviceSpecT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1509,6 +1505,7 @@ struct EdgeTpuDeviceSpec FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct EdgeTpuDeviceSpecBuilder {
+  typedef EdgeTpuDeviceSpec Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_platform_type(tflite::EdgeTpuDeviceSpec_::PlatformType platform_type) {
@@ -1527,7 +1524,6 @@ struct EdgeTpuDeviceSpecBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  EdgeTpuDeviceSpecBuilder &operator=(const EdgeTpuDeviceSpecBuilder &);
   flatbuffers::Offset<EdgeTpuDeviceSpec> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<EdgeTpuDeviceSpec>(end);
@@ -1568,16 +1564,13 @@ flatbuffers::Offset<EdgeTpuDeviceSpec> CreateEdgeTpuDeviceSpec(flatbuffers::Flat
 
 struct EdgeTpuInactivePowerConfigT : public flatbuffers::NativeTable {
   typedef EdgeTpuInactivePowerConfig TableType;
-  tflite::EdgeTpuPowerState inactive_power_state;
-  int64_t inactive_timeout_us;
-  EdgeTpuInactivePowerConfigT()
-      : inactive_power_state(tflite::EdgeTpuPowerState_UNDEFINED_POWERSTATE),
-        inactive_timeout_us(0) {
-  }
+  tflite::EdgeTpuPowerState inactive_power_state = tflite::EdgeTpuPowerState_UNDEFINED_POWERSTATE;
+  int64_t inactive_timeout_us = 0;
 };
 
 struct EdgeTpuInactivePowerConfig FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef EdgeTpuInactivePowerConfigT NativeTableType;
+  typedef EdgeTpuInactivePowerConfigBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_INACTIVE_POWER_STATE = 4,
     VT_INACTIVE_TIMEOUT_US = 6
@@ -1590,8 +1583,8 @@ struct EdgeTpuInactivePowerConfig FLATBUFFERS_FINAL_CLASS : private flatbuffers:
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_INACTIVE_POWER_STATE) &&
-           VerifyField<int64_t>(verifier, VT_INACTIVE_TIMEOUT_US) &&
+           VerifyField<int32_t>(verifier, VT_INACTIVE_POWER_STATE, 4) &&
+           VerifyField<int64_t>(verifier, VT_INACTIVE_TIMEOUT_US, 8) &&
            verifier.EndTable();
   }
   EdgeTpuInactivePowerConfigT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1600,6 +1593,7 @@ struct EdgeTpuInactivePowerConfig FLATBUFFERS_FINAL_CLASS : private flatbuffers:
 };
 
 struct EdgeTpuInactivePowerConfigBuilder {
+  typedef EdgeTpuInactivePowerConfig Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_inactive_power_state(tflite::EdgeTpuPowerState inactive_power_state) {
@@ -1612,7 +1606,6 @@ struct EdgeTpuInactivePowerConfigBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  EdgeTpuInactivePowerConfigBuilder &operator=(const EdgeTpuInactivePowerConfigBuilder &);
   flatbuffers::Offset<EdgeTpuInactivePowerConfig> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<EdgeTpuInactivePowerConfig>(end);
@@ -1634,23 +1627,22 @@ flatbuffers::Offset<EdgeTpuInactivePowerConfig> CreateEdgeTpuInactivePowerConfig
 
 struct EdgeTpuSettingsT : public flatbuffers::NativeTable {
   typedef EdgeTpuSettings TableType;
-  tflite::EdgeTpuPowerState inference_power_state;
-  std::vector<std::unique_ptr<tflite::EdgeTpuInactivePowerConfigT>> inactive_power_configs;
-  int32_t inference_priority;
-  std::unique_ptr<tflite::EdgeTpuDeviceSpecT> edgetpu_device_spec;
-  std::string model_token;
-  tflite::EdgeTpuSettings_::FloatTruncationType float_truncation_type;
-  tflite::EdgeTpuSettings_::QosClass qos_class;
-  EdgeTpuSettingsT()
-      : inference_power_state(tflite::EdgeTpuPowerState_UNDEFINED_POWERSTATE),
-        inference_priority(-1),
-        float_truncation_type(tflite::EdgeTpuSettings_::FloatTruncationType_UNSPECIFIED),
-        qos_class(tflite::EdgeTpuSettings_::QosClass_QOS_UNDEFINED) {
-  }
+  tflite::EdgeTpuPowerState inference_power_state = tflite::EdgeTpuPowerState_UNDEFINED_POWERSTATE;
+  std::vector<std::unique_ptr<tflite::EdgeTpuInactivePowerConfigT>> inactive_power_configs{};
+  int32_t inference_priority = -1;
+  std::unique_ptr<tflite::EdgeTpuDeviceSpecT> edgetpu_device_spec{};
+  std::string model_token{};
+  tflite::EdgeTpuSettings_::FloatTruncationType float_truncation_type = tflite::EdgeTpuSettings_::FloatTruncationType_UNSPECIFIED;
+  tflite::EdgeTpuSettings_::QosClass qos_class = tflite::EdgeTpuSettings_::QosClass_QOS_UNDEFINED;
+  EdgeTpuSettingsT() = default;
+  EdgeTpuSettingsT(const EdgeTpuSettingsT &o);
+  EdgeTpuSettingsT(EdgeTpuSettingsT&&) FLATBUFFERS_NOEXCEPT = default;
+  EdgeTpuSettingsT &operator=(EdgeTpuSettingsT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct EdgeTpuSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef EdgeTpuSettingsT NativeTableType;
+  typedef EdgeTpuSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_INFERENCE_POWER_STATE = 4,
     VT_INACTIVE_POWER_CONFIGS = 6,
@@ -1683,17 +1675,17 @@ struct EdgeTpuSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_INFERENCE_POWER_STATE) &&
+           VerifyField<int32_t>(verifier, VT_INFERENCE_POWER_STATE, 4) &&
            VerifyOffset(verifier, VT_INACTIVE_POWER_CONFIGS) &&
            verifier.VerifyVector(inactive_power_configs()) &&
            verifier.VerifyVectorOfTables(inactive_power_configs()) &&
-           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY) &&
+           VerifyField<int32_t>(verifier, VT_INFERENCE_PRIORITY, 4) &&
            VerifyOffset(verifier, VT_EDGETPU_DEVICE_SPEC) &&
            verifier.VerifyTable(edgetpu_device_spec()) &&
            VerifyOffset(verifier, VT_MODEL_TOKEN) &&
            verifier.VerifyString(model_token()) &&
-           VerifyField<int32_t>(verifier, VT_FLOAT_TRUNCATION_TYPE) &&
-           VerifyField<int32_t>(verifier, VT_QOS_CLASS) &&
+           VerifyField<int32_t>(verifier, VT_FLOAT_TRUNCATION_TYPE, 4) &&
+           VerifyField<int32_t>(verifier, VT_QOS_CLASS, 4) &&
            verifier.EndTable();
   }
   EdgeTpuSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1702,6 +1694,7 @@ struct EdgeTpuSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct EdgeTpuSettingsBuilder {
+  typedef EdgeTpuSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_inference_power_state(tflite::EdgeTpuPowerState inference_power_state) {
@@ -1729,7 +1722,6 @@ struct EdgeTpuSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  EdgeTpuSettingsBuilder &operator=(const EdgeTpuSettingsBuilder &);
   flatbuffers::Offset<EdgeTpuSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<EdgeTpuSettings>(end);
@@ -1783,19 +1775,15 @@ flatbuffers::Offset<EdgeTpuSettings> CreateEdgeTpuSettings(flatbuffers::FlatBuff
 
 struct CoralSettingsT : public flatbuffers::NativeTable {
   typedef CoralSettings TableType;
-  std::string device;
-  tflite::CoralSettings_::Performance performance;
-  bool usb_always_dfu;
-  int32_t usb_max_bulk_in_queue_length;
-  CoralSettingsT()
-      : performance(tflite::CoralSettings_::Performance_UNDEFINED),
-        usb_always_dfu(false),
-        usb_max_bulk_in_queue_length(0) {
-  }
+  std::string device{};
+  tflite::CoralSettings_::Performance performance = tflite::CoralSettings_::Performance_UNDEFINED;
+  bool usb_always_dfu = false;
+  int32_t usb_max_bulk_in_queue_length = 0;
 };
 
 struct CoralSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef CoralSettingsT NativeTableType;
+  typedef CoralSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_DEVICE = 4,
     VT_PERFORMANCE = 6,
@@ -1818,9 +1806,9 @@ struct CoralSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_DEVICE) &&
            verifier.VerifyString(device()) &&
-           VerifyField<int32_t>(verifier, VT_PERFORMANCE) &&
-           VerifyField<uint8_t>(verifier, VT_USB_ALWAYS_DFU) &&
-           VerifyField<int32_t>(verifier, VT_USB_MAX_BULK_IN_QUEUE_LENGTH) &&
+           VerifyField<int32_t>(verifier, VT_PERFORMANCE, 4) &&
+           VerifyField<uint8_t>(verifier, VT_USB_ALWAYS_DFU, 1) &&
+           VerifyField<int32_t>(verifier, VT_USB_MAX_BULK_IN_QUEUE_LENGTH, 4) &&
            verifier.EndTable();
   }
   CoralSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1829,6 +1817,7 @@ struct CoralSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct CoralSettingsBuilder {
+  typedef CoralSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_device(flatbuffers::Offset<flatbuffers::String> device) {
@@ -1847,7 +1836,6 @@ struct CoralSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  CoralSettingsBuilder &operator=(const CoralSettingsBuilder &);
   flatbuffers::Offset<CoralSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<CoralSettings>(end);
@@ -1888,14 +1876,12 @@ flatbuffers::Offset<CoralSettings> CreateCoralSettings(flatbuffers::FlatBufferBu
 
 struct CPUSettingsT : public flatbuffers::NativeTable {
   typedef CPUSettings TableType;
-  int32_t num_threads;
-  CPUSettingsT()
-      : num_threads(-1) {
-  }
+  int32_t num_threads = -1;
 };
 
 struct CPUSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef CPUSettingsT NativeTableType;
+  typedef CPUSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_NUM_THREADS = 4
   };
@@ -1904,7 +1890,7 @@ struct CPUSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUM_THREADS) &&
+           VerifyField<int32_t>(verifier, VT_NUM_THREADS, 4) &&
            verifier.EndTable();
   }
   CPUSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -1913,6 +1899,7 @@ struct CPUSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct CPUSettingsBuilder {
+  typedef CPUSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_num_threads(int32_t num_threads) {
@@ -1922,7 +1909,6 @@ struct CPUSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  CPUSettingsBuilder &operator=(const CPUSettingsBuilder &);
   flatbuffers::Offset<CPUSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<CPUSettings>(end);
@@ -1942,25 +1928,26 @@ flatbuffers::Offset<CPUSettings> CreateCPUSettings(flatbuffers::FlatBufferBuilde
 
 struct TFLiteSettingsT : public flatbuffers::NativeTable {
   typedef TFLiteSettings TableType;
-  tflite::Delegate delegate;
-  std::unique_ptr<tflite::NNAPISettingsT> nnapi_settings;
-  std::unique_ptr<tflite::GPUSettingsT> gpu_settings;
-  std::unique_ptr<tflite::HexagonSettingsT> hexagon_settings;
-  std::unique_ptr<tflite::XNNPackSettingsT> xnnpack_settings;
-  std::unique_ptr<tflite::CoreMLSettingsT> coreml_settings;
-  std::unique_ptr<tflite::CPUSettingsT> cpu_settings;
-  int32_t max_delegated_partitions;
-  std::unique_ptr<tflite::EdgeTpuSettingsT> edgetpu_settings;
-  std::unique_ptr<tflite::CoralSettingsT> coral_settings;
-  std::unique_ptr<tflite::FallbackSettingsT> fallback_settings;
-  TFLiteSettingsT()
-      : delegate(tflite::Delegate_NONE),
-        max_delegated_partitions(0) {
-  }
+  tflite::Delegate delegate = tflite::Delegate_NONE;
+  std::unique_ptr<tflite::NNAPISettingsT> nnapi_settings{};
+  std::unique_ptr<tflite::GPUSettingsT> gpu_settings{};
+  std::unique_ptr<tflite::HexagonSettingsT> hexagon_settings{};
+  std::unique_ptr<tflite::XNNPackSettingsT> xnnpack_settings{};
+  std::unique_ptr<tflite::CoreMLSettingsT> coreml_settings{};
+  std::unique_ptr<tflite::CPUSettingsT> cpu_settings{};
+  int32_t max_delegated_partitions = 0;
+  std::unique_ptr<tflite::EdgeTpuSettingsT> edgetpu_settings{};
+  std::unique_ptr<tflite::CoralSettingsT> coral_settings{};
+  std::unique_ptr<tflite::FallbackSettingsT> fallback_settings{};
+  TFLiteSettingsT() = default;
+  TFLiteSettingsT(const TFLiteSettingsT &o);
+  TFLiteSettingsT(TFLiteSettingsT&&) FLATBUFFERS_NOEXCEPT = default;
+  TFLiteSettingsT &operator=(TFLiteSettingsT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct TFLiteSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef TFLiteSettingsT NativeTableType;
+  typedef TFLiteSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_DELEGATE = 4,
     VT_NNAPI_SETTINGS = 6,
@@ -2009,7 +1996,7 @@ struct TFLiteSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_DELEGATE) &&
+           VerifyField<int32_t>(verifier, VT_DELEGATE, 4) &&
            VerifyOffset(verifier, VT_NNAPI_SETTINGS) &&
            verifier.VerifyTable(nnapi_settings()) &&
            VerifyOffset(verifier, VT_GPU_SETTINGS) &&
@@ -2022,7 +2009,7 @@ struct TFLiteSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
            verifier.VerifyTable(coreml_settings()) &&
            VerifyOffset(verifier, VT_CPU_SETTINGS) &&
            verifier.VerifyTable(cpu_settings()) &&
-           VerifyField<int32_t>(verifier, VT_MAX_DELEGATED_PARTITIONS) &&
+           VerifyField<int32_t>(verifier, VT_MAX_DELEGATED_PARTITIONS, 4) &&
            VerifyOffset(verifier, VT_EDGETPU_SETTINGS) &&
            verifier.VerifyTable(edgetpu_settings()) &&
            VerifyOffset(verifier, VT_CORAL_SETTINGS) &&
@@ -2037,6 +2024,7 @@ struct TFLiteSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct TFLiteSettingsBuilder {
+  typedef TFLiteSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_delegate(tflite::Delegate delegate) {
@@ -2076,7 +2064,6 @@ struct TFLiteSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  TFLiteSettingsBuilder &operator=(const TFLiteSettingsBuilder &);
   flatbuffers::Offset<TFLiteSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<TFLiteSettings>(end);
@@ -2116,16 +2103,13 @@ flatbuffers::Offset<TFLiteSettings> CreateTFLiteSettings(flatbuffers::FlatBuffer
 
 struct FallbackSettingsT : public flatbuffers::NativeTable {
   typedef FallbackSettings TableType;
-  bool allow_automatic_fallback_on_compilation_error;
-  bool allow_automatic_fallback_on_execution_error;
-  FallbackSettingsT()
-      : allow_automatic_fallback_on_compilation_error(false),
-        allow_automatic_fallback_on_execution_error(false) {
-  }
+  bool allow_automatic_fallback_on_compilation_error = false;
+  bool allow_automatic_fallback_on_execution_error = false;
 };
 
 struct FallbackSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef FallbackSettingsT NativeTableType;
+  typedef FallbackSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_ALLOW_AUTOMATIC_FALLBACK_ON_COMPILATION_ERROR = 4,
     VT_ALLOW_AUTOMATIC_FALLBACK_ON_EXECUTION_ERROR = 6
@@ -2138,8 +2122,8 @@ struct FallbackSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_ALLOW_AUTOMATIC_FALLBACK_ON_COMPILATION_ERROR) &&
-           VerifyField<uint8_t>(verifier, VT_ALLOW_AUTOMATIC_FALLBACK_ON_EXECUTION_ERROR) &&
+           VerifyField<uint8_t>(verifier, VT_ALLOW_AUTOMATIC_FALLBACK_ON_COMPILATION_ERROR, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ALLOW_AUTOMATIC_FALLBACK_ON_EXECUTION_ERROR, 1) &&
            verifier.EndTable();
   }
   FallbackSettingsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2148,6 +2132,7 @@ struct FallbackSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct FallbackSettingsBuilder {
+  typedef FallbackSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_allow_automatic_fallback_on_compilation_error(bool allow_automatic_fallback_on_compilation_error) {
@@ -2160,7 +2145,6 @@ struct FallbackSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  FallbackSettingsBuilder &operator=(const FallbackSettingsBuilder &);
   flatbuffers::Offset<FallbackSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<FallbackSettings>(end);
@@ -2182,14 +2166,13 @@ flatbuffers::Offset<FallbackSettings> CreateFallbackSettings(flatbuffers::FlatBu
 
 struct BenchmarkMetricT : public flatbuffers::NativeTable {
   typedef BenchmarkMetric TableType;
-  std::string name;
-  std::vector<float> values;
-  BenchmarkMetricT() {
-  }
+  std::string name{};
+  std::vector<float> values{};
 };
 
 struct BenchmarkMetric FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BenchmarkMetricT NativeTableType;
+  typedef BenchmarkMetricBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_NAME = 4,
     VT_VALUES = 6
@@ -2214,6 +2197,7 @@ struct BenchmarkMetric FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct BenchmarkMetricBuilder {
+  typedef BenchmarkMetric Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_name(flatbuffers::Offset<flatbuffers::String> name) {
@@ -2226,7 +2210,6 @@ struct BenchmarkMetricBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BenchmarkMetricBuilder &operator=(const BenchmarkMetricBuilder &);
   flatbuffers::Offset<BenchmarkMetric> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BenchmarkMetric>(end);
@@ -2260,19 +2243,20 @@ flatbuffers::Offset<BenchmarkMetric> CreateBenchmarkMetric(flatbuffers::FlatBuff
 
 struct BenchmarkResultT : public flatbuffers::NativeTable {
   typedef BenchmarkResult TableType;
-  std::vector<int64_t> initialization_time_us;
-  std::vector<int64_t> inference_time_us;
-  int32_t max_memory_kb;
-  bool ok;
-  std::vector<std::unique_ptr<tflite::BenchmarkMetricT>> metrics;
-  BenchmarkResultT()
-      : max_memory_kb(0),
-        ok(false) {
-  }
+  std::vector<int64_t> initialization_time_us{};
+  std::vector<int64_t> inference_time_us{};
+  int32_t max_memory_kb = 0;
+  bool ok = false;
+  std::vector<std::unique_ptr<tflite::BenchmarkMetricT>> metrics{};
+  BenchmarkResultT() = default;
+  BenchmarkResultT(const BenchmarkResultT &o);
+  BenchmarkResultT(BenchmarkResultT&&) FLATBUFFERS_NOEXCEPT = default;
+  BenchmarkResultT &operator=(BenchmarkResultT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct BenchmarkResult FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BenchmarkResultT NativeTableType;
+  typedef BenchmarkResultBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_INITIALIZATION_TIME_US = 4,
     VT_INFERENCE_TIME_US = 6,
@@ -2301,8 +2285,8 @@ struct BenchmarkResult FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
            verifier.VerifyVector(initialization_time_us()) &&
            VerifyOffset(verifier, VT_INFERENCE_TIME_US) &&
            verifier.VerifyVector(inference_time_us()) &&
-           VerifyField<int32_t>(verifier, VT_MAX_MEMORY_KB) &&
-           VerifyField<uint8_t>(verifier, VT_OK) &&
+           VerifyField<int32_t>(verifier, VT_MAX_MEMORY_KB, 4) &&
+           VerifyField<uint8_t>(verifier, VT_OK, 1) &&
            VerifyOffset(verifier, VT_METRICS) &&
            verifier.VerifyVector(metrics()) &&
            verifier.VerifyVectorOfTables(metrics()) &&
@@ -2314,6 +2298,7 @@ struct BenchmarkResult FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct BenchmarkResultBuilder {
+  typedef BenchmarkResult Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_initialization_time_us(flatbuffers::Offset<flatbuffers::Vector<int64_t>> initialization_time_us) {
@@ -2335,7 +2320,6 @@ struct BenchmarkResultBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BenchmarkResultBuilder &operator=(const BenchmarkResultBuilder &);
   flatbuffers::Offset<BenchmarkResult> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BenchmarkResult>(end);
@@ -2382,18 +2366,14 @@ flatbuffers::Offset<BenchmarkResult> CreateBenchmarkResult(flatbuffers::FlatBuff
 
 struct ErrorCodeT : public flatbuffers::NativeTable {
   typedef ErrorCode TableType;
-  tflite::Delegate source;
-  int32_t tflite_error;
-  int64_t underlying_api_error;
-  ErrorCodeT()
-      : source(tflite::Delegate_NONE),
-        tflite_error(0),
-        underlying_api_error(0) {
-  }
+  tflite::Delegate source = tflite::Delegate_NONE;
+  int32_t tflite_error = 0;
+  int64_t underlying_api_error = 0;
 };
 
 struct ErrorCode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef ErrorCodeT NativeTableType;
+  typedef ErrorCodeBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_SOURCE = 4,
     VT_TFLITE_ERROR = 6,
@@ -2410,9 +2390,9 @@ struct ErrorCode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_SOURCE) &&
-           VerifyField<int32_t>(verifier, VT_TFLITE_ERROR) &&
-           VerifyField<int64_t>(verifier, VT_UNDERLYING_API_ERROR) &&
+           VerifyField<int32_t>(verifier, VT_SOURCE, 4) &&
+           VerifyField<int32_t>(verifier, VT_TFLITE_ERROR, 4) &&
+           VerifyField<int64_t>(verifier, VT_UNDERLYING_API_ERROR, 8) &&
            verifier.EndTable();
   }
   ErrorCodeT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2421,6 +2401,7 @@ struct ErrorCode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct ErrorCodeBuilder {
+  typedef ErrorCode Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_source(tflite::Delegate source) {
@@ -2436,7 +2417,6 @@ struct ErrorCodeBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  ErrorCodeBuilder &operator=(const ErrorCodeBuilder &);
   flatbuffers::Offset<ErrorCode> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<ErrorCode>(end);
@@ -2460,21 +2440,20 @@ flatbuffers::Offset<ErrorCode> CreateErrorCode(flatbuffers::FlatBufferBuilder &_
 
 struct BenchmarkErrorT : public flatbuffers::NativeTable {
   typedef BenchmarkError TableType;
-  tflite::BenchmarkStage stage;
-  int32_t exit_code;
-  int32_t signal;
-  std::vector<std::unique_ptr<tflite::ErrorCodeT>> error_code;
-  int32_t mini_benchmark_error_code;
-  BenchmarkErrorT()
-      : stage(tflite::BenchmarkStage_UNKNOWN),
-        exit_code(0),
-        signal(0),
-        mini_benchmark_error_code(0) {
-  }
+  tflite::BenchmarkStage stage = tflite::BenchmarkStage_UNKNOWN;
+  int32_t exit_code = 0;
+  int32_t signal = 0;
+  std::vector<std::unique_ptr<tflite::ErrorCodeT>> error_code{};
+  int32_t mini_benchmark_error_code = 0;
+  BenchmarkErrorT() = default;
+  BenchmarkErrorT(const BenchmarkErrorT &o);
+  BenchmarkErrorT(BenchmarkErrorT&&) FLATBUFFERS_NOEXCEPT = default;
+  BenchmarkErrorT &operator=(BenchmarkErrorT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct BenchmarkError FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BenchmarkErrorT NativeTableType;
+  typedef BenchmarkErrorBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_STAGE = 4,
     VT_EXIT_CODE = 6,
@@ -2499,13 +2478,13 @@ struct BenchmarkError FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_STAGE) &&
-           VerifyField<int32_t>(verifier, VT_EXIT_CODE) &&
-           VerifyField<int32_t>(verifier, VT_SIGNAL) &&
+           VerifyField<int32_t>(verifier, VT_STAGE, 4) &&
+           VerifyField<int32_t>(verifier, VT_EXIT_CODE, 4) &&
+           VerifyField<int32_t>(verifier, VT_SIGNAL, 4) &&
            VerifyOffset(verifier, VT_ERROR_CODE) &&
            verifier.VerifyVector(error_code()) &&
            verifier.VerifyVectorOfTables(error_code()) &&
-           VerifyField<int32_t>(verifier, VT_MINI_BENCHMARK_ERROR_CODE) &&
+           VerifyField<int32_t>(verifier, VT_MINI_BENCHMARK_ERROR_CODE, 4) &&
            verifier.EndTable();
   }
   BenchmarkErrorT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2514,6 +2493,7 @@ struct BenchmarkError FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct BenchmarkErrorBuilder {
+  typedef BenchmarkError Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_stage(tflite::BenchmarkStage stage) {
@@ -2535,7 +2515,6 @@ struct BenchmarkErrorBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BenchmarkErrorBuilder &operator=(const BenchmarkErrorBuilder &);
   flatbuffers::Offset<BenchmarkError> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BenchmarkError>(end);
@@ -2580,21 +2559,21 @@ flatbuffers::Offset<BenchmarkError> CreateBenchmarkError(flatbuffers::FlatBuffer
 
 struct BenchmarkEventT : public flatbuffers::NativeTable {
   typedef BenchmarkEvent TableType;
-  std::unique_ptr<tflite::TFLiteSettingsT> tflite_settings;
-  tflite::BenchmarkEventType event_type;
-  std::unique_ptr<tflite::BenchmarkResultT> result;
-  std::unique_ptr<tflite::BenchmarkErrorT> error;
-  int64_t boottime_us;
-  int64_t wallclock_us;
-  BenchmarkEventT()
-      : event_type(tflite::BenchmarkEventType_UNDEFINED_BENCHMARK_EVENT_TYPE),
-        boottime_us(0),
-        wallclock_us(0) {
-  }
+  std::unique_ptr<tflite::TFLiteSettingsT> tflite_settings{};
+  tflite::BenchmarkEventType event_type = tflite::BenchmarkEventType_UNDEFINED_BENCHMARK_EVENT_TYPE;
+  std::unique_ptr<tflite::BenchmarkResultT> result{};
+  std::unique_ptr<tflite::BenchmarkErrorT> error{};
+  int64_t boottime_us = 0;
+  int64_t wallclock_us = 0;
+  BenchmarkEventT() = default;
+  BenchmarkEventT(const BenchmarkEventT &o);
+  BenchmarkEventT(BenchmarkEventT&&) FLATBUFFERS_NOEXCEPT = default;
+  BenchmarkEventT &operator=(BenchmarkEventT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct BenchmarkEvent FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BenchmarkEventT NativeTableType;
+  typedef BenchmarkEventBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_TFLITE_SETTINGS = 4,
     VT_EVENT_TYPE = 6,
@@ -2625,13 +2604,13 @@ struct BenchmarkEvent FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_TFLITE_SETTINGS) &&
            verifier.VerifyTable(tflite_settings()) &&
-           VerifyField<int32_t>(verifier, VT_EVENT_TYPE) &&
+           VerifyField<int32_t>(verifier, VT_EVENT_TYPE, 4) &&
            VerifyOffset(verifier, VT_RESULT) &&
            verifier.VerifyTable(result()) &&
            VerifyOffset(verifier, VT_ERROR) &&
            verifier.VerifyTable(error()) &&
-           VerifyField<int64_t>(verifier, VT_BOOTTIME_US) &&
-           VerifyField<int64_t>(verifier, VT_WALLCLOCK_US) &&
+           VerifyField<int64_t>(verifier, VT_BOOTTIME_US, 8) &&
+           VerifyField<int64_t>(verifier, VT_WALLCLOCK_US, 8) &&
            verifier.EndTable();
   }
   BenchmarkEventT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2640,6 +2619,7 @@ struct BenchmarkEvent FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct BenchmarkEventBuilder {
+  typedef BenchmarkEvent Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_tflite_settings(flatbuffers::Offset<tflite::TFLiteSettings> tflite_settings) {
@@ -2664,7 +2644,6 @@ struct BenchmarkEventBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BenchmarkEventBuilder &operator=(const BenchmarkEventBuilder &);
   flatbuffers::Offset<BenchmarkEvent> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BenchmarkEvent>(end);
@@ -2694,17 +2673,18 @@ flatbuffers::Offset<BenchmarkEvent> CreateBenchmarkEvent(flatbuffers::FlatBuffer
 
 struct BestAccelerationDecisionT : public flatbuffers::NativeTable {
   typedef BestAccelerationDecision TableType;
-  int32_t number_of_source_events;
-  std::unique_ptr<tflite::BenchmarkEventT> min_latency_event;
-  int64_t min_inference_time_us;
-  BestAccelerationDecisionT()
-      : number_of_source_events(0),
-        min_inference_time_us(0) {
-  }
+  int32_t number_of_source_events = 0;
+  std::unique_ptr<tflite::BenchmarkEventT> min_latency_event{};
+  int64_t min_inference_time_us = 0;
+  BestAccelerationDecisionT() = default;
+  BestAccelerationDecisionT(const BestAccelerationDecisionT &o);
+  BestAccelerationDecisionT(BestAccelerationDecisionT&&) FLATBUFFERS_NOEXCEPT = default;
+  BestAccelerationDecisionT &operator=(BestAccelerationDecisionT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct BestAccelerationDecision FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BestAccelerationDecisionT NativeTableType;
+  typedef BestAccelerationDecisionBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_NUMBER_OF_SOURCE_EVENTS = 4,
     VT_MIN_LATENCY_EVENT = 6,
@@ -2721,10 +2701,10 @@ struct BestAccelerationDecision FLATBUFFERS_FINAL_CLASS : private flatbuffers::T
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUMBER_OF_SOURCE_EVENTS) &&
+           VerifyField<int32_t>(verifier, VT_NUMBER_OF_SOURCE_EVENTS, 4) &&
            VerifyOffset(verifier, VT_MIN_LATENCY_EVENT) &&
            verifier.VerifyTable(min_latency_event()) &&
-           VerifyField<int64_t>(verifier, VT_MIN_INFERENCE_TIME_US) &&
+           VerifyField<int64_t>(verifier, VT_MIN_INFERENCE_TIME_US, 8) &&
            verifier.EndTable();
   }
   BestAccelerationDecisionT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2733,6 +2713,7 @@ struct BestAccelerationDecision FLATBUFFERS_FINAL_CLASS : private flatbuffers::T
 };
 
 struct BestAccelerationDecisionBuilder {
+  typedef BestAccelerationDecision Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_number_of_source_events(int32_t number_of_source_events) {
@@ -2748,7 +2729,6 @@ struct BestAccelerationDecisionBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BestAccelerationDecisionBuilder &operator=(const BestAccelerationDecisionBuilder &);
   flatbuffers::Offset<BestAccelerationDecision> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BestAccelerationDecision>(end);
@@ -2772,14 +2752,12 @@ flatbuffers::Offset<BestAccelerationDecision> CreateBestAccelerationDecision(fla
 
 struct BenchmarkInitializationFailureT : public flatbuffers::NativeTable {
   typedef BenchmarkInitializationFailure TableType;
-  int32_t initialization_status;
-  BenchmarkInitializationFailureT()
-      : initialization_status(0) {
-  }
+  int32_t initialization_status = 0;
 };
 
 struct BenchmarkInitializationFailure FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BenchmarkInitializationFailureT NativeTableType;
+  typedef BenchmarkInitializationFailureBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_INITIALIZATION_STATUS = 4
   };
@@ -2788,7 +2766,7 @@ struct BenchmarkInitializationFailure FLATBUFFERS_FINAL_CLASS : private flatbuff
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_INITIALIZATION_STATUS) &&
+           VerifyField<int32_t>(verifier, VT_INITIALIZATION_STATUS, 4) &&
            verifier.EndTable();
   }
   BenchmarkInitializationFailureT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2797,6 +2775,7 @@ struct BenchmarkInitializationFailure FLATBUFFERS_FINAL_CLASS : private flatbuff
 };
 
 struct BenchmarkInitializationFailureBuilder {
+  typedef BenchmarkInitializationFailure Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_initialization_status(int32_t initialization_status) {
@@ -2806,7 +2785,6 @@ struct BenchmarkInitializationFailureBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BenchmarkInitializationFailureBuilder &operator=(const BenchmarkInitializationFailureBuilder &);
   flatbuffers::Offset<BenchmarkInitializationFailure> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BenchmarkInitializationFailure>(end);
@@ -2826,17 +2804,19 @@ flatbuffers::Offset<BenchmarkInitializationFailure> CreateBenchmarkInitializatio
 
 struct MiniBenchmarkEventT : public flatbuffers::NativeTable {
   typedef MiniBenchmarkEvent TableType;
-  bool is_log_flushing_event;
-  std::unique_ptr<tflite::BestAccelerationDecisionT> best_acceleration_decision;
-  std::unique_ptr<tflite::BenchmarkInitializationFailureT> initialization_failure;
-  std::unique_ptr<tflite::BenchmarkEventT> benchmark_event;
-  MiniBenchmarkEventT()
-      : is_log_flushing_event(false) {
-  }
+  bool is_log_flushing_event = false;
+  std::unique_ptr<tflite::BestAccelerationDecisionT> best_acceleration_decision{};
+  std::unique_ptr<tflite::BenchmarkInitializationFailureT> initialization_failure{};
+  std::unique_ptr<tflite::BenchmarkEventT> benchmark_event{};
+  MiniBenchmarkEventT() = default;
+  MiniBenchmarkEventT(const MiniBenchmarkEventT &o);
+  MiniBenchmarkEventT(MiniBenchmarkEventT&&) FLATBUFFERS_NOEXCEPT = default;
+  MiniBenchmarkEventT &operator=(MiniBenchmarkEventT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct MiniBenchmarkEvent FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef MiniBenchmarkEventT NativeTableType;
+  typedef MiniBenchmarkEventBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_IS_LOG_FLUSHING_EVENT = 4,
     VT_BEST_ACCELERATION_DECISION = 6,
@@ -2857,7 +2837,7 @@ struct MiniBenchmarkEvent FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_IS_LOG_FLUSHING_EVENT) &&
+           VerifyField<uint8_t>(verifier, VT_IS_LOG_FLUSHING_EVENT, 1) &&
            VerifyOffset(verifier, VT_BEST_ACCELERATION_DECISION) &&
            verifier.VerifyTable(best_acceleration_decision()) &&
            VerifyOffset(verifier, VT_INITIALIZATION_FAILURE) &&
@@ -2872,6 +2852,7 @@ struct MiniBenchmarkEvent FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct MiniBenchmarkEventBuilder {
+  typedef MiniBenchmarkEvent Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_is_log_flushing_event(bool is_log_flushing_event) {
@@ -2890,7 +2871,6 @@ struct MiniBenchmarkEventBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  MiniBenchmarkEventBuilder &operator=(const MiniBenchmarkEventBuilder &);
   flatbuffers::Offset<MiniBenchmarkEvent> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<MiniBenchmarkEvent>(end);
@@ -2916,19 +2896,15 @@ flatbuffers::Offset<MiniBenchmarkEvent> CreateMiniBenchmarkEvent(flatbuffers::Fl
 
 struct ModelFileT : public flatbuffers::NativeTable {
   typedef ModelFile TableType;
-  std::string filename;
-  int64_t fd;
-  int64_t offset;
-  int64_t length;
-  ModelFileT()
-      : fd(0),
-        offset(0),
-        length(0) {
-  }
+  std::string filename{};
+  int64_t fd = 0;
+  int64_t offset = 0;
+  int64_t length = 0;
 };
 
 struct ModelFile FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef ModelFileT NativeTableType;
+  typedef ModelFileBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_FILENAME = 4,
     VT_FD = 6,
@@ -2951,9 +2927,9 @@ struct ModelFile FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_FILENAME) &&
            verifier.VerifyString(filename()) &&
-           VerifyField<int64_t>(verifier, VT_FD) &&
-           VerifyField<int64_t>(verifier, VT_OFFSET) &&
-           VerifyField<int64_t>(verifier, VT_LENGTH) &&
+           VerifyField<int64_t>(verifier, VT_FD, 8) &&
+           VerifyField<int64_t>(verifier, VT_OFFSET, 8) &&
+           VerifyField<int64_t>(verifier, VT_LENGTH, 8) &&
            verifier.EndTable();
   }
   ModelFileT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -2962,6 +2938,7 @@ struct ModelFile FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
 };
 
 struct ModelFileBuilder {
+  typedef ModelFile Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_filename(flatbuffers::Offset<flatbuffers::String> filename) {
@@ -2980,7 +2957,6 @@ struct ModelFileBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  ModelFileBuilder &operator=(const ModelFileBuilder &);
   flatbuffers::Offset<ModelFile> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<ModelFile>(end);
@@ -3021,14 +2997,13 @@ flatbuffers::Offset<ModelFile> CreateModelFile(flatbuffers::FlatBufferBuilder &_
 
 struct BenchmarkStoragePathsT : public flatbuffers::NativeTable {
   typedef BenchmarkStoragePaths TableType;
-  std::string storage_file_path;
-  std::string data_directory_path;
-  BenchmarkStoragePathsT() {
-  }
+  std::string storage_file_path{};
+  std::string data_directory_path{};
 };
 
 struct BenchmarkStoragePaths FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef BenchmarkStoragePathsT NativeTableType;
+  typedef BenchmarkStoragePathsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_STORAGE_FILE_PATH = 4,
     VT_DATA_DIRECTORY_PATH = 6
@@ -3053,6 +3028,7 @@ struct BenchmarkStoragePaths FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tabl
 };
 
 struct BenchmarkStoragePathsBuilder {
+  typedef BenchmarkStoragePaths Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_storage_file_path(flatbuffers::Offset<flatbuffers::String> storage_file_path) {
@@ -3065,7 +3041,6 @@ struct BenchmarkStoragePathsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  BenchmarkStoragePathsBuilder &operator=(const BenchmarkStoragePathsBuilder &);
   flatbuffers::Offset<BenchmarkStoragePaths> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<BenchmarkStoragePaths>(end);
@@ -3099,15 +3074,18 @@ flatbuffers::Offset<BenchmarkStoragePaths> CreateBenchmarkStoragePaths(flatbuffe
 
 struct MinibenchmarkSettingsT : public flatbuffers::NativeTable {
   typedef MinibenchmarkSettings TableType;
-  std::vector<std::unique_ptr<tflite::TFLiteSettingsT>> settings_to_test;
-  std::unique_ptr<tflite::ModelFileT> model_file;
-  std::unique_ptr<tflite::BenchmarkStoragePathsT> storage_paths;
-  MinibenchmarkSettingsT() {
-  }
+  std::vector<std::unique_ptr<tflite::TFLiteSettingsT>> settings_to_test{};
+  std::unique_ptr<tflite::ModelFileT> model_file{};
+  std::unique_ptr<tflite::BenchmarkStoragePathsT> storage_paths{};
+  MinibenchmarkSettingsT() = default;
+  MinibenchmarkSettingsT(const MinibenchmarkSettingsT &o);
+  MinibenchmarkSettingsT(MinibenchmarkSettingsT&&) FLATBUFFERS_NOEXCEPT = default;
+  MinibenchmarkSettingsT &operator=(MinibenchmarkSettingsT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct MinibenchmarkSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   typedef MinibenchmarkSettingsT NativeTableType;
+  typedef MinibenchmarkSettingsBuilder Builder;
   enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
     VT_SETTINGS_TO_TEST = 4,
     VT_MODEL_FILE = 6,
@@ -3139,6 +3117,7 @@ struct MinibenchmarkSettings FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tabl
 };
 
 struct MinibenchmarkSettingsBuilder {
+  typedef MinibenchmarkSettings Table;
   flatbuffers::FlatBufferBuilder &fbb_;
   flatbuffers::uoffset_t start_;
   void add_settings_to_test(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::TFLiteSettings>>> settings_to_test) {
@@ -3154,7 +3133,6 @@ struct MinibenchmarkSettingsBuilder {
         : fbb_(_fbb) {
     start_ = fbb_.StartTable();
   }
-  MinibenchmarkSettingsBuilder &operator=(const MinibenchmarkSettingsBuilder &);
   flatbuffers::Offset<MinibenchmarkSettings> Finish() {
     const auto end = fbb_.EndTable(start_);
     auto o = flatbuffers::Offset<MinibenchmarkSettings>(end);
@@ -3204,20 +3182,37 @@ inline bool operator!=(const ComputeSettingsT &lhs, const ComputeSettingsT &rhs)
 }
 
 
+inline ComputeSettingsT::ComputeSettingsT(const ComputeSettingsT &o)
+      : preference(o.preference),
+        tflite_settings((o.tflite_settings) ? new tflite::TFLiteSettingsT(*o.tflite_settings) : nullptr),
+        model_namespace_for_statistics(o.model_namespace_for_statistics),
+        model_identifier_for_statistics(o.model_identifier_for_statistics),
+        settings_to_test_locally((o.settings_to_test_locally) ? new tflite::MinibenchmarkSettingsT(*o.settings_to_test_locally) : nullptr) {
+}
+
+inline ComputeSettingsT &ComputeSettingsT::operator=(ComputeSettingsT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(preference, o.preference);
+  std::swap(tflite_settings, o.tflite_settings);
+  std::swap(model_namespace_for_statistics, o.model_namespace_for_statistics);
+  std::swap(model_identifier_for_statistics, o.model_identifier_for_statistics);
+  std::swap(settings_to_test_locally, o.settings_to_test_locally);
+  return *this;
+}
+
 inline ComputeSettingsT *ComputeSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new ComputeSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<ComputeSettingsT>(new ComputeSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void ComputeSettings::UnPackTo(ComputeSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
   { auto _e = preference(); _o->preference = _e; }
-  { auto _e = tflite_settings(); if (_e) _o->tflite_settings = std::unique_ptr<tflite::TFLiteSettingsT>(_e->UnPack(_resolver)); }
+  { auto _e = tflite_settings(); if (_e) { if(_o->tflite_settings) { _e->UnPackTo(_o->tflite_settings.get(), _resolver); } else { _o->tflite_settings = std::unique_ptr<tflite::TFLiteSettingsT>(_e->UnPack(_resolver)); } } }
   { auto _e = model_namespace_for_statistics(); if (_e) _o->model_namespace_for_statistics = _e->str(); }
   { auto _e = model_identifier_for_statistics(); if (_e) _o->model_identifier_for_statistics = _e->str(); }
-  { auto _e = settings_to_test_locally(); if (_e) _o->settings_to_test_locally = std::unique_ptr<tflite::MinibenchmarkSettingsT>(_e->UnPack(_resolver)); }
+  { auto _e = settings_to_test_locally(); if (_e) { if(_o->settings_to_test_locally) { _e->UnPackTo(_o->settings_to_test_locally.get(), _resolver); } else { _o->settings_to_test_locally = std::unique_ptr<tflite::MinibenchmarkSettingsT>(_e->UnPack(_resolver)); } } }
 }
 
 inline flatbuffers::Offset<ComputeSettings> ComputeSettings::Pack(flatbuffers::FlatBufferBuilder &_fbb, const ComputeSettingsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -3264,10 +3259,41 @@ inline bool operator!=(const NNAPISettingsT &lhs, const NNAPISettingsT &rhs) {
 }
 
 
+inline NNAPISettingsT::NNAPISettingsT(const NNAPISettingsT &o)
+      : accelerator_name(o.accelerator_name),
+        cache_directory(o.cache_directory),
+        model_token(o.model_token),
+        execution_preference(o.execution_preference),
+        no_of_nnapi_instances_to_cache(o.no_of_nnapi_instances_to_cache),
+        fallback_settings((o.fallback_settings) ? new tflite::FallbackSettingsT(*o.fallback_settings) : nullptr),
+        allow_nnapi_cpu_on_android_10_plus(o.allow_nnapi_cpu_on_android_10_plus),
+        execution_priority(o.execution_priority),
+        allow_dynamic_dimensions(o.allow_dynamic_dimensions),
+        allow_fp16_precision_for_fp32(o.allow_fp16_precision_for_fp32),
+        use_burst_computation(o.use_burst_computation),
+        support_library_handle(o.support_library_handle) {
+}
+
+inline NNAPISettingsT &NNAPISettingsT::operator=(NNAPISettingsT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(accelerator_name, o.accelerator_name);
+  std::swap(cache_directory, o.cache_directory);
+  std::swap(model_token, o.model_token);
+  std::swap(execution_preference, o.execution_preference);
+  std::swap(no_of_nnapi_instances_to_cache, o.no_of_nnapi_instances_to_cache);
+  std::swap(fallback_settings, o.fallback_settings);
+  std::swap(allow_nnapi_cpu_on_android_10_plus, o.allow_nnapi_cpu_on_android_10_plus);
+  std::swap(execution_priority, o.execution_priority);
+  std::swap(allow_dynamic_dimensions, o.allow_dynamic_dimensions);
+  std::swap(allow_fp16_precision_for_fp32, o.allow_fp16_precision_for_fp32);
+  std::swap(use_burst_computation, o.use_burst_computation);
+  std::swap(support_library_handle, o.support_library_handle);
+  return *this;
+}
+
 inline NNAPISettingsT *NNAPISettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new NNAPISettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<NNAPISettingsT>(new NNAPISettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void NNAPISettings::UnPackTo(NNAPISettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3278,7 +3304,7 @@ inline void NNAPISettings::UnPackTo(NNAPISettingsT *_o, const flatbuffers::resol
   { auto _e = model_token(); if (_e) _o->model_token = _e->str(); }
   { auto _e = execution_preference(); _o->execution_preference = _e; }
   { auto _e = no_of_nnapi_instances_to_cache(); _o->no_of_nnapi_instances_to_cache = _e; }
-  { auto _e = fallback_settings(); if (_e) _o->fallback_settings = std::unique_ptr<tflite::FallbackSettingsT>(_e->UnPack(_resolver)); }
+  { auto _e = fallback_settings(); if (_e) { if(_o->fallback_settings) { _e->UnPackTo(_o->fallback_settings.get(), _resolver); } else { _o->fallback_settings = std::unique_ptr<tflite::FallbackSettingsT>(_e->UnPack(_resolver)); } } }
   { auto _e = allow_nnapi_cpu_on_android_10_plus(); _o->allow_nnapi_cpu_on_android_10_plus = _e; }
   { auto _e = execution_priority(); _o->execution_priority = _e; }
   { auto _e = allow_dynamic_dimensions(); _o->allow_dynamic_dimensions = _e; }
@@ -3343,9 +3369,9 @@ inline bool operator!=(const GPUSettingsT &lhs, const GPUSettingsT &rhs) {
 
 
 inline GPUSettingsT *GPUSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new GPUSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<GPUSettingsT>(new GPUSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void GPUSettings::UnPackTo(GPUSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3407,9 +3433,9 @@ inline bool operator!=(const HexagonSettingsT &lhs, const HexagonSettingsT &rhs)
 
 
 inline HexagonSettingsT *HexagonSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new HexagonSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<HexagonSettingsT>(new HexagonSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void HexagonSettings::UnPackTo(HexagonSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3453,9 +3479,9 @@ inline bool operator!=(const XNNPackSettingsT &lhs, const XNNPackSettingsT &rhs)
 
 
 inline XNNPackSettingsT *XNNPackSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new XNNPackSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<XNNPackSettingsT>(new XNNPackSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void XNNPackSettings::UnPackTo(XNNPackSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3493,9 +3519,9 @@ inline bool operator!=(const CoreMLSettingsT &lhs, const CoreMLSettingsT &rhs) {
 
 
 inline CoreMLSettingsT *CoreMLSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new CoreMLSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<CoreMLSettingsT>(new CoreMLSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void CoreMLSettings::UnPackTo(CoreMLSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3542,9 +3568,9 @@ inline bool operator!=(const EdgeTpuDeviceSpecT &lhs, const EdgeTpuDeviceSpecT &
 
 
 inline EdgeTpuDeviceSpecT *EdgeTpuDeviceSpec::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new EdgeTpuDeviceSpecT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<EdgeTpuDeviceSpecT>(new EdgeTpuDeviceSpecT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void EdgeTpuDeviceSpec::UnPackTo(EdgeTpuDeviceSpecT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3589,9 +3615,9 @@ inline bool operator!=(const EdgeTpuInactivePowerConfigT &lhs, const EdgeTpuInac
 
 
 inline EdgeTpuInactivePowerConfigT *EdgeTpuInactivePowerConfig::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new EdgeTpuInactivePowerConfigT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<EdgeTpuInactivePowerConfigT>(new EdgeTpuInactivePowerConfigT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void EdgeTpuInactivePowerConfig::UnPackTo(EdgeTpuInactivePowerConfigT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3634,19 +3660,41 @@ inline bool operator!=(const EdgeTpuSettingsT &lhs, const EdgeTpuSettingsT &rhs)
 }
 
 
+inline EdgeTpuSettingsT::EdgeTpuSettingsT(const EdgeTpuSettingsT &o)
+      : inference_power_state(o.inference_power_state),
+        inference_priority(o.inference_priority),
+        edgetpu_device_spec((o.edgetpu_device_spec) ? new tflite::EdgeTpuDeviceSpecT(*o.edgetpu_device_spec) : nullptr),
+        model_token(o.model_token),
+        float_truncation_type(o.float_truncation_type),
+        qos_class(o.qos_class) {
+  inactive_power_configs.reserve(o.inactive_power_configs.size());
+  for (const auto &v : o.inactive_power_configs) { inactive_power_configs.emplace_back((v) ? new tflite::EdgeTpuInactivePowerConfigT(*v) : nullptr); }
+}
+
+inline EdgeTpuSettingsT &EdgeTpuSettingsT::operator=(EdgeTpuSettingsT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(inference_power_state, o.inference_power_state);
+  std::swap(inactive_power_configs, o.inactive_power_configs);
+  std::swap(inference_priority, o.inference_priority);
+  std::swap(edgetpu_device_spec, o.edgetpu_device_spec);
+  std::swap(model_token, o.model_token);
+  std::swap(float_truncation_type, o.float_truncation_type);
+  std::swap(qos_class, o.qos_class);
+  return *this;
+}
+
 inline EdgeTpuSettingsT *EdgeTpuSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new EdgeTpuSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<EdgeTpuSettingsT>(new EdgeTpuSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void EdgeTpuSettings::UnPackTo(EdgeTpuSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
   { auto _e = inference_power_state(); _o->inference_power_state = _e; }
-  { auto _e = inactive_power_configs(); if (_e) { _o->inactive_power_configs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->inactive_power_configs[_i] = std::unique_ptr<tflite::EdgeTpuInactivePowerConfigT>(_e->Get(_i)->UnPack(_resolver)); } } }
+  { auto _e = inactive_power_configs(); if (_e) { _o->inactive_power_configs.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->inactive_power_configs[_i]) { _e->Get(_i)->UnPackTo(_o->inactive_power_configs[_i].get(), _resolver); } else { _o->inactive_power_configs[_i] = std::unique_ptr<tflite::EdgeTpuInactivePowerConfigT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = inference_priority(); _o->inference_priority = _e; }
-  { auto _e = edgetpu_device_spec(); if (_e) _o->edgetpu_device_spec = std::unique_ptr<tflite::EdgeTpuDeviceSpecT>(_e->UnPack(_resolver)); }
+  { auto _e = edgetpu_device_spec(); if (_e) { if(_o->edgetpu_device_spec) { _e->UnPackTo(_o->edgetpu_device_spec.get(), _resolver); } else { _o->edgetpu_device_spec = std::unique_ptr<tflite::EdgeTpuDeviceSpecT>(_e->UnPack(_resolver)); } } }
   { auto _e = model_token(); if (_e) _o->model_token = _e->str(); }
   { auto _e = float_truncation_type(); _o->float_truncation_type = _e; }
   { auto _e = qos_class(); _o->qos_class = _e; }
@@ -3693,9 +3741,9 @@ inline bool operator!=(const CoralSettingsT &lhs, const CoralSettingsT &rhs) {
 
 
 inline CoralSettingsT *CoralSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new CoralSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<CoralSettingsT>(new CoralSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void CoralSettings::UnPackTo(CoralSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3739,9 +3787,9 @@ inline bool operator!=(const CPUSettingsT &lhs, const CPUSettingsT &rhs) {
 
 
 inline CPUSettingsT *CPUSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new CPUSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<CPUSettingsT>(new CPUSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void CPUSettings::UnPackTo(CPUSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3785,26 +3833,55 @@ inline bool operator!=(const TFLiteSettingsT &lhs, const TFLiteSettingsT &rhs) {
 }
 
 
+inline TFLiteSettingsT::TFLiteSettingsT(const TFLiteSettingsT &o)
+      : delegate(o.delegate),
+        nnapi_settings((o.nnapi_settings) ? new tflite::NNAPISettingsT(*o.nnapi_settings) : nullptr),
+        gpu_settings((o.gpu_settings) ? new tflite::GPUSettingsT(*o.gpu_settings) : nullptr),
+        hexagon_settings((o.hexagon_settings) ? new tflite::HexagonSettingsT(*o.hexagon_settings) : nullptr),
+        xnnpack_settings((o.xnnpack_settings) ? new tflite::XNNPackSettingsT(*o.xnnpack_settings) : nullptr),
+        coreml_settings((o.coreml_settings) ? new tflite::CoreMLSettingsT(*o.coreml_settings) : nullptr),
+        cpu_settings((o.cpu_settings) ? new tflite::CPUSettingsT(*o.cpu_settings) : nullptr),
+        max_delegated_partitions(o.max_delegated_partitions),
+        edgetpu_settings((o.edgetpu_settings) ? new tflite::EdgeTpuSettingsT(*o.edgetpu_settings) : nullptr),
+        coral_settings((o.coral_settings) ? new tflite::CoralSettingsT(*o.coral_settings) : nullptr),
+        fallback_settings((o.fallback_settings) ? new tflite::FallbackSettingsT(*o.fallback_settings) : nullptr) {
+}
+
+inline TFLiteSettingsT &TFLiteSettingsT::operator=(TFLiteSettingsT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(delegate, o.delegate);
+  std::swap(nnapi_settings, o.nnapi_settings);
+  std::swap(gpu_settings, o.gpu_settings);
+  std::swap(hexagon_settings, o.hexagon_settings);
+  std::swap(xnnpack_settings, o.xnnpack_settings);
+  std::swap(coreml_settings, o.coreml_settings);
+  std::swap(cpu_settings, o.cpu_settings);
+  std::swap(max_delegated_partitions, o.max_delegated_partitions);
+  std::swap(edgetpu_settings, o.edgetpu_settings);
+  std::swap(coral_settings, o.coral_settings);
+  std::swap(fallback_settings, o.fallback_settings);
+  return *this;
+}
+
 inline TFLiteSettingsT *TFLiteSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new TFLiteSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<TFLiteSettingsT>(new TFLiteSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void TFLiteSettings::UnPackTo(TFLiteSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
   { auto _e = delegate(); _o->delegate = _e; }
-  { auto _e = nnapi_settings(); if (_e) _o->nnapi_settings = std::unique_ptr<tflite::NNAPISettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = gpu_settings(); if (_e) _o->gpu_settings = std::unique_ptr<tflite::GPUSettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = hexagon_settings(); if (_e) _o->hexagon_settings = std::unique_ptr<tflite::HexagonSettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = xnnpack_settings(); if (_e) _o->xnnpack_settings = std::unique_ptr<tflite::XNNPackSettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = coreml_settings(); if (_e) _o->coreml_settings = std::unique_ptr<tflite::CoreMLSettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = cpu_settings(); if (_e) _o->cpu_settings = std::unique_ptr<tflite::CPUSettingsT>(_e->UnPack(_resolver)); }
+  { auto _e = nnapi_settings(); if (_e) { if(_o->nnapi_settings) { _e->UnPackTo(_o->nnapi_settings.get(), _resolver); } else { _o->nnapi_settings = std::unique_ptr<tflite::NNAPISettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = gpu_settings(); if (_e) { if(_o->gpu_settings) { _e->UnPackTo(_o->gpu_settings.get(), _resolver); } else { _o->gpu_settings = std::unique_ptr<tflite::GPUSettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = hexagon_settings(); if (_e) { if(_o->hexagon_settings) { _e->UnPackTo(_o->hexagon_settings.get(), _resolver); } else { _o->hexagon_settings = std::unique_ptr<tflite::HexagonSettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = xnnpack_settings(); if (_e) { if(_o->xnnpack_settings) { _e->UnPackTo(_o->xnnpack_settings.get(), _resolver); } else { _o->xnnpack_settings = std::unique_ptr<tflite::XNNPackSettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = coreml_settings(); if (_e) { if(_o->coreml_settings) { _e->UnPackTo(_o->coreml_settings.get(), _resolver); } else { _o->coreml_settings = std::unique_ptr<tflite::CoreMLSettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = cpu_settings(); if (_e) { if(_o->cpu_settings) { _e->UnPackTo(_o->cpu_settings.get(), _resolver); } else { _o->cpu_settings = std::unique_ptr<tflite::CPUSettingsT>(_e->UnPack(_resolver)); } } }
   { auto _e = max_delegated_partitions(); _o->max_delegated_partitions = _e; }
-  { auto _e = edgetpu_settings(); if (_e) _o->edgetpu_settings = std::unique_ptr<tflite::EdgeTpuSettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = coral_settings(); if (_e) _o->coral_settings = std::unique_ptr<tflite::CoralSettingsT>(_e->UnPack(_resolver)); }
-  { auto _e = fallback_settings(); if (_e) _o->fallback_settings = std::unique_ptr<tflite::FallbackSettingsT>(_e->UnPack(_resolver)); }
+  { auto _e = edgetpu_settings(); if (_e) { if(_o->edgetpu_settings) { _e->UnPackTo(_o->edgetpu_settings.get(), _resolver); } else { _o->edgetpu_settings = std::unique_ptr<tflite::EdgeTpuSettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = coral_settings(); if (_e) { if(_o->coral_settings) { _e->UnPackTo(_o->coral_settings.get(), _resolver); } else { _o->coral_settings = std::unique_ptr<tflite::CoralSettingsT>(_e->UnPack(_resolver)); } } }
+  { auto _e = fallback_settings(); if (_e) { if(_o->fallback_settings) { _e->UnPackTo(_o->fallback_settings.get(), _resolver); } else { _o->fallback_settings = std::unique_ptr<tflite::FallbackSettingsT>(_e->UnPack(_resolver)); } } }
 }
 
 inline flatbuffers::Offset<TFLiteSettings> TFLiteSettings::Pack(flatbuffers::FlatBufferBuilder &_fbb, const TFLiteSettingsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -3854,9 +3931,9 @@ inline bool operator!=(const FallbackSettingsT &lhs, const FallbackSettingsT &rh
 
 
 inline FallbackSettingsT *FallbackSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new FallbackSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<FallbackSettingsT>(new FallbackSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void FallbackSettings::UnPackTo(FallbackSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3895,9 +3972,9 @@ inline bool operator!=(const BenchmarkMetricT &lhs, const BenchmarkMetricT &rhs)
 
 
 inline BenchmarkMetricT *BenchmarkMetric::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BenchmarkMetricT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BenchmarkMetricT>(new BenchmarkMetricT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BenchmarkMetric::UnPackTo(BenchmarkMetricT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3938,10 +4015,28 @@ inline bool operator!=(const BenchmarkResultT &lhs, const BenchmarkResultT &rhs)
 }
 
 
+inline BenchmarkResultT::BenchmarkResultT(const BenchmarkResultT &o)
+      : initialization_time_us(o.initialization_time_us),
+        inference_time_us(o.inference_time_us),
+        max_memory_kb(o.max_memory_kb),
+        ok(o.ok) {
+  metrics.reserve(o.metrics.size());
+  for (const auto &v : o.metrics) { metrics.emplace_back((v) ? new tflite::BenchmarkMetricT(*v) : nullptr); }
+}
+
+inline BenchmarkResultT &BenchmarkResultT::operator=(BenchmarkResultT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(initialization_time_us, o.initialization_time_us);
+  std::swap(inference_time_us, o.inference_time_us);
+  std::swap(max_memory_kb, o.max_memory_kb);
+  std::swap(ok, o.ok);
+  std::swap(metrics, o.metrics);
+  return *this;
+}
+
 inline BenchmarkResultT *BenchmarkResult::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BenchmarkResultT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BenchmarkResultT>(new BenchmarkResultT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BenchmarkResult::UnPackTo(BenchmarkResultT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -3951,7 +4046,7 @@ inline void BenchmarkResult::UnPackTo(BenchmarkResultT *_o, const flatbuffers::r
   { auto _e = inference_time_us(); if (_e) { _o->inference_time_us.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->inference_time_us[_i] = _e->Get(_i); } } }
   { auto _e = max_memory_kb(); _o->max_memory_kb = _e; }
   { auto _e = ok(); _o->ok = _e; }
-  { auto _e = metrics(); if (_e) { _o->metrics.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->metrics[_i] = std::unique_ptr<tflite::BenchmarkMetricT>(_e->Get(_i)->UnPack(_resolver)); } } }
+  { auto _e = metrics(); if (_e) { _o->metrics.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->metrics[_i]) { _e->Get(_i)->UnPackTo(_o->metrics[_i].get(), _resolver); } else { _o->metrics[_i] = std::unique_ptr<tflite::BenchmarkMetricT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
 }
 
 inline flatbuffers::Offset<BenchmarkResult> BenchmarkResult::Pack(flatbuffers::FlatBufferBuilder &_fbb, const BenchmarkResultT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -3990,9 +4085,9 @@ inline bool operator!=(const ErrorCodeT &lhs, const ErrorCodeT &rhs) {
 
 
 inline ErrorCodeT *ErrorCode::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new ErrorCodeT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<ErrorCodeT>(new ErrorCodeT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void ErrorCode::UnPackTo(ErrorCodeT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -4036,10 +4131,28 @@ inline bool operator!=(const BenchmarkErrorT &lhs, const BenchmarkErrorT &rhs) {
 }
 
 
+inline BenchmarkErrorT::BenchmarkErrorT(const BenchmarkErrorT &o)
+      : stage(o.stage),
+        exit_code(o.exit_code),
+        signal(o.signal),
+        mini_benchmark_error_code(o.mini_benchmark_error_code) {
+  error_code.reserve(o.error_code.size());
+  for (const auto &v : o.error_code) { error_code.emplace_back((v) ? new tflite::ErrorCodeT(*v) : nullptr); }
+}
+
+inline BenchmarkErrorT &BenchmarkErrorT::operator=(BenchmarkErrorT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(stage, o.stage);
+  std::swap(exit_code, o.exit_code);
+  std::swap(signal, o.signal);
+  std::swap(error_code, o.error_code);
+  std::swap(mini_benchmark_error_code, o.mini_benchmark_error_code);
+  return *this;
+}
+
 inline BenchmarkErrorT *BenchmarkError::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BenchmarkErrorT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BenchmarkErrorT>(new BenchmarkErrorT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BenchmarkError::UnPackTo(BenchmarkErrorT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -4048,7 +4161,7 @@ inline void BenchmarkError::UnPackTo(BenchmarkErrorT *_o, const flatbuffers::res
   { auto _e = stage(); _o->stage = _e; }
   { auto _e = exit_code(); _o->exit_code = _e; }
   { auto _e = signal(); _o->signal = _e; }
-  { auto _e = error_code(); if (_e) { _o->error_code.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->error_code[_i] = std::unique_ptr<tflite::ErrorCodeT>(_e->Get(_i)->UnPack(_resolver)); } } }
+  { auto _e = error_code(); if (_e) { _o->error_code.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->error_code[_i]) { _e->Get(_i)->UnPackTo(_o->error_code[_i].get(), _resolver); } else { _o->error_code[_i] = std::unique_ptr<tflite::ErrorCodeT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
   { auto _e = mini_benchmark_error_code(); _o->mini_benchmark_error_code = _e; }
 }
 
@@ -4090,19 +4203,38 @@ inline bool operator!=(const BenchmarkEventT &lhs, const BenchmarkEventT &rhs) {
 }
 
 
+inline BenchmarkEventT::BenchmarkEventT(const BenchmarkEventT &o)
+      : tflite_settings((o.tflite_settings) ? new tflite::TFLiteSettingsT(*o.tflite_settings) : nullptr),
+        event_type(o.event_type),
+        result((o.result) ? new tflite::BenchmarkResultT(*o.result) : nullptr),
+        error((o.error) ? new tflite::BenchmarkErrorT(*o.error) : nullptr),
+        boottime_us(o.boottime_us),
+        wallclock_us(o.wallclock_us) {
+}
+
+inline BenchmarkEventT &BenchmarkEventT::operator=(BenchmarkEventT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(tflite_settings, o.tflite_settings);
+  std::swap(event_type, o.event_type);
+  std::swap(result, o.result);
+  std::swap(error, o.error);
+  std::swap(boottime_us, o.boottime_us);
+  std::swap(wallclock_us, o.wallclock_us);
+  return *this;
+}
+
 inline BenchmarkEventT *BenchmarkEvent::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BenchmarkEventT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BenchmarkEventT>(new BenchmarkEventT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BenchmarkEvent::UnPackTo(BenchmarkEventT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
-  { auto _e = tflite_settings(); if (_e) _o->tflite_settings = std::unique_ptr<tflite::TFLiteSettingsT>(_e->UnPack(_resolver)); }
+  { auto _e = tflite_settings(); if (_e) { if(_o->tflite_settings) { _e->UnPackTo(_o->tflite_settings.get(), _resolver); } else { _o->tflite_settings = std::unique_ptr<tflite::TFLiteSettingsT>(_e->UnPack(_resolver)); } } }
   { auto _e = event_type(); _o->event_type = _e; }
-  { auto _e = result(); if (_e) _o->result = std::unique_ptr<tflite::BenchmarkResultT>(_e->UnPack(_resolver)); }
-  { auto _e = error(); if (_e) _o->error = std::unique_ptr<tflite::BenchmarkErrorT>(_e->UnPack(_resolver)); }
+  { auto _e = result(); if (_e) { if(_o->result) { _e->UnPackTo(_o->result.get(), _resolver); } else { _o->result = std::unique_ptr<tflite::BenchmarkResultT>(_e->UnPack(_resolver)); } } }
+  { auto _e = error(); if (_e) { if(_o->error) { _e->UnPackTo(_o->error.get(), _resolver); } else { _o->error = std::unique_ptr<tflite::BenchmarkErrorT>(_e->UnPack(_resolver)); } } }
   { auto _e = boottime_us(); _o->boottime_us = _e; }
   { auto _e = wallclock_us(); _o->wallclock_us = _e; }
 }
@@ -4144,17 +4276,30 @@ inline bool operator!=(const BestAccelerationDecisionT &lhs, const BestAccelerat
 }
 
 
+inline BestAccelerationDecisionT::BestAccelerationDecisionT(const BestAccelerationDecisionT &o)
+      : number_of_source_events(o.number_of_source_events),
+        min_latency_event((o.min_latency_event) ? new tflite::BenchmarkEventT(*o.min_latency_event) : nullptr),
+        min_inference_time_us(o.min_inference_time_us) {
+}
+
+inline BestAccelerationDecisionT &BestAccelerationDecisionT::operator=(BestAccelerationDecisionT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(number_of_source_events, o.number_of_source_events);
+  std::swap(min_latency_event, o.min_latency_event);
+  std::swap(min_inference_time_us, o.min_inference_time_us);
+  return *this;
+}
+
 inline BestAccelerationDecisionT *BestAccelerationDecision::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BestAccelerationDecisionT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BestAccelerationDecisionT>(new BestAccelerationDecisionT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BestAccelerationDecision::UnPackTo(BestAccelerationDecisionT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
   { auto _e = number_of_source_events(); _o->number_of_source_events = _e; }
-  { auto _e = min_latency_event(); if (_e) _o->min_latency_event = std::unique_ptr<tflite::BenchmarkEventT>(_e->UnPack(_resolver)); }
+  { auto _e = min_latency_event(); if (_e) { if(_o->min_latency_event) { _e->UnPackTo(_o->min_latency_event.get(), _resolver); } else { _o->min_latency_event = std::unique_ptr<tflite::BenchmarkEventT>(_e->UnPack(_resolver)); } } }
   { auto _e = min_inference_time_us(); _o->min_inference_time_us = _e; }
 }
 
@@ -4188,9 +4333,9 @@ inline bool operator!=(const BenchmarkInitializationFailureT &lhs, const Benchma
 
 
 inline BenchmarkInitializationFailureT *BenchmarkInitializationFailure::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BenchmarkInitializationFailureT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BenchmarkInitializationFailureT>(new BenchmarkInitializationFailureT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BenchmarkInitializationFailure::UnPackTo(BenchmarkInitializationFailureT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -4227,19 +4372,34 @@ inline bool operator!=(const MiniBenchmarkEventT &lhs, const MiniBenchmarkEventT
 }
 
 
+inline MiniBenchmarkEventT::MiniBenchmarkEventT(const MiniBenchmarkEventT &o)
+      : is_log_flushing_event(o.is_log_flushing_event),
+        best_acceleration_decision((o.best_acceleration_decision) ? new tflite::BestAccelerationDecisionT(*o.best_acceleration_decision) : nullptr),
+        initialization_failure((o.initialization_failure) ? new tflite::BenchmarkInitializationFailureT(*o.initialization_failure) : nullptr),
+        benchmark_event((o.benchmark_event) ? new tflite::BenchmarkEventT(*o.benchmark_event) : nullptr) {
+}
+
+inline MiniBenchmarkEventT &MiniBenchmarkEventT::operator=(MiniBenchmarkEventT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(is_log_flushing_event, o.is_log_flushing_event);
+  std::swap(best_acceleration_decision, o.best_acceleration_decision);
+  std::swap(initialization_failure, o.initialization_failure);
+  std::swap(benchmark_event, o.benchmark_event);
+  return *this;
+}
+
 inline MiniBenchmarkEventT *MiniBenchmarkEvent::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new MiniBenchmarkEventT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<MiniBenchmarkEventT>(new MiniBenchmarkEventT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void MiniBenchmarkEvent::UnPackTo(MiniBenchmarkEventT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
   { auto _e = is_log_flushing_event(); _o->is_log_flushing_event = _e; }
-  { auto _e = best_acceleration_decision(); if (_e) _o->best_acceleration_decision = std::unique_ptr<tflite::BestAccelerationDecisionT>(_e->UnPack(_resolver)); }
-  { auto _e = initialization_failure(); if (_e) _o->initialization_failure = std::unique_ptr<tflite::BenchmarkInitializationFailureT>(_e->UnPack(_resolver)); }
-  { auto _e = benchmark_event(); if (_e) _o->benchmark_event = std::unique_ptr<tflite::BenchmarkEventT>(_e->UnPack(_resolver)); }
+  { auto _e = best_acceleration_decision(); if (_e) { if(_o->best_acceleration_decision) { _e->UnPackTo(_o->best_acceleration_decision.get(), _resolver); } else { _o->best_acceleration_decision = std::unique_ptr<tflite::BestAccelerationDecisionT>(_e->UnPack(_resolver)); } } }
+  { auto _e = initialization_failure(); if (_e) { if(_o->initialization_failure) { _e->UnPackTo(_o->initialization_failure.get(), _resolver); } else { _o->initialization_failure = std::unique_ptr<tflite::BenchmarkInitializationFailureT>(_e->UnPack(_resolver)); } } }
+  { auto _e = benchmark_event(); if (_e) { if(_o->benchmark_event) { _e->UnPackTo(_o->benchmark_event.get(), _resolver); } else { _o->benchmark_event = std::unique_ptr<tflite::BenchmarkEventT>(_e->UnPack(_resolver)); } } }
 }
 
 inline flatbuffers::Offset<MiniBenchmarkEvent> MiniBenchmarkEvent::Pack(flatbuffers::FlatBufferBuilder &_fbb, const MiniBenchmarkEventT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -4277,9 +4437,9 @@ inline bool operator!=(const ModelFileT &lhs, const ModelFileT &rhs) {
 
 
 inline ModelFileT *ModelFile::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new ModelFileT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<ModelFileT>(new ModelFileT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void ModelFile::UnPackTo(ModelFileT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -4324,9 +4484,9 @@ inline bool operator!=(const BenchmarkStoragePathsT &lhs, const BenchmarkStorage
 
 
 inline BenchmarkStoragePathsT *BenchmarkStoragePaths::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new BenchmarkStoragePathsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<BenchmarkStoragePathsT>(new BenchmarkStoragePathsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void BenchmarkStoragePaths::UnPackTo(BenchmarkStoragePathsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
@@ -4365,18 +4525,32 @@ inline bool operator!=(const MinibenchmarkSettingsT &lhs, const MinibenchmarkSet
 }
 
 
+inline MinibenchmarkSettingsT::MinibenchmarkSettingsT(const MinibenchmarkSettingsT &o)
+      : model_file((o.model_file) ? new tflite::ModelFileT(*o.model_file) : nullptr),
+        storage_paths((o.storage_paths) ? new tflite::BenchmarkStoragePathsT(*o.storage_paths) : nullptr) {
+  settings_to_test.reserve(o.settings_to_test.size());
+  for (const auto &v : o.settings_to_test) { settings_to_test.emplace_back((v) ? new tflite::TFLiteSettingsT(*v) : nullptr); }
+}
+
+inline MinibenchmarkSettingsT &MinibenchmarkSettingsT::operator=(MinibenchmarkSettingsT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(settings_to_test, o.settings_to_test);
+  std::swap(model_file, o.model_file);
+  std::swap(storage_paths, o.storage_paths);
+  return *this;
+}
+
 inline MinibenchmarkSettingsT *MinibenchmarkSettings::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
-  auto _o = new MinibenchmarkSettingsT();
-  UnPackTo(_o, _resolver);
-  return _o;
+  auto _o = std::unique_ptr<MinibenchmarkSettingsT>(new MinibenchmarkSettingsT());
+  UnPackTo(_o.get(), _resolver);
+  return _o.release();
 }
 
 inline void MinibenchmarkSettings::UnPackTo(MinibenchmarkSettingsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
   (void)_o;
   (void)_resolver;
-  { auto _e = settings_to_test(); if (_e) { _o->settings_to_test.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->settings_to_test[_i] = std::unique_ptr<tflite::TFLiteSettingsT>(_e->Get(_i)->UnPack(_resolver)); } } }
-  { auto _e = model_file(); if (_e) _o->model_file = std::unique_ptr<tflite::ModelFileT>(_e->UnPack(_resolver)); }
-  { auto _e = storage_paths(); if (_e) _o->storage_paths = std::unique_ptr<tflite::BenchmarkStoragePathsT>(_e->UnPack(_resolver)); }
+  { auto _e = settings_to_test(); if (_e) { _o->settings_to_test.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->settings_to_test[_i]) { _e->Get(_i)->UnPackTo(_o->settings_to_test[_i].get(), _resolver); } else { _o->settings_to_test[_i] = std::unique_ptr<tflite::TFLiteSettingsT>(_e->Get(_i)->UnPack(_resolver)); }; } } }
+  { auto _e = model_file(); if (_e) { if(_o->model_file) { _e->UnPackTo(_o->model_file.get(), _resolver); } else { _o->model_file = std::unique_ptr<tflite::ModelFileT>(_e->UnPack(_resolver)); } } }
+  { auto _e = storage_paths(); if (_e) { if(_o->storage_paths) { _e->UnPackTo(_o->storage_paths.get(), _resolver); } else { _o->storage_paths = std::unique_ptr<tflite::BenchmarkStoragePathsT>(_e->UnPack(_resolver)); } } }
 }
 
 inline flatbuffers::Offset<MinibenchmarkSettings> MinibenchmarkSettings::Pack(flatbuffers::FlatBufferBuilder &_fbb, const MinibenchmarkSettingsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
@@ -4399,4 +4573,4 @@ inline flatbuffers::Offset<MinibenchmarkSettings> CreateMinibenchmarkSettings(fl
 
 }  // namespace tflite
 
-#endif  // FLATBUFFERS_GENERATED_CONFIGURATIONFORGENERATION_TFLITE_H_
+#endif  // FLATBUFFERS_GENERATED_CONFIGURATION_TFLITE_H_
diff --git a/tensorflow/lite/schema/schema_generated.h b/tensorflow/lite/schema/schema_generated.h
index 0de55f064ef..886eddaeaac 100755
--- a/tensorflow/lite/schema/schema_generated.h
+++ b/tensorflow/lite/schema/schema_generated.h
@@ -4023,10 +4023,10 @@ struct QuantizationParameters FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
            verifier.VerifyVector(scale()) &&
            VerifyOffset(verifier, VT_ZERO_POINT) &&
            verifier.VerifyVector(zero_point()) &&
-           VerifyField<uint8_t>(verifier, VT_DETAILS_TYPE) &&
+           VerifyField<uint8_t>(verifier, VT_DETAILS_TYPE, 1) &&
            VerifyOffset(verifier, VT_DETAILS) &&
            VerifyQuantizationDetails(verifier, details(), details_type()) &&
-           VerifyField<int32_t>(verifier, VT_QUANTIZED_DIMENSION) &&
+           VerifyField<int32_t>(verifier, VT_QUANTIZED_DIMENSION, 4) &&
            verifier.EndTable();
   }
   QuantizationParametersT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -4367,12 +4367,12 @@ struct DimensionMetadata FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FORMAT) &&
-           VerifyField<int32_t>(verifier, VT_DENSE_SIZE) &&
-           VerifyField<uint8_t>(verifier, VT_ARRAY_SEGMENTS_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_FORMAT, 1) &&
+           VerifyField<int32_t>(verifier, VT_DENSE_SIZE, 4) &&
+           VerifyField<uint8_t>(verifier, VT_ARRAY_SEGMENTS_TYPE, 1) &&
            VerifyOffset(verifier, VT_ARRAY_SEGMENTS) &&
            VerifySparseIndexVector(verifier, array_segments(), array_segments_type()) &&
-           VerifyField<uint8_t>(verifier, VT_ARRAY_INDICES_TYPE) &&
+           VerifyField<uint8_t>(verifier, VT_ARRAY_INDICES_TYPE, 1) &&
            VerifyOffset(verifier, VT_ARRAY_INDICES) &&
            VerifySparseIndexVector(verifier, array_indices(), array_indices_type()) &&
            verifier.EndTable();
@@ -4464,6 +4464,10 @@ struct SparsityParametersT : public flatbuffers::NativeTable {
   std::vector<int32_t> traversal_order{};
   std::vector<int32_t> block_map{};
   std::vector<std::unique_ptr<tflite::DimensionMetadataT>> dim_metadata{};
+  SparsityParametersT() = default;
+  SparsityParametersT(const SparsityParametersT &o);
+  SparsityParametersT(SparsityParametersT&&) FLATBUFFERS_NOEXCEPT = default;
+  SparsityParametersT &operator=(SparsityParametersT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct SparsityParameters FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -4562,6 +4566,10 @@ struct TensorT : public flatbuffers::NativeTable {
   bool is_variable = false;
   std::unique_ptr<tflite::SparsityParametersT> sparsity{};
   std::vector<int32_t> shape_signature{};
+  TensorT() = default;
+  TensorT(const TensorT &o);
+  TensorT(TensorT&&) FLATBUFFERS_NOEXCEPT = default;
+  TensorT &operator=(TensorT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct Tensor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -4605,13 +4613,13 @@ struct Tensor FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_SHAPE) &&
            verifier.VerifyVector(shape()) &&
-           VerifyField<int8_t>(verifier, VT_TYPE) &&
-           VerifyField<uint32_t>(verifier, VT_BUFFER) &&
+           VerifyField<int8_t>(verifier, VT_TYPE, 1) &&
+           VerifyField<uint32_t>(verifier, VT_BUFFER, 4) &&
            VerifyOffset(verifier, VT_NAME) &&
            verifier.VerifyString(name()) &&
            VerifyOffset(verifier, VT_QUANTIZATION) &&
            verifier.VerifyTable(quantization()) &&
-           VerifyField<uint8_t>(verifier, VT_IS_VARIABLE) &&
+           VerifyField<uint8_t>(verifier, VT_IS_VARIABLE, 1) &&
            VerifyOffset(verifier, VT_SPARSITY) &&
            verifier.VerifyTable(sparsity()) &&
            VerifyOffset(verifier, VT_SHAPE_SIGNATURE) &&
@@ -4752,12 +4760,12 @@ struct Conv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_PADDING) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_W) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_H) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_W_FACTOR) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_H_FACTOR) &&
+           VerifyField<int8_t>(verifier, VT_PADDING, 1) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_W, 4) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_H, 4) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_W_FACTOR, 4) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_H_FACTOR, 4) &&
            verifier.EndTable();
   }
   Conv2DOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -4869,14 +4877,14 @@ struct Conv3DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_PADDING) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_D) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_W) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_H) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_D_FACTOR) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_W_FACTOR) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_H_FACTOR) &&
+           VerifyField<int8_t>(verifier, VT_PADDING, 1) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_D, 4) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_W, 4) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_H, 4) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_D_FACTOR, 4) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_W_FACTOR, 4) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_H_FACTOR, 4) &&
            verifier.EndTable();
   }
   Conv3DOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -4988,12 +4996,12 @@ struct Pool2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_PADDING) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_W) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_H) &&
-           VerifyField<int32_t>(verifier, VT_FILTER_WIDTH) &&
-           VerifyField<int32_t>(verifier, VT_FILTER_HEIGHT) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
+           VerifyField<int8_t>(verifier, VT_PADDING, 1) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_W, 4) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_H, 4) &&
+           VerifyField<int32_t>(verifier, VT_FILTER_WIDTH, 4) &&
+           VerifyField<int32_t>(verifier, VT_FILTER_HEIGHT, 4) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
            verifier.EndTable();
   }
   Pool2DOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5100,13 +5108,13 @@ struct DepthwiseConv2DOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_PADDING) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_W) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_H) &&
-           VerifyField<int32_t>(verifier, VT_DEPTH_MULTIPLIER) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_W_FACTOR) &&
-           VerifyField<int32_t>(verifier, VT_DILATION_H_FACTOR) &&
+           VerifyField<int8_t>(verifier, VT_PADDING, 1) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_W, 4) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_H, 4) &&
+           VerifyField<int32_t>(verifier, VT_DEPTH_MULTIPLIER, 4) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_W_FACTOR, 4) &&
+           VerifyField<int32_t>(verifier, VT_DILATION_H_FACTOR, 4) &&
            verifier.EndTable();
   }
   DepthwiseConv2DOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5198,7 +5206,7 @@ struct ConcatEmbeddingsOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Ta
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUM_CHANNELS) &&
+           VerifyField<int32_t>(verifier, VT_NUM_CHANNELS, 4) &&
            VerifyOffset(verifier, VT_NUM_COLUMNS_PER_CHANNEL) &&
            verifier.VerifyVector(num_columns_per_channel()) &&
            VerifyOffset(verifier, VT_EMBEDDING_DIM_PER_CHANNEL) &&
@@ -5278,7 +5286,7 @@ struct LSHProjectionOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_TYPE, 1) &&
            verifier.EndTable();
   }
   LSHProjectionOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5340,9 +5348,9 @@ struct SVDFOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_RANK) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<int32_t>(verifier, VT_RANK, 4) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   SVDFOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5409,8 +5417,8 @@ struct RNNOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   RNNOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5477,9 +5485,9 @@ struct SequenceRNNOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR, 1) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   SequenceRNNOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5556,10 +5564,10 @@ struct BidirectionalSequenceRNNOptions FLATBUFFERS_FINAL_CLASS : private flatbuf
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<uint8_t>(verifier, VT_MERGE_OUTPUTS) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR, 1) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<uint8_t>(verifier, VT_MERGE_OUTPUTS, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   BidirectionalSequenceRNNOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5641,10 +5649,10 @@ struct FullyConnectedOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tabl
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<int8_t>(verifier, VT_WEIGHTS_FORMAT) &&
-           VerifyField<uint8_t>(verifier, VT_KEEP_NUM_DIMS) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<int8_t>(verifier, VT_WEIGHTS_FORMAT, 1) &&
+           VerifyField<uint8_t>(verifier, VT_KEEP_NUM_DIMS, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   FullyConnectedOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5711,7 +5719,7 @@ struct SoftmaxOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<float>(verifier, VT_BETA) &&
+           VerifyField<float>(verifier, VT_BETA, 4) &&
            verifier.EndTable();
   }
   SoftmaxOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5768,8 +5776,8 @@ struct ConcatenationOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_AXIS) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
+           VerifyField<int32_t>(verifier, VT_AXIS, 4) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
            verifier.EndTable();
   }
   ConcatenationOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5831,8 +5839,8 @@ struct AddOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<uint8_t>(verifier, VT_POT_SCALE_INT16) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<uint8_t>(verifier, VT_POT_SCALE_INT16, 1) &&
            verifier.EndTable();
   }
   AddOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5889,7 +5897,7 @@ struct MulOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
            verifier.EndTable();
   }
   MulOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -5941,7 +5949,7 @@ struct L2NormOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
            verifier.EndTable();
   }
   L2NormOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6008,10 +6016,10 @@ struct LocalResponseNormalizationOptions FLATBUFFERS_FINAL_CLASS : private flatb
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_RADIUS) &&
-           VerifyField<float>(verifier, VT_BIAS) &&
-           VerifyField<float>(verifier, VT_ALPHA) &&
-           VerifyField<float>(verifier, VT_BETA) &&
+           VerifyField<int32_t>(verifier, VT_RADIUS, 4) &&
+           VerifyField<float>(verifier, VT_BIAS, 4) &&
+           VerifyField<float>(verifier, VT_ALPHA, 4) &&
+           VerifyField<float>(verifier, VT_BETA, 4) &&
            verifier.EndTable();
   }
   LocalResponseNormalizationOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6098,11 +6106,11 @@ struct LSTMOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<float>(verifier, VT_CELL_CLIP) &&
-           VerifyField<float>(verifier, VT_PROJ_CLIP) &&
-           VerifyField<int8_t>(verifier, VT_KERNEL_TYPE) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<float>(verifier, VT_CELL_CLIP, 4) &&
+           VerifyField<float>(verifier, VT_PROJ_CLIP, 4) &&
+           VerifyField<int8_t>(verifier, VT_KERNEL_TYPE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   LSTMOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6194,11 +6202,11 @@ struct UnidirectionalSequenceLSTMOptions FLATBUFFERS_FINAL_CLASS : private flatb
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<float>(verifier, VT_CELL_CLIP) &&
-           VerifyField<float>(verifier, VT_PROJ_CLIP) &&
-           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<float>(verifier, VT_CELL_CLIP, 4) &&
+           VerifyField<float>(verifier, VT_PROJ_CLIP, 4) &&
+           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   UnidirectionalSequenceLSTMOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6295,12 +6303,12 @@ struct BidirectionalSequenceLSTMOptions FLATBUFFERS_FINAL_CLASS : private flatbu
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<float>(verifier, VT_CELL_CLIP) &&
-           VerifyField<float>(verifier, VT_PROJ_CLIP) &&
-           VerifyField<uint8_t>(verifier, VT_MERGE_OUTPUTS) &&
-           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<float>(verifier, VT_CELL_CLIP, 4) &&
+           VerifyField<float>(verifier, VT_PROJ_CLIP, 4) &&
+           VerifyField<uint8_t>(verifier, VT_MERGE_OUTPUTS, 1) &&
+           VerifyField<uint8_t>(verifier, VT_TIME_MAJOR, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   BidirectionalSequenceLSTMOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6382,8 +6390,8 @@ struct ResizeBilinearOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tabl
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_ALIGN_CORNERS) &&
-           VerifyField<uint8_t>(verifier, VT_HALF_PIXEL_CENTERS) &&
+           VerifyField<uint8_t>(verifier, VT_ALIGN_CORNERS, 1) &&
+           VerifyField<uint8_t>(verifier, VT_HALF_PIXEL_CENTERS, 1) &&
            verifier.EndTable();
   }
   ResizeBilinearOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6445,8 +6453,8 @@ struct ResizeNearestNeighborOptions FLATBUFFERS_FINAL_CLASS : private flatbuffer
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_ALIGN_CORNERS) &&
-           VerifyField<uint8_t>(verifier, VT_HALF_PIXEL_CENTERS) &&
+           VerifyField<uint8_t>(verifier, VT_ALIGN_CORNERS, 1) &&
+           VerifyField<uint8_t>(verifier, VT_HALF_PIXEL_CENTERS, 1) &&
            verifier.EndTable();
   }
   ResizeNearestNeighborOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6503,7 +6511,7 @@ struct CallOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint32_t>(verifier, VT_SUBGRAPH) &&
+           VerifyField<uint32_t>(verifier, VT_SUBGRAPH, 4) &&
            verifier.EndTable();
   }
   CallOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6783,9 +6791,9 @@ struct SkipGramOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NGRAM_SIZE) &&
-           VerifyField<int32_t>(verifier, VT_MAX_SKIP_SIZE) &&
-           VerifyField<uint8_t>(verifier, VT_INCLUDE_ALL_NGRAMS) &&
+           VerifyField<int32_t>(verifier, VT_NGRAM_SIZE, 4) &&
+           VerifyField<int32_t>(verifier, VT_MAX_SKIP_SIZE, 4) &&
+           VerifyField<uint8_t>(verifier, VT_INCLUDE_ALL_NGRAMS, 1) &&
            verifier.EndTable();
   }
   SkipGramOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6847,7 +6855,7 @@ struct SpaceToDepthOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_BLOCK_SIZE) &&
+           VerifyField<int32_t>(verifier, VT_BLOCK_SIZE, 4) &&
            verifier.EndTable();
   }
   SpaceToDepthOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6899,7 +6907,7 @@ struct DepthToSpaceOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_BLOCK_SIZE) &&
+           VerifyField<int32_t>(verifier, VT_BLOCK_SIZE, 4) &&
            verifier.EndTable();
   }
   DepthToSpaceOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -6956,8 +6964,8 @@ struct SubOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
-           VerifyField<uint8_t>(verifier, VT_POT_SCALE_INT16) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
+           VerifyField<uint8_t>(verifier, VT_POT_SCALE_INT16, 1) &&
            verifier.EndTable();
   }
   SubOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7014,7 +7022,7 @@ struct DivOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION) &&
+           VerifyField<int8_t>(verifier, VT_FUSED_ACTIVATION_FUNCTION, 1) &&
            verifier.EndTable();
   }
   DivOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7105,7 +7113,7 @@ struct EmbeddingLookupSparseOptions FLATBUFFERS_FINAL_CLASS : private flatbuffer
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_COMBINER) &&
+           VerifyField<int8_t>(verifier, VT_COMBINER, 1) &&
            verifier.EndTable();
   }
   EmbeddingLookupSparseOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7162,8 +7170,8 @@ struct GatherOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_AXIS) &&
-           VerifyField<int32_t>(verifier, VT_BATCH_DIMS) &&
+           VerifyField<int32_t>(verifier, VT_AXIS, 4) &&
+           VerifyField<int32_t>(verifier, VT_BATCH_DIMS, 4) &&
            verifier.EndTable();
   }
   GatherOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7337,7 +7345,7 @@ struct ReducerOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_KEEP_DIMS) &&
+           VerifyField<uint8_t>(verifier, VT_KEEP_DIMS, 1) &&
            verifier.EndTable();
   }
   ReducerOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7451,7 +7459,7 @@ struct SplitOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUM_SPLITS) &&
+           VerifyField<int32_t>(verifier, VT_NUM_SPLITS, 4) &&
            verifier.EndTable();
   }
   SplitOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7503,7 +7511,7 @@ struct SplitVOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUM_SPLITS) &&
+           VerifyField<int32_t>(verifier, VT_NUM_SPLITS, 4) &&
            verifier.EndTable();
   }
   SplitVOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7575,11 +7583,11 @@ struct StridedSliceOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_BEGIN_MASK) &&
-           VerifyField<int32_t>(verifier, VT_END_MASK) &&
-           VerifyField<int32_t>(verifier, VT_ELLIPSIS_MASK) &&
-           VerifyField<int32_t>(verifier, VT_NEW_AXIS_MASK) &&
-           VerifyField<int32_t>(verifier, VT_SHRINK_AXIS_MASK) &&
+           VerifyField<int32_t>(verifier, VT_BEGIN_MASK, 4) &&
+           VerifyField<int32_t>(verifier, VT_END_MASK, 4) &&
+           VerifyField<int32_t>(verifier, VT_ELLIPSIS_MASK, 4) &&
+           VerifyField<int32_t>(verifier, VT_NEW_AXIS_MASK, 4) &&
+           VerifyField<int32_t>(verifier, VT_SHRINK_AXIS_MASK, 4) &&
            verifier.EndTable();
   }
   StridedSliceOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7695,8 +7703,8 @@ struct CastOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_IN_DATA_TYPE) &&
-           VerifyField<int8_t>(verifier, VT_OUT_DATA_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_IN_DATA_TYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_OUT_DATA_TYPE, 1) &&
            verifier.EndTable();
   }
   CastOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7870,7 +7878,7 @@ struct ArgMaxOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_OUTPUT_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_OUTPUT_TYPE, 1) &&
            verifier.EndTable();
   }
   ArgMaxOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -7922,7 +7930,7 @@ struct ArgMinOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_OUTPUT_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_OUTPUT_TYPE, 1) &&
            verifier.EndTable();
   }
   ArgMinOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -8257,9 +8265,9 @@ struct TransposeConvOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_PADDING) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_W) &&
-           VerifyField<int32_t>(verifier, VT_STRIDE_H) &&
+           VerifyField<int8_t>(verifier, VT_PADDING, 1) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_W, 4) &&
+           VerifyField<int32_t>(verifier, VT_STRIDE_H, 4) &&
            verifier.EndTable();
   }
   TransposeConvOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -8360,7 +8368,7 @@ struct SparseToDenseOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_VALIDATE_INDICES) &&
+           VerifyField<uint8_t>(verifier, VT_VALIDATE_INDICES, 1) &&
            verifier.EndTable();
   }
   SparseToDenseOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -8490,7 +8498,7 @@ struct ShapeOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_OUT_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_OUT_TYPE, 1) &&
            verifier.EndTable();
   }
   ShapeOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -8635,10 +8643,10 @@ struct FakeQuantOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<float>(verifier, VT_MIN) &&
-           VerifyField<float>(verifier, VT_MAX) &&
-           VerifyField<int32_t>(verifier, VT_NUM_BITS) &&
-           VerifyField<uint8_t>(verifier, VT_NARROW_RANGE) &&
+           VerifyField<float>(verifier, VT_MIN, 4) &&
+           VerifyField<float>(verifier, VT_MAX, 4) &&
+           VerifyField<int32_t>(verifier, VT_NUM_BITS, 4) &&
+           VerifyField<uint8_t>(verifier, VT_NARROW_RANGE, 1) &&
            verifier.EndTable();
   }
   FakeQuantOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -8710,8 +8718,8 @@ struct PackOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_VALUES_COUNT) &&
-           VerifyField<int32_t>(verifier, VT_AXIS) &&
+           VerifyField<int32_t>(verifier, VT_VALUES_COUNT, 4) &&
+           VerifyField<int32_t>(verifier, VT_AXIS, 4) &&
            verifier.EndTable();
   }
   PackOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -8807,7 +8815,7 @@ struct OneHotOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_AXIS) &&
+           VerifyField<int32_t>(verifier, VT_AXIS, 4) &&
            verifier.EndTable();
   }
   OneHotOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9020,8 +9028,8 @@ struct UnpackOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_NUM) &&
-           VerifyField<int32_t>(verifier, VT_AXIS) &&
+           VerifyField<int32_t>(verifier, VT_NUM, 4) &&
+           VerifyField<int32_t>(verifier, VT_AXIS, 4) &&
            verifier.EndTable();
   }
   UnpackOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9312,7 +9320,7 @@ struct LeakyReluOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<float>(verifier, VT_ALPHA) &&
+           VerifyField<float>(verifier, VT_ALPHA, 4) &&
            verifier.EndTable();
   }
   LeakyReluOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9403,7 +9411,7 @@ struct MirrorPadOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_MODE) &&
+           VerifyField<int8_t>(verifier, VT_MODE, 1) &&
            verifier.EndTable();
   }
   MirrorPadOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9455,7 +9463,7 @@ struct UniqueOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_IDX_OUT_TYPE) &&
+           VerifyField<int8_t>(verifier, VT_IDX_OUT_TYPE, 1) &&
            verifier.EndTable();
   }
   UniqueOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9668,8 +9676,8 @@ struct ReverseSequenceOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Tab
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_SEQ_DIM) &&
-           VerifyField<int32_t>(verifier, VT_BATCH_DIM) &&
+           VerifyField<int32_t>(verifier, VT_SEQ_DIM, 4) &&
+           VerifyField<int32_t>(verifier, VT_BATCH_DIM, 4) &&
            verifier.EndTable();
   }
   ReverseSequenceOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9848,8 +9856,8 @@ struct IfOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_THEN_SUBGRAPH_INDEX) &&
-           VerifyField<int32_t>(verifier, VT_ELSE_SUBGRAPH_INDEX) &&
+           VerifyField<int32_t>(verifier, VT_THEN_SUBGRAPH_INDEX, 4) &&
+           VerifyField<int32_t>(verifier, VT_ELSE_SUBGRAPH_INDEX, 4) &&
            verifier.EndTable();
   }
   IfOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9906,7 +9914,7 @@ struct CallOnceOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_INIT_SUBGRAPH_INDEX) &&
+           VerifyField<int32_t>(verifier, VT_INIT_SUBGRAPH_INDEX, 4) &&
            verifier.EndTable();
   }
   CallOnceOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -9963,8 +9971,8 @@ struct WhileOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_COND_SUBGRAPH_INDEX) &&
-           VerifyField<int32_t>(verifier, VT_BODY_SUBGRAPH_INDEX) &&
+           VerifyField<int32_t>(verifier, VT_COND_SUBGRAPH_INDEX, 4) &&
+           VerifyField<int32_t>(verifier, VT_BODY_SUBGRAPH_INDEX, 4) &&
            verifier.EndTable();
   }
   WhileOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -10265,9 +10273,9 @@ struct BatchMatMulOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_ADJ_X) &&
-           VerifyField<uint8_t>(verifier, VT_ADJ_Y) &&
-           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS) &&
+           VerifyField<uint8_t>(verifier, VT_ADJ_X, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ADJ_Y, 1) &&
+           VerifyField<uint8_t>(verifier, VT_ASYMMETRIC_QUANTIZE_INPUTS, 1) &&
            verifier.EndTable();
   }
   BatchMatMulOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -10334,8 +10342,8 @@ struct CumsumOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_EXCLUSIVE) &&
-           VerifyField<uint8_t>(verifier, VT_REVERSE) &&
+           VerifyField<uint8_t>(verifier, VT_EXCLUSIVE, 1) &&
+           VerifyField<uint8_t>(verifier, VT_REVERSE, 1) &&
            verifier.EndTable();
   }
   CumsumOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -10480,9 +10488,9 @@ struct HashtableOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int32_t>(verifier, VT_TABLE_ID) &&
-           VerifyField<int8_t>(verifier, VT_KEY_DTYPE) &&
-           VerifyField<int8_t>(verifier, VT_VALUE_DTYPE) &&
+           VerifyField<int32_t>(verifier, VT_TABLE_ID, 4) &&
+           VerifyField<int8_t>(verifier, VT_KEY_DTYPE, 1) &&
+           VerifyField<int8_t>(verifier, VT_VALUE_DTYPE, 1) &&
            verifier.EndTable();
   }
   HashtableOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -10821,8 +10829,8 @@ struct RandomOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int64_t>(verifier, VT_SEED) &&
-           VerifyField<int64_t>(verifier, VT_SEED2) &&
+           VerifyField<int64_t>(verifier, VT_SEED, 8) &&
+           VerifyField<int64_t>(verifier, VT_SEED2, 8) &&
            verifier.EndTable();
   }
   RandomOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -10941,7 +10949,7 @@ struct GeluOptions FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint8_t>(verifier, VT_APPROXIMATE) &&
+           VerifyField<uint8_t>(verifier, VT_APPROXIMATE, 1) &&
            verifier.EndTable();
   }
   GeluOptionsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -11047,11 +11055,11 @@ struct OperatorCode FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<int8_t>(verifier, VT_DEPRECATED_BUILTIN_CODE) &&
+           VerifyField<int8_t>(verifier, VT_DEPRECATED_BUILTIN_CODE, 1) &&
            VerifyOffset(verifier, VT_CUSTOM_CODE) &&
            verifier.VerifyString(custom_code()) &&
-           VerifyField<int32_t>(verifier, VT_VERSION) &&
-           VerifyField<int32_t>(verifier, VT_BUILTIN_CODE) &&
+           VerifyField<int32_t>(verifier, VT_VERSION, 4) &&
+           VerifyField<int32_t>(verifier, VT_BUILTIN_CODE, 4) &&
            verifier.EndTable();
   }
   OperatorCodeT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -11524,17 +11532,17 @@ struct Operator FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint32_t>(verifier, VT_OPCODE_INDEX) &&
+           VerifyField<uint32_t>(verifier, VT_OPCODE_INDEX, 4) &&
            VerifyOffset(verifier, VT_INPUTS) &&
            verifier.VerifyVector(inputs()) &&
            VerifyOffset(verifier, VT_OUTPUTS) &&
            verifier.VerifyVector(outputs()) &&
-           VerifyField<uint8_t>(verifier, VT_BUILTIN_OPTIONS_TYPE) &&
+           VerifyField<uint8_t>(verifier, VT_BUILTIN_OPTIONS_TYPE, 1) &&
            VerifyOffset(verifier, VT_BUILTIN_OPTIONS) &&
            VerifyBuiltinOptions(verifier, builtin_options(), builtin_options_type()) &&
            VerifyOffset(verifier, VT_CUSTOM_OPTIONS) &&
            verifier.VerifyVector(custom_options()) &&
-           VerifyField<int8_t>(verifier, VT_CUSTOM_OPTIONS_FORMAT) &&
+           VerifyField<int8_t>(verifier, VT_CUSTOM_OPTIONS_FORMAT, 1) &&
            VerifyOffset(verifier, VT_MUTATING_VARIABLE_INPUTS) &&
            verifier.VerifyVector(mutating_variable_inputs()) &&
            VerifyOffset(verifier, VT_INTERMEDIATES) &&
@@ -12118,6 +12126,10 @@ struct SubGraphT : public flatbuffers::NativeTable {
   std::vector<int32_t> outputs{};
   std::vector<std::unique_ptr<tflite::OperatorT>> operators{};
   std::string name{};
+  SubGraphT() = default;
+  SubGraphT(const SubGraphT &o);
+  SubGraphT(SubGraphT&&) FLATBUFFERS_NOEXCEPT = default;
+  SubGraphT &operator=(SubGraphT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct SubGraph FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -12321,7 +12333,7 @@ struct Metadata FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_NAME) &&
            verifier.VerifyString(name()) &&
-           VerifyField<uint32_t>(verifier, VT_BUFFER) &&
+           VerifyField<uint32_t>(verifier, VT_BUFFER, 4) &&
            verifier.EndTable();
   }
   MetadataT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -12396,7 +12408,7 @@ struct TensorMap FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
     return VerifyTableStart(verifier) &&
            VerifyOffset(verifier, VT_NAME) &&
            verifier.VerifyString(name()) &&
-           VerifyField<uint32_t>(verifier, VT_TENSOR_INDEX) &&
+           VerifyField<uint32_t>(verifier, VT_TENSOR_INDEX, 4) &&
            verifier.EndTable();
   }
   TensorMapT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -12454,6 +12466,10 @@ struct SignatureDefT : public flatbuffers::NativeTable {
   std::vector<std::unique_ptr<tflite::TensorMapT>> outputs{};
   std::string signature_key{};
   uint32_t subgraph_index = 0;
+  SignatureDefT() = default;
+  SignatureDefT(const SignatureDefT &o);
+  SignatureDefT(SignatureDefT&&) FLATBUFFERS_NOEXCEPT = default;
+  SignatureDefT &operator=(SignatureDefT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct SignatureDef FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -12487,7 +12503,7 @@ struct SignatureDef FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
            verifier.VerifyVectorOfTables(outputs()) &&
            VerifyOffset(verifier, VT_SIGNATURE_KEY) &&
            verifier.VerifyString(signature_key()) &&
-           VerifyField<uint32_t>(verifier, VT_SUBGRAPH_INDEX) &&
+           VerifyField<uint32_t>(verifier, VT_SUBGRAPH_INDEX, 4) &&
            verifier.EndTable();
   }
   SignatureDefT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
@@ -12565,6 +12581,10 @@ struct ModelT : public flatbuffers::NativeTable {
   std::vector<int32_t> metadata_buffer{};
   std::vector<std::unique_ptr<tflite::MetadataT>> metadata{};
   std::vector<std::unique_ptr<tflite::SignatureDefT>> signature_defs{};
+  ModelT() = default;
+  ModelT(const ModelT &o);
+  ModelT(ModelT&&) FLATBUFFERS_NOEXCEPT = default;
+  ModelT &operator=(ModelT o) FLATBUFFERS_NOEXCEPT;
 };
 
 struct Model FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
@@ -12606,7 +12626,7 @@ struct Model FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
   }
   bool Verify(flatbuffers::Verifier &verifier) const {
     return VerifyTableStart(verifier) &&
-           VerifyField<uint32_t>(verifier, VT_VERSION) &&
+           VerifyField<uint32_t>(verifier, VT_VERSION, 4) &&
            VerifyOffset(verifier, VT_OPERATOR_CODES) &&
            verifier.VerifyVector(operator_codes()) &&
            verifier.VerifyVectorOfTables(operator_codes()) &&
@@ -12917,6 +12937,20 @@ inline flatbuffers::Offset<DimensionMetadata> CreateDimensionMetadata(flatbuffer
       _array_indices);
 }
 
+inline SparsityParametersT::SparsityParametersT(const SparsityParametersT &o)
+      : traversal_order(o.traversal_order),
+        block_map(o.block_map) {
+  dim_metadata.reserve(o.dim_metadata.size());
+  for (const auto &v : o.dim_metadata) { dim_metadata.emplace_back((v) ? new tflite::DimensionMetadataT(*v) : nullptr); }
+}
+
+inline SparsityParametersT &SparsityParametersT::operator=(SparsityParametersT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(traversal_order, o.traversal_order);
+  std::swap(block_map, o.block_map);
+  std::swap(dim_metadata, o.dim_metadata);
+  return *this;
+}
+
 inline SparsityParametersT *SparsityParameters::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
   auto _o = std::unique_ptr<SparsityParametersT>(new SparsityParametersT());
   UnPackTo(_o.get(), _resolver);
@@ -12949,6 +12983,29 @@ inline flatbuffers::Offset<SparsityParameters> CreateSparsityParameters(flatbuff
       _dim_metadata);
 }
 
+inline TensorT::TensorT(const TensorT &o)
+      : shape(o.shape),
+        type(o.type),
+        buffer(o.buffer),
+        name(o.name),
+        quantization((o.quantization) ? new tflite::QuantizationParametersT(*o.quantization) : nullptr),
+        is_variable(o.is_variable),
+        sparsity((o.sparsity) ? new tflite::SparsityParametersT(*o.sparsity) : nullptr),
+        shape_signature(o.shape_signature) {
+}
+
+inline TensorT &TensorT::operator=(TensorT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(shape, o.shape);
+  std::swap(type, o.type);
+  std::swap(buffer, o.buffer);
+  std::swap(name, o.name);
+  std::swap(quantization, o.quantization);
+  std::swap(is_variable, o.is_variable);
+  std::swap(sparsity, o.sparsity);
+  std::swap(shape_signature, o.shape_signature);
+  return *this;
+}
+
 inline TensorT *Tensor::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
   auto _o = std::unique_ptr<TensorT>(new TensorT());
   UnPackTo(_o.get(), _resolver);
@@ -16198,6 +16255,25 @@ inline flatbuffers::Offset<Operator> CreateOperator(flatbuffers::FlatBufferBuild
       _intermediates);
 }
 
+inline SubGraphT::SubGraphT(const SubGraphT &o)
+      : inputs(o.inputs),
+        outputs(o.outputs),
+        name(o.name) {
+  tensors.reserve(o.tensors.size());
+  for (const auto &v : o.tensors) { tensors.emplace_back((v) ? new tflite::TensorT(*v) : nullptr); }
+  operators.reserve(o.operators.size());
+  for (const auto &v : o.operators) { operators.emplace_back((v) ? new tflite::OperatorT(*v) : nullptr); }
+}
+
+inline SubGraphT &SubGraphT::operator=(SubGraphT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(tensors, o.tensors);
+  std::swap(inputs, o.inputs);
+  std::swap(outputs, o.outputs);
+  std::swap(operators, o.operators);
+  std::swap(name, o.name);
+  return *this;
+}
+
 inline SubGraphT *SubGraph::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
   auto _o = std::unique_ptr<SubGraphT>(new SubGraphT());
   UnPackTo(_o.get(), _resolver);
@@ -16321,6 +16397,23 @@ inline flatbuffers::Offset<TensorMap> CreateTensorMap(flatbuffers::FlatBufferBui
       _tensor_index);
 }
 
+inline SignatureDefT::SignatureDefT(const SignatureDefT &o)
+      : signature_key(o.signature_key),
+        subgraph_index(o.subgraph_index) {
+  inputs.reserve(o.inputs.size());
+  for (const auto &v : o.inputs) { inputs.emplace_back((v) ? new tflite::TensorMapT(*v) : nullptr); }
+  outputs.reserve(o.outputs.size());
+  for (const auto &v : o.outputs) { outputs.emplace_back((v) ? new tflite::TensorMapT(*v) : nullptr); }
+}
+
+inline SignatureDefT &SignatureDefT::operator=(SignatureDefT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(inputs, o.inputs);
+  std::swap(outputs, o.outputs);
+  std::swap(signature_key, o.signature_key);
+  std::swap(subgraph_index, o.subgraph_index);
+  return *this;
+}
+
 inline SignatureDefT *SignatureDef::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
   auto _o = std::unique_ptr<SignatureDefT>(new SignatureDefT());
   UnPackTo(_o.get(), _resolver);
@@ -16356,6 +16449,34 @@ inline flatbuffers::Offset<SignatureDef> CreateSignatureDef(flatbuffers::FlatBuf
       _subgraph_index);
 }
 
+inline ModelT::ModelT(const ModelT &o)
+      : version(o.version),
+        description(o.description),
+        metadata_buffer(o.metadata_buffer) {
+  operator_codes.reserve(o.operator_codes.size());
+  for (const auto &v : o.operator_codes) { operator_codes.emplace_back((v) ? new tflite::OperatorCodeT(*v) : nullptr); }
+  subgraphs.reserve(o.subgraphs.size());
+  for (const auto &v : o.subgraphs) { subgraphs.emplace_back((v) ? new tflite::SubGraphT(*v) : nullptr); }
+  buffers.reserve(o.buffers.size());
+  for (const auto &v : o.buffers) { buffers.emplace_back((v) ? new tflite::BufferT(*v) : nullptr); }
+  metadata.reserve(o.metadata.size());
+  for (const auto &v : o.metadata) { metadata.emplace_back((v) ? new tflite::MetadataT(*v) : nullptr); }
+  signature_defs.reserve(o.signature_defs.size());
+  for (const auto &v : o.signature_defs) { signature_defs.emplace_back((v) ? new tflite::SignatureDefT(*v) : nullptr); }
+}
+
+inline ModelT &ModelT::operator=(ModelT o) FLATBUFFERS_NOEXCEPT {
+  std::swap(version, o.version);
+  std::swap(operator_codes, o.operator_codes);
+  std::swap(subgraphs, o.subgraphs);
+  std::swap(description, o.description);
+  std::swap(buffers, o.buffers);
+  std::swap(metadata_buffer, o.metadata_buffer);
+  std::swap(metadata, o.metadata);
+  std::swap(signature_defs, o.signature_defs);
+  return *this;
+}
+
 inline ModelT *Model::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
   auto _o = std::unique_ptr<ModelT>(new ModelT());
   UnPackTo(_o.get(), _resolver);
@@ -19111,6 +19232,11 @@ inline bool ModelBufferHasIdentifier(const void *buf) {
       buf, ModelIdentifier());
 }
 
+inline bool SizePrefixedModelBufferHasIdentifier(const void *buf) {
+  return flatbuffers::BufferHasIdentifier(
+      buf, ModelIdentifier(), true);
+}
+
 inline bool VerifyModelBuffer(
     flatbuffers::Verifier &verifier) {
   return verifier.VerifyBuffer<tflite::Model>(ModelIdentifier());
diff --git a/tensorflow/lite/tools/cmake/modules/flatbuffers.cmake b/tensorflow/lite/tools/cmake/modules/flatbuffers.cmake
index ac91a03a48a..d1d06d09505 100644
--- a/tensorflow/lite/tools/cmake/modules/flatbuffers.cmake
+++ b/tensorflow/lite/tools/cmake/modules/flatbuffers.cmake
@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(
   flatbuffers
   GIT_REPOSITORY https://github.com/google/flatbuffers
   # Sync with tensorflow/third_party/flatbuffers/workspace.bzl
-  GIT_TAG v2.0.5
+  GIT_TAG v2.0.6
   GIT_SHALLOW TRUE
   GIT_PROGRESS TRUE
   SOURCE_DIR "${CMAKE_BINARY_DIR}/flatbuffers"
diff --git a/tensorflow/opensource_only.files b/tensorflow/opensource_only.files
index 8cf130f5509..8ccaae2b9c6 100644
--- a/tensorflow/opensource_only.files
+++ b/tensorflow/opensource_only.files
@@ -61,6 +61,7 @@ tensorflow/lite/delegates/gpu/cl/serialization_generated.h:
 tensorflow/lite/delegates/gpu/common/gpu_model_generated.h:
 tensorflow/lite/delegates/gpu/common/task/serialization_base_generated.h:
 tensorflow/lite/delegates/hexagon/hexagon_nn/BUILD:
+tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h:
 tensorflow/lite/experimental/acceleration/mini_benchmark/libjpeg.h:
 tensorflow/lite/experimental/acceleration/mini_benchmark/special_rules.bzl:
 tensorflow/lite/ios/BUILD:
diff --git a/third_party/flatbuffers/workspace.bzl b/third_party/flatbuffers/workspace.bzl
index 84aae9dc51f..ceb8a2720da 100644
--- a/third_party/flatbuffers/workspace.bzl
+++ b/third_party/flatbuffers/workspace.bzl
@@ -5,9 +5,9 @@ load("//third_party:repo.bzl", "tf_http_archive", "tf_mirror_urls")
 def repo():
     tf_http_archive(
         name = "flatbuffers",
-        strip_prefix = "flatbuffers-2.0.5",
-        sha256 = "b01e97c988c429e164c5c7df9e87c80007ca87f593c0d73733ba536ddcbc8f98",
-        urls = tf_mirror_urls("https://github.com/google/flatbuffers/archive/v2.0.5.tar.gz"),
+        strip_prefix = "flatbuffers-2.0.6",
+        sha256 = "e2dc24985a85b278dd06313481a9ca051d048f9474e0f199e372fea3ea4248c9",
+        urls = tf_mirror_urls("https://github.com/google/flatbuffers/archive/v2.0.6.tar.gz"),
         build_file = "//third_party/flatbuffers:flatbuffers.BUILD",
         system_build_file = "//third_party/flatbuffers:BUILD.system",
         link_files = {
