This test shows segfaults, at least on some system.
PyTorch CI HUD indicates some failures with it are known.

Author: Alexander Grund (TU Dresden)

diff --git a/test/inductor/test_flex_attention.py b/test/inductor/test_flex_attention.py
index 740faa0b375..ea5e311b7cd 100644
--- a/test/inductor/test_flex_attention.py
+++ b/test/inductor/test_flex_attention.py
@@ -3474,6 +3474,7 @@ def forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1):
         )
         FileCheck().check("BLOCK_M : tl.constexpr = 16").run(code[0])
 
+    @unittest.skip("Segfaults on CPU")
     @supported_platform
     def test_block_mask_non_divisible(self, device):
         seq = torch.arange(1023, device=device) // 128
