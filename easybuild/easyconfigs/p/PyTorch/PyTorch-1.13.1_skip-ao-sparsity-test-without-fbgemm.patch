Those tests (from test_ao_sparsity) require FBGEMM which may not be available.
So add the skip decorator.
See https://github.com/pytorch/pytorch/issues/87364

Author: Alexander Grund (TU Dresden)

diff --git a/test/ao/sparsity/test_composability.py b/test/ao/sparsity/test_composability.py
index 6a1b6067a4c..b2eed72e3e3 100644
--- a/test/ao/sparsity/test_composability.py
+++ b/test/ao/sparsity/test_composability.py
@@ -9,6 +9,7 @@ import torch.ao.quantization as tq
 from torch import nn
 from torch.ao import sparsity
 from torch.testing._internal.common_utils import TestCase
+from torch.testing._internal.common_quantization import skipIfNoFBGEMM
 from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, convert_to_reference_fx, prepare_qat_fx
 from torch.ao.sparsity import fqn_to_module
 
@@ -62,6 +63,7 @@ def _calculate_sparsity(tensor):
 # This series of tests are to check the composability goals for sparsity and quantization. Namely
 # that performing quantization and sparsity model manipulations in various orderings
 # does not cause problems
+@skipIfNoFBGEMM
 class TestComposability(TestCase):
     # This test checks whether performing quantization prepare before sparse prepare
     # causes any issues and verifies that the correct observers are inserted and that
