The test often times out and seems to be considered flaky by PyTorch:
https://github.com/pytorch/pytorch/issues/78068

Author: Alexander Grund (TU Dresden)
diff --git a/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py b/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py
index 730b2c2c0ac..5f9b9545700 100644
--- a/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py
+++ b/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py
@@ -6,6 +6,7 @@ import itertools
 import math
 import pickle
 import sys
+from unittest import skip
 
 import torch
 import torch.distributed as dist
@@ -2432,6 +2432,7 @@ class TestShardedTensorFromLocalShards(ShardedTensorTestBase):
     @with_comms
     @skip_if_lt_x_gpu(4)
     @requires_nccl()
+    @skip("Times out often")
     def test_init_from_local_shards(self):
         local_shard_metadata = ShardMetadata(
             shard_offsets=[(self.rank // 2) * 5, (self.rank % 2) * 5],
