Some tests fail when run with anything but 2 GPUs and others when run with anything but 2 or 4 GPUs.
So limit to 2 GPUs.

See https://github.com/pytorch/pytorch/issues/59548

Author: Alexander Grund (TU Dresden)

diff --git a/test/distributed/optim/test_zero_redundancy_optimizer.py b/test/distributed/optim/test_zero_redundancy_optimizer.py
index 06f1b4f484..bc82f6c304 100644
--- a/test/distributed/optim/test_zero_redundancy_optimizer.py
+++ b/test/distributed/optim/test_zero_redundancy_optimizer.py
@@ -233,7 +233,7 @@ class TestZeroRedundancyOptimizerSingleRank(TestZeroRedundancyOptimizer):
 class TestZeroRedundancyOptimizerDistributed(TestZeroRedundancyOptimizer):
     @property
     def world_size(self):
-        return min(4, max(2, torch.cuda.device_count()))
+        return 2
 
     @skip_if_rocm
     def test_step(self):
