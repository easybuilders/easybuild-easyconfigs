test_ring_flex_attention and test_ring_flex_attention_mask both fail in similar ways:

> torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped
> ...
>  Developer debug context: module: _warnings, qualname: warn, skip reason: <missing reason>

See https://github.com/pytorch/pytorch/pull/161667#issuecomment-3298676991 
    & https://github.com/pytorch/pytorch/issues/162843

Author: Alexander Grund (TU Dresden)

diff --git a/test/distributed/tensor/test_attention.py b/test/distributed/tensor/test_attention.py
index a2543d443e4..a28fb45e992 100644
--- a/test/distributed/tensor/test_attention.py
+++ b/test/distributed/tensor/test_attention.py
@@ -531,6 +531,7 @@ def generate_doc_mask_mod(
     return doc_mask_mod
 
 
+@unittest.skip("FAILS")
 class RingFlexAttentionTest(DTensorTestBase):
     @property
     def world_size(self) -> int:
