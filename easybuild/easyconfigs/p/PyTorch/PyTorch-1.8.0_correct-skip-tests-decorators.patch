Based off https://github.com/pytorch/pytorch/pull/53736
Fixed to apply to PyTorch 1.8.0 by Simon Branford (University of Birmingham)
--- test/test_spectral_ops.py	2021-03-12 19:36:32.910758000 +0000
+++ test/test_spectral_ops.py	2021-03-12 19:41:42.314654931 +0000
@@ -185,6 +185,7 @@
         with self.assertRaisesRegex(RuntimeError, match):
             op(t)
 
+    @onlyOnCPUAndCUDA
     def test_fft_invalid_dtypes(self, device):
         t = torch.randn(64, device=device, dtype=torch.complex128)
 
@@ -599,7 +600,6 @@
         _test_complex((40, 60, 3, 80), 3, lambda x: x.transpose(2, 0).select(0, 2)[5:55, :, 10:])
         _test_complex((30, 55, 50, 22), 3, lambda x: x[:, 3:53, 15:40, 1:21])
 
-    @skipCUDAIfRocm
     @skipCPUIfNoMkl
     @onlyOnCPUAndCUDA
     @dtypes(torch.double)
@@ -680,8 +680,8 @@
                         self.assertEqual(torch.backends.cuda.cufft_plan_cache.max_size, 11)  # default is cuda:1
 
     # passes on ROCm w/ python 2.7, fails w/ python 3.6
-    @skipCUDAIfRocm
     @skipCPUIfNoMkl
+    @onlyOnCPUAndCUDA
     @dtypes(torch.double)
     def test_stft(self, device, dtype):
         if not TEST_LIBROSA:
@@ -712,9 +712,8 @@
             else:
                 window = None
             if expected_error is None:
-                with self.maybeWarnsRegex(UserWarning, "stft with return_complex=False"):
-                    result = x.stft(n_fft, hop_length, win_length, window,
-                                    center=center, return_complex=False)
+                result = x.stft(n_fft, hop_length, win_length, window,
+                                center=center, return_complex=False)
                 # NB: librosa defaults to np.complex64 output, no matter what
                 # the input dtype
                 ref_result = librosa_stft(x, n_fft, hop_length, win_length, window, center)
@@ -748,7 +747,7 @@
         _test((10,), 5, 4, win_sizes=(1, 1), expected_error=RuntimeError)
 
 
-    @skipCUDAIfRocm
+    @onlyOnCPUAndCUDA
     @skipCPUIfNoMkl
     @dtypes(torch.double, torch.cdouble)
     def test_complex_stft_roundtrip(self, device, dtype):
@@ -790,7 +789,7 @@
                                       length=x.size(-1), **common_kwargs)
             self.assertEqual(x_roundtrip, x)
 
-    @skipCUDAIfRocm
+    @onlyOnCPUAndCUDA
     @skipCPUIfNoMkl
     @dtypes(torch.double, torch.cdouble)
     def test_stft_roundtrip_complex_window(self, device, dtype):
@@ -831,6 +830,7 @@
                 self.assertEqual(x_roundtrip, x)
 
 
+    @onlyOnCPUAndCUDA
     @skipCUDAIfRocm
     @skipCPUIfNoMkl
     @dtypes(torch.cdouble)
@@ -851,7 +851,7 @@
             actual = torch.stft(*args, window=window, center=False)
             self.assertEqual(actual, expected)
 
-    @skipCUDAIfRocm
+    @onlyOnCPUAndCUDA
     @skipCPUIfNoMkl
     @dtypes(torch.cdouble)
     def test_complex_stft_real_equiv(self, device, dtype):
@@ -885,6 +885,7 @@
                                 center=center, normalized=normalized)
             self.assertEqual(expected, actual)
 
+    @onlyOnCPUAndCUDA
     @skipCUDAIfRocm
     @skipCPUIfNoMkl
     @dtypes(torch.cdouble)
@@ -912,6 +913,7 @@
                                  return_complex=True)
             self.assertEqual(expected, actual)
 
+    @onlyOnCPUAndCUDA
     @skipCUDAIfRocm
     @skipCPUIfNoMkl
     def test_complex_stft_onesided(self, device):
@@ -934,12 +936,15 @@
             x.stft(10, pad_mode='constant', onesided=True)
 
     # stft is currently warning that it requires return-complex while an upgrader is written
+    @onlyOnCPUAndCUDA
+    @skipIf(not TEST_MKL, "Test requires MKL")
     def test_stft_requires_complex(self, device):
         x = torch.rand(100)
         y = x.stft(10, pad_mode='constant')
         # with self.assertRaisesRegex(RuntimeError, 'stft requires the return_complex parameter'):
         #     y = x.stft(10, pad_mode='constant')
 
+    @onlyOnCPUAndCUDA
     @skipCUDAIfRocm
     @skipCPUIfNoMkl
     def test_fft_input_modification(self, device):
