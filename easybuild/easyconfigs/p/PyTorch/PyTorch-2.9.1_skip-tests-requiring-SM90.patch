Avoid test_intra_node_comm_all_reduce failing on e.g. A100:

> [rank1]:E1022 09:55:08.823000 3580472 torch/testing/_internal/common_distributed.py:721] RuntimeError: CUDA error: device-side assert triggered...
> [rank1]:E1022 09:55:08.823000 3580472 torch/testing/_internal/common_distributed.py:721]  exiting process 1 with exit code: 10
> ...
> :318: st_vec: block: [0,0,0], thread: [87,0,0] Assertion `false` failed.
> /pytorch-v2.7.1/torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h:318: st_vec: block: [0,0,0], thread: [88,0,0] Assertion `false` failed.

test_fused_all_gather_scaled_matmul fails with a NCCL error due to FP8 usage and hangs forever.
See https://github.com/pytorch/pytorch/issues/171796

test_fused_scaled_matmul_reduce_scatter fails with
> RuntimeError: torch._scaled_mm is only supported on CUDA devices with compute capability >= 9.0 or 8.9, or ROCm MI300+


Author: Alexander Grund (TU Dresden)

diff --git a/test/distributed/test_c10d_nccl.py b/test/distributed/test_c10d_nccl.py
index 0a0f3ee4ca2..07702566fd8 100644
--- a/test/distributed/test_c10d_nccl.py
+++ b/test/distributed/test_c10d_nccl.py
@@ -3350,7 +3350,7 @@ class CommTest(test_c10d_common.AbstractCommTest, MultiProcessTestCase):
     @runOnRocmArch(MI300_ARCH)
     def test_intra_node_comm_all_reduce(self):
         from torch._C._distributed_c10d import _get_intra_node_comm_usage_counter
-        from torch.testing._internal.common_cuda import SM80OrLater
+        from torch.testing._internal.common_cuda import SM90OrLater
 
         for peer in range(self.world_size):
             if peer == self.rank:
@@ -3358,8 +3358,8 @@ class CommTest(test_c10d_common.AbstractCommTest, MultiProcessTestCase):
             if not torch._C._cuda_canDeviceAccessPeer(self.rank, peer):
                 raise SkipTest("Test requires p2p access")
 
-        if not SM80OrLater:
-            raise SkipTest("Test requires sm>=80")
+        if not SM90OrLater:
+            raise SkipTest("Test requires sm>=90")
 
         store = c10d.FileStore(self.file_name, self.world_size)
         os.environ["ENABLE_INTRA_NODE_COMM"] = "1"
diff --git a/test/distributed/test_symmetric_memory.py b/test/distributed/test_symmetric_memory.py
index eeeb24bec30..9d55b620840 100644
--- a/test/distributed/test_symmetric_memory.py
+++ b/test/distributed/test_symmetric_memory.py
@@ -4,7 +4,7 @@ import itertools
 import os
 import random
 from contextlib import nullcontext
-from unittest import skip, skipIf
+from unittest import skip, skipIf, skipUnless
 
 import torch
 import torch.distributed as dist
@@ -22,7 +22,7 @@ from torch.distributed._symmetric_memory import (
     restride_A_for_fused_matmul_reduce_scatter,
     restride_A_shard_for_fused_all_gather_matmul,
 )
-from torch.testing._internal.common_cuda import _get_torch_cuda_version, SM90OrLater
+from torch.testing._internal.common_cuda import _get_torch_cuda_version, SM90OrLater, IS_SM89
 from torch.testing._internal.common_device_type import e4m3_type
 from torch.testing._internal.common_distributed import (
     MultiProcContinuousTest,
@@ -399,6 +399,10 @@ class AsyncTPTest(MultiProcContinuousTest):
 
     @runOnRocmArch(MI300_ARCH)
     @skip_if_lt_x_gpu(2)
+    @skipIf(
+        not SM90OrLater,
+        "_fused_all_gather_scaled_matmul_fallback w/ FP8 only supports sm>=90",
+    )
     @parametrize("gather_dim", [0, 1])
     @parametrize(
         "scale_mode", ["tensor-wise", "row-wise-replicated", "row-wise-sharded"]
@@ -512,6 +516,10 @@ class AsyncTPTest(MultiProcContinuousTest):
 
     @skipIfRocm  # AsyncTP support changed _fused_scaled_matmul_reduce_scatter_fallback API, need more changes
     @skip_if_lt_x_gpu(2)
+    @skipUnless(
+        SM90OrLater or IS_SM89,
+        "torch._scaled_mm (from fused_scaled_matmul_reduce_scatter) only supports sm>=90 or 8.9",
+    )
     @parametrize("scatter_dim", [0, 1])
     @parametrize("rowwise", [True, False])
     def test_fused_scaled_matmul_reduce_scatter(
