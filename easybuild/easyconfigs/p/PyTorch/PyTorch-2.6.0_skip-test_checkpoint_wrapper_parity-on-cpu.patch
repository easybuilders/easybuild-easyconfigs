When no GPUs are available test_checkpoint_wrapper_parity fails with
> AttributeError: module 'torch.cpu' has no attribute 'reset_peak_memory_stats'

Author: Alexander Grund (TU Dresden)
diff --git a/test/distributed/fsdp/test_checkpoint_wrapper.py b/test/distributed/fsdp/test_checkpoint_wrapper.py
index 0f873b49297..afda0c13a6c 100644
--- a/test/distributed/fsdp/test_checkpoint_wrapper.py
+++ b/test/distributed/fsdp/test_checkpoint_wrapper.py
@@ -1,6 +1,7 @@
 # Owner(s): ["oncall: distributed"]
 
 import contextlib
+import unittest
 from copy import deepcopy
 from functools import partial
 
@@ -132,6 +133,7 @@ class CheckpointWrapperTest(TestCase):
         m(torch.randn(2, 1)).sum().backward()
         self.assertEqual(2, count)
 
+    @unittest.skipIf(device_type.type == "cpu", "CPU does not support max_memory_allocated")
     def test_checkpoint_wrapper_parity(self):
         """
         Tests that using checkpoint_wrapper or the functional
