Raise an error at the point where an implementation for the non-FBGEMM-using
build is missing (see the TODO). Without that the results are simply not calculated.
See https://github.com/pytorch/pytorch/issues/47748

Author: Alexander Grund (TU Dresden)

diff --git a/aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp b/aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp
index cb82d9aee4..c47d10dc05 100644
--- a/aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp
@@ -104,6 +104,9 @@ at::Tensor PackedEmbeddingBagWeight::embeddingbag_byte(
               "FBGEMM GenerateEmbeddingSpMDM kernel failed for 8-bit input");
         });
   }
+#else
+  TORCH_CHECK(
+      false, "This PyTorch installation was not built with FBGEMM operators");
 #endif
   // TODO add default (non-FBGEMM) implementation.
   return output;
diff --git a/test/quantization/test_quantize.py b/test/quantization/test_quantize.py
index e54eb33770..05598fe417 100644
--- a/test/quantization/test_quantize.py
+++ b/test/quantization/test_quantize.py
@@ -534,6 +534,7 @@ class TestPostTrainingStatic(QuantizationTestCase):
         self.checkQuantizedLinear(model.fc)
 
 
+    @skipIfNoFBGEMM
     def test_quantized_embedding_bag(self):
         r""" Test the post-training quantization flow, serialization and scripting
         of embedding_bag modules
diff --git a/test/quantization/test_quantized_module.py b/test/quantization/test_quantized_module.py
index fe0d52a935..098dd8ce22 100644
--- a/test/quantization/test_quantized_module.py
+++ b/test/quantization/test_quantized_module.py
@@ -712,6 +712,7 @@ class TestStaticQuantizedModule(QuantizationTestCase):
         embedding_dim=st.integers(5, 50).filter(lambda x: x % 4 == 0),
         set_qconfig=st.booleans(),
     )
+    @skipIfNoFBGEMM
     def test_embedding_api(self, num_embeddings, embedding_dim, set_qconfig):
         num_lengths = np.random.randint(1, 6)
         lengths = np.random.randint(0, 21, size=num_lengths).astype(np.int32)
diff --git a/test/quantization/test_quantized_op.py b/test/quantization/test_quantized_op.py
index 7efc863cfa..51704e3d1d 100644
--- a/test/quantization/test_quantized_op.py
+++ b/test/quantization/test_quantized_op.py
@@ -3015,6 +3015,7 @@ class TestQuantizedEmbeddingOps(TestCase):
     """ Tests the correctness of the quantized embedding lookup operator """
     @given(num_embeddings=st.integers(10, 100),
            embedding_dim=st.integers(5, 50).filter(lambda x: x % 4 == 0))
+    @skipIfNoFBGEMM
     def test_embedding_byte(self, num_embeddings, embedding_dim):
         quant_op = torch.ops.quantized.embedding_byte
         prepack_op = torch.ops.quantized.embedding_bag_prepack
