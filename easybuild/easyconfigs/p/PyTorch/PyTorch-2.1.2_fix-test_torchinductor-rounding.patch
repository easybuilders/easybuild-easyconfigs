test_round_correctness_dynamic_shapes_cpu in test_torchinductor_dynamic_shapes.py
and test_comprehensive_round_cpu_float16 in test_torchinductor_opinfo
may fail, e.g. on PPC, due to unsafe math optimizations changing the rounding.
In particular it does not round halfway cases to even leading to a diff of 1.0:

Mismatched elements: 6 / 200 (3.0%)
Greatest absolute difference: 1.0 at index (15,) (up to 1e-07 allowed)
Greatest relative difference: 0.5 at index (75,) (up to 1e-07 allowed)

See https://github.com/pytorch/pytorch/issues/117346
Fix by disabling -funsafe-math-optimizations for these tests.

Author: Alexander Grund (TU Dresden)

diff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py
index e6f1ea5a056..d6922da0289 100644
--- a/test/inductor/test_torchinductor.py
+++ b/test/inductor/test_torchinductor.py
@@ -81,7 +81,7 @@ from torch._inductor import config, test_operators
 from torch._inductor.compile_fx import compile_fx, compile_fx_inner
 from torch._inductor.utils import has_torchvision_roi_align
 
-from torch.testing._internal.common_utils import slowTest
+from torch.testing._internal.common_utils import slowTest, disable_unsafe_math_optimizations
 from torch.testing._internal.inductor_utils import HAS_CPU, HAS_CUDA
 
 HAS_MULTIGPU = HAS_CUDA and torch.cuda.device_count() >= 2
@@ -1287,6 +1287,7 @@ class CommonTemplate:
         # with *100 we are always getting a number exactly at .5 which we don't do right in half
         self.common(fn, (torch.randn(8, 8) * 100, torch.randn(8, 8) * 10))
 
+    @disable_unsafe_math_optimizations
     def test_round_correctness(self):
         if self.device == "cuda":
             raise unittest.SkipTest("need to debug tl.libdevice on A100/V100")
diff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py
index 5f6fa9e3eda..b25d5663b4e 100644
--- a/torch/testing/_internal/common_methods_invocations.py
+++ b/torch/testing/_internal/common_methods_invocations.py
@@ -16,7 +16,7 @@ from torch import inf, nan
 from typing import Any, Dict, List, Tuple, Union, Sequence
 from torch.testing import make_tensor
 from torch.testing._internal.common_dtype import (
-    _dispatch_dtypes, floating_types, floating_types_and, complex_types, floating_and_complex_types,
+    _dispatch_dtypes, floating_types, floating_types_and, floating_types_and_half, complex_types, floating_and_complex_types,
     floating_and_complex_types_and, all_types_and_complex_and, all_types_and, all_types_and_complex, integral_types_and,
     all_types, empty_types, complex_types_and, integral_types
 )
@@ -30,6 +30,7 @@ from torch.testing._internal.common_cuda import (
     _get_torch_cuda_version, _get_torch_rocm_version,
 )
 from torch.testing._internal.common_utils import (
+    disable_unsafe_math_optimizations,
     make_fullrank_matrices_with_distinct_singular_values,
     TEST_WITH_ROCM, IS_WINDOWS, IS_MACOS, TEST_SCIPY,
     torch_to_numpy_dtype_dict, TEST_WITH_ASAN,
@@ -14353,6 +14354,11 @@ op_db: List[OpInfo] = [
                                     'TestNNCOpInfo',
                                     'test_nnc_correctness',
                                     dtypes=(torch.bfloat16,)),
+                       DecorateInfo(disable_unsafe_math_optimizations,
+                                    'TestInductorOpInfo',
+                                    'test_comprehensive',
+                                    dtypes=floating_types_and_half(),
+                                    device_type='cpu'),
                    ),
                    supports_sparse=True,
                    supports_sparse_csr=True,
diff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py
index 1e18ca2afec..fe183bbc736 100644
--- a/torch/testing/_internal/common_utils.py
+++ b/torch/testing/_internal/common_utils.py
@@ -999,6 +999,14 @@ def is_avx512_vnni_supported():
 
 IS_AVX512_VNNI_SUPPORTED = is_avx512_vnni_supported()
 
+def disable_unsafe_math_optimizations(method):
+    @wraps(method)
+    def wrapper(*args, **kwargs):
+        flags = torch._inductor.codecache.optimization_flags() + ' -fno-unsafe-math-optimizations'
+        with unittest.mock.patch("torch._inductor.codecache.optimization_flags", return_value=flags):
+            method(*args, **kwargs)
+    return wrapper
+
 if IS_WINDOWS:
     @contextmanager
     def TemporaryFileName(*args, **kwargs):
