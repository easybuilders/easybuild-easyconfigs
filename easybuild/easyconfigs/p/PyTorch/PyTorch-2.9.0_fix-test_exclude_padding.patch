PadMMTest.test_exclude_padding fails on H100 with
>     self.assertTrue(len(local_cache) == 2)
> AssertionError: False is not true

Increasing the size triggers the intended code.
See https://github.com/pytorch/pytorch/pull/169177

Author: Alexander Grund (TU Dresden)

diff --git a/test/inductor/test_pad_mm.py b/test/inductor/test_pad_mm.py
--- a/test/inductor/test_pad_mm.py
+++ b/test/inductor/test_pad_mm.py
@@ -425,7 +426,10 @@ class PadMMTest(TestCase):
         def mm(a, b):
             return a @ b
 
-        mm(torch.rand([25, 25], device="cuda"), torch.rand([25, 25], device="cuda"))
+        # Size must be big enough such that `is_mm_compute_bound` returns True and we need padding to 4 elements
+        # machine balance is ~8.3 (A100), 14.1 (H100), size must be 3x that, see arithmetic_intensity for M=N=K
+        size = [59, 59]
+        mm(torch.rand(size, device="cuda"), torch.rand(size, device="cuda"))
         local_cache = get_pad_cache().get_local_cache()
         self.assertTrue(len(local_cache) == 2)
         FileCheck().check_count("exclude_pad:False", 2, exactly=True).run(
@@ -436,7 +440,7 @@ class PadMMTest(TestCase):
         def mm(a, b):
             return (a + 1) @ b
 
-        mm(torch.rand([25, 25], device="cuda"), torch.rand([25, 25], device="cuda"))
+        mm(torch.rand(size, device="cuda"), torch.rand(size, device="cuda"))
         local_cache = get_pad_cache().get_local_cache()
         # reuse original base timing
         self.assertTrue(len(local_cache) == 3)
