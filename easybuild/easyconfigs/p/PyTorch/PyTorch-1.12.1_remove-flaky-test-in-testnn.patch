This test is flaky when run with the type float16.
I.e. TestNNDeviceTypeCPU.test_embedding_bag_device_cpu_int32_int32_float16 & 
     TestNNDeviceTypeCPU.test_embedding_bag_device_cpu_int64_int64_float16
fail.
See https://github.com/pytorch/pytorch/issues/86638
So remove the half precision test.

Author: Alexander Grund (TU Dresden)

diff --git a/test/test_nn.py b/test/test_nn.py
index aad884ebd4f..b514f72a3e9 100644
--- a/test/test_nn.py
+++ b/test/test_nn.py
@@ -18236,7 +18236,7 @@ class TestNNDeviceType(NNTestCase):
             self.assertRaises(RuntimeError, lambda: es(input.view(-1), offset))
 
     @skipMeta
-    @dtypes(*itertools.product((torch.int, torch.long), (torch.int, torch.long), (torch.float, torch.double, torch.half)))
+    @dtypes(*itertools.product((torch.int, torch.long), (torch.int, torch.long), (torch.float, torch.double)))
     def test_embedding_bag_device(self, device, dtypes):
         self._test_EmbeddingBag(device, 'sum', False, wdtype=dtypes[2], dtype=dtypes[0], odtype=dtypes[1])
         self._test_EmbeddingBag(device, 'mean', False, wdtype=dtypes[2], dtype=dtypes[0], odtype=dtypes[1])
