# Author: Caspar van Leeuwen, SURF
# Fixes failing tests in test_zero_redundancy_optimizer for A100 due to the use of TensorFloat32
# We increase tolerances in the assert's, so that the small differences that arise due to the 
# use of TensorFloat32 fall within the limits of torch.allclose(...)
# See https://github.com/pytorch/pytorch/issues/67764
diff -Nru pytorch.orig/test/distributed/optim/test_zero_redundancy_optimizer.py pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py
--- pytorch.orig/test/distributed/optim/test_zero_redundancy_optimizer.py	2021-11-02 15:50:15.430729833 +0100
+++ pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py	2021-11-03 14:38:23.244771749 +0100
@@ -881,13 +881,15 @@
                 local_loss = cast(torch.Tensor, local_optim.step(closure=closure_local))
                 ddp_loss = cast(torch.Tensor, zero_optim.step(closure=closure_ddp)).to(cpu_device)
 
+                # Increased tolerances are needed to pass test when using TensorFloat32
+                # see https://github.com/pytorch/pytorch/issues/67764
                 assert torch.allclose(
-                    local_loss, ddp_loss
+                    local_loss, ddp_loss, rtol=1e-03
                 ), "Losses differ between local optim and ZeroRedundancyOptimizer"
 
                 for local_p, ddp_p in zip(local_model.parameters(), ddp_model.parameters()):
                     ddp_p = ddp_p.to(cpu_device)
-                    assert torch.allclose(local_p, ddp_p), "Models differ after a step"
+                    assert torch.allclose(local_p, ddp_p, rtol=1e-04, atol=1e-04), "Models differ after a step"
 
     @common_distributed.skip_if_lt_x_gpu(4)
     def test_zero_model_parallel_with_bucket_view(self):
