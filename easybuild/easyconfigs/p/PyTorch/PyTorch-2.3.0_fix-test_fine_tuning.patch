Fixes
> TypeError: get_state_dict() missing 1 required positional argument: 'optimizers'

From 61d30b6e8acbd3cfb087761defa74f19f9be96bb Mon Sep 17 00:00:00 2001
From: cdzhan <zhancdi@163.com>
Date: Mon, 24 Jun 2024 20:02:08 +0800
Subject: [PATCH] [easy][DCP] Fix test_fine_tuning.py for get/set_state_dict
 API changes

---
 test/distributed/checkpoint/e2e/test_fine_tuning.py | 6 ++++--
 1 file changed, 4 insertions(+), 2 deletions(-)

diff --git a/test/distributed/checkpoint/e2e/test_fine_tuning.py b/test/distributed/checkpoint/e2e/test_fine_tuning.py
index a93f242187709c..fd21524882c839 100644
--- a/test/distributed/checkpoint/e2e/test_fine_tuning.py
+++ b/test/distributed/checkpoint/e2e/test_fine_tuning.py
@@ -9,7 +9,9 @@
 import torch.nn as nn
 from torch.distributed._tensor import init_device_mesh
 from torch.distributed.checkpoint.state_dict import (
+    get_model_state_dict,
     get_state_dict,
+    set_model_state_dict,
     set_state_dict,
     StateDictOptions,
 )
@@ -120,7 +122,7 @@ def finetune(self, pretrain_dir: str, finetune_dir: str) -> None:
         # Simulate that the fine tuning restart after 3 iterations
         for i in range(2):
             # Load pretrain submodules checkpoint
-            pretrain_state_dict, _ = get_state_dict(
+            pretrain_state_dict = get_model_state_dict(
                 model,
                 submodules={model.pretrain},
                 options=StateDictOptions(keep_submodule_prefixes=False),
@@ -129,7 +131,7 @@ def finetune(self, pretrain_dir: str, finetune_dir: str) -> None:
                 {"model": pretrain_state_dict},
                 storage_reader=dist_cp.FileSystemReader(pretrain_dir),
             )
-            set_state_dict(
+            set_model_state_dict(
                 model,
                 model_state_dict={model.pretrain: pretrain_state_dict},
                 options=StateDictOptions(strict=False),
