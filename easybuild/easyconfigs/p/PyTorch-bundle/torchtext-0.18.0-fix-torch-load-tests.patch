Fix UnpicklingError in tests due to change in behaviour of 'weights_only" in torch.load in PyTorch 2.6.0+
See https://pytorch.org/docs/stable/generated/torch.load.html
author: Alex Domingo (Vrije Universiteit Brussel)
diff -Nru text-0.18.0.orig/test/torchtext_unittest/prototype/test_functional.py text-0.18.0/test/torchtext_unittest/prototype/test_functional.py
--- text-0.18.0.orig/test/torchtext_unittest/prototype/test_functional.py	2025-11-24 23:24:44.864334802 +0100
+++ text-0.18.0/test/torchtext_unittest/prototype/test_functional.py	2025-11-24 23:31:17.061812433 +0100
@@ -84,7 +84,7 @@
             save_path = os.path.join(self.test_dir, "ben_pybind.pt")
             ben = basic_english_normalize()
             torch.save(ben, save_path)
-            loaded_ben = torch.load(save_path)
+            loaded_ben = torch.load(save_path, weights_only=False)
             self.assertEqual(loaded_ben(test_sample), ref_results)
 
         with self.subTest("torchscript"):
@@ -93,5 +93,5 @@
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             ben = basic_english_normalize().__prepare_scriptable__()
             torch.save(ben, save_path)
-            loaded_ben = torch.load(save_path)
+            loaded_ben = torch.load(save_path, weights_only=False)
             self.assertEqual(loaded_ben(test_sample), ref_results)
diff -Nru text-0.18.0.orig/test/torchtext_unittest/prototype/test_transforms.py text-0.18.0/test/torchtext_unittest/prototype/test_transforms.py
--- text-0.18.0.orig/test/torchtext_unittest/prototype/test_transforms.py	2025-11-24 23:24:44.864687000 +0100
+++ text-0.18.0/test/torchtext_unittest/prototype/test_transforms.py	2025-11-24 23:31:03.315333000 +0100
@@ -125,7 +125,7 @@
             save_path = os.path.join(self.test_dir, "spm_pybind.pt")
             spm = sentencepiece_tokenizer((model_path))
             torch.save(spm, save_path)
-            loaded_spm = torch.load(save_path)
+            loaded_spm = torch.load(save_path, weights_only=False)
             self.assertEqual(expected, loaded_spm(input))
 
         with self.subTest("torchscript"):
@@ -134,5 +134,5 @@
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             spm = sentencepiece_tokenizer((model_path)).__prepare_scriptable__()
             torch.save(spm, save_path)
-            loaded_spm = torch.load(save_path)
+            loaded_spm = torch.load(save_path, weights_only=False)
             self.assertEqual(expected, loaded_spm(input))
diff -Nru text-0.18.0.orig/test/torchtext_unittest/prototype/test_vectors.py text-0.18.0/test/torchtext_unittest/prototype/test_vectors.py
--- text-0.18.0.orig/test/torchtext_unittest/prototype/test_vectors.py	2025-11-24 23:24:44.865096000 +0100
+++ text-0.18.0/test/torchtext_unittest/prototype/test_vectors.py	2025-11-24 23:30:42.688612000 +0100
@@ -141,7 +141,7 @@
         with self.subTest("pybind"):
             vector_path = os.path.join(self.test_dir, "vectors_pybind.pt")
             torch.save(vectors_obj, vector_path)
-            loaded_vectors_obj = torch.load(vector_path)
+            loaded_vectors_obj = torch.load(vector_path, weights_only=False)
 
             self.assertEqual(loaded_vectors_obj["a"], tensorA)
             self.assertEqual(loaded_vectors_obj["b"], tensorB)
@@ -152,7 +152,7 @@
             # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             torch.save(vectors_obj.__prepare_scriptable__(), vector_path)
-            loaded_vectors_obj = torch.load(vector_path)
+            loaded_vectors_obj = torch.load(vector_path, weights_only=False)
 
             self.assertEqual(loaded_vectors_obj["a"], tensorA)
             self.assertEqual(loaded_vectors_obj["b"], tensorB)
diff -Nru text-0.18.0.orig/test/torchtext_unittest/test_transforms.py text-0.18.0/test/torchtext_unittest/test_transforms.py
--- text-0.18.0.orig/test/torchtext_unittest/test_transforms.py	2025-11-24 23:24:44.923291000 +0100
+++ text-0.18.0/test/torchtext_unittest/test_transforms.py	2025-11-24 23:31:27.419319212 +0100
@@ -732,7 +732,7 @@
         tokenizer = self._load_tokenizer(test_scripting=False, return_tokens=False)
         tokenizer_path = os.path.join(self.test_dir, "gpt2_tokenizer_pybind.pt")
         torch.save(tokenizer, tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._gpt2_bpe_tokenizer((loaded_tokenizer))
 
     def test_gpt2_bpe_tokenizer_save_load_torchscript(self) -> None:
@@ -741,7 +741,7 @@
         # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
         # Not expect users to use the torchbind version on eager mode but still need a CI test here.
         torch.save(tokenizer.__prepare_scriptable__(), tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._gpt2_bpe_tokenizer((loaded_tokenizer))
 
 
@@ -994,7 +994,7 @@
         tokenizer = self._load_tokenizer(init_using_merge_only=True, test_scripting=False, return_tokens=False)
         tokenizer_path = os.path.join(self.test_dir, "gpt2_tokenizer_pybind.pt")
         torch.save(tokenizer, tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._clip_tokenizer((loaded_tokenizer))
 
     def test_clip_tokenizer_save_load_torchscript(self) -> None:
@@ -1003,7 +1003,7 @@
         # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
         # Not expect users to use the torchbind version on eager mode but still need a CI test here.
         torch.save(tokenizer.__prepare_scriptable__(), tokenizer_path)
-        loaded_tokenizer = torch.load(tokenizer_path)
+        loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
         self._clip_tokenizer((loaded_tokenizer))
 
 
@@ -1172,7 +1172,7 @@
             self._bert_tokenizer((loaded_tokenizer), do_lower_case=do_lower_case)
         else:
             torch.save(tokenizer, tokenizer_path)
-            loaded_tokenizer = torch.load(tokenizer_path)
+            loaded_tokenizer = torch.load(tokenizer_path, weights_only=False)
             self._bert_tokenizer((loaded_tokenizer), do_lower_case=do_lower_case)
 
 
@@ -1239,7 +1239,7 @@
 
             tokenizer = RegexTokenizer(self.patterns_list)
             torch.save(tokenizer, save_path)
-            loaded_tokenizer = torch.load(save_path)
+            loaded_tokenizer = torch.load(save_path, weights_only=False)
             results = loaded_tokenizer(self.test_sample)
             self.assertEqual(results, self.ref_results)
 
diff -Nru text-0.18.0.orig/test/torchtext_unittest/test_vocab.py text-0.18.0/test/torchtext_unittest/test_vocab.py
--- text-0.18.0.orig/test/torchtext_unittest/test_vocab.py	2025-11-24 23:24:44.924272000 +0100
+++ text-0.18.0/test/torchtext_unittest/test_vocab.py	2025-11-24 23:30:05.779945673 +0100
@@ -206,7 +206,7 @@
         with self.subTest("pybind"):
             vocab_path = os.path.join(self.test_dir, "vocab_pybind.pt")
             torch.save(v, vocab_path)
-            loaded_v = torch.load(vocab_path)
+            loaded_v = torch.load(vocab_path, weights_only=False)
             self.assertEqual(v.get_itos(), expected_itos)
             self.assertEqual(dict(loaded_v.get_stoi()), expected_stoi)
             self.assertEqual(v["not in vocab"], 0)
@@ -216,7 +216,7 @@
             # Call the __prepare_scriptable__() func and convert the building block to the torbhind version
             # Not expect users to use the torchbind version on eager mode but still need a CI test here.
             torch.save(v.__prepare_scriptable__(), vocab_path)
-            loaded_v = torch.load(vocab_path)
+            loaded_v = torch.load(vocab_path, weights_only=False)
             self.assertEqual(v.get_itos(), expected_itos)
             self.assertEqual(dict(loaded_v.get_stoi()), expected_stoi)
             self.assertEqual(v["not in vocab"], 0)
