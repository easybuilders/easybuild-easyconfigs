Fix:
.../PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "

From: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1412#issuecomment-1109419010
diff -ur pytorch-CycleGAN-and-pix2pix-9f8f61e.orig/models/base_model.py pytorch-CycleGAN-and-pix2pix-9f8f61e/models/base_model.py
--- pytorch-CycleGAN-and-pix2pix-9f8f61e.orig/models/base_model.py	2023-03-14 21:28:49.000000000 +0100
+++ pytorch-CycleGAN-and-pix2pix-9f8f61e/models/base_model.py	2023-06-13 21:50:16.637178772 +0200
@@ -114,16 +114,7 @@
         return self.image_paths
 
     def update_learning_rate(self):
-        """Update learning rates for all the networks; called at the end of every epoch"""
-        old_lr = self.optimizers[0].param_groups[0]['lr']
-        for scheduler in self.schedulers:
-            if self.opt.lr_policy == 'plateau':
-                scheduler.step(self.metric)
-            else:
-                scheduler.step()
-
-        lr = self.optimizers[0].param_groups[0]['lr']
-        print('learning rate %.7f -> %.7f' % (old_lr, lr))
+        pass
 
     def get_current_visuals(self):
         """Return visualization images. train.py will display these images with visdom, and save the images to a HTML"""
diff -ur pytorch-CycleGAN-and-pix2pix-9f8f61e.orig/models/pix2pix_model.py pytorch-CycleGAN-and-pix2pix-9f8f61e/models/pix2pix_model.py
--- pytorch-CycleGAN-and-pix2pix-9f8f61e.orig/models/pix2pix_model.py	2023-03-14 21:28:49.000000000 +0100
+++ pytorch-CycleGAN-and-pix2pix-9f8f61e/models/pix2pix_model.py	2023-06-13 22:02:28.673948000 +0200
@@ -64,6 +64,7 @@
             # define loss functions
             self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)
             self.criterionL1 = torch.nn.L1Loss()
+            self.old_lr = opt.lr
             # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.
             self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
             self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
@@ -125,3 +126,13 @@
         self.optimizer_G.zero_grad()        # set G's gradients to zero
         self.backward_G()                   # calculate graidents for G
         self.optimizer_G.step()             # update G's weights
+
+    def update_learning_rate(self):
+        lrd = self.opt.lr / self.opt.n_epochs_decay
+        lr = self.old_lr - lrd
+        for param_group in self.optimizer_D.param_groups:
+            param_group['lr'] = lr
+        for param_group in self.optimizer_G.param_groups:
+            param_group['lr'] = lr
+        print('update learning rate: %f -> %f' % (self.old_lr, lr))
+        self.old_lr = lr
diff -ur pytorch-CycleGAN-and-pix2pix-9f8f61e.orig/train.py pytorch-CycleGAN-and-pix2pix-9f8f61e/train.py
--- pytorch-CycleGAN-and-pix2pix-9f8f61e.orig/train.py	2023-03-14 21:28:49.000000000 +0100
+++ pytorch-CycleGAN-and-pix2pix-9f8f61e/train.py	2023-06-13 22:03:55.965877000 +0200
@@ -75,3 +75,5 @@
             model.save_networks(epoch)
 
         print('End of epoch %d / %d \t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))
+        if epoch > opt.n_epochs:
+            model.update_learning_rate()  # update learning rates at the end of every epoch.
