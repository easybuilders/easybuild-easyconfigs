easyblock = 'PythonBundle'

name = 'ONNX-Runtime'
version = '1.20.1'

homepage = 'https://onnxruntime.ai'
description = """ONNX Runtime inference can enable faster customer experiences and lower costs,
supporting models from deep learning frameworks such as PyTorch and
TensorFlow/Keras as well as classical machine learning libraries such as
scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different
hardware, drivers, and operating systems, and provides optimal performance by
leveraging hardware accelerators where applicable alongside graph optimizations
and transforms."""

toolchain = {'name': 'intel', 'version': '2023b'}

dependencies = [
    ('Python', '3.11.5'),
    ('ONNX', '1.17.0'),
    ('flatbuffers-python', '24.3.25'),
    ('sympy', '1.13.3'),
]

local_whl_tmpl = 'onnxruntime-%%(version)s-cp311-cp311-manylinux_2_27_%s.manylinux_2_28_%s.whl'

exts_list = [
    ('humanfriendly', '10.0', {
        'checksums': ['6b0b831ce8f15f7300721aa49829fc4e83921a9a301cc7f606be6686a2288ddc'],
    }),
    ('coloredlogs', '15.0.1', {
        'checksums': ['7c991aa71a4577af2f82600d8f8f3a89f936baeaf9b50a9c197da014e5bf16b0'],
    }),
    (name, version, {
        'modulename': 'onnxruntime',
        'source_urls': ['http://pypi.python.org/packages/source/o/onnxruntime'],
        'sources': ['onnxruntime-%(version)s-cp311-cp311-manylinux_2_27_%(arch)s.manylinux_2_28_%(arch)s.whl'],
        'checksums': ['5eec64c0269dcdb8d9a9a53dc4d64f87b9e0c19801d9321246a53b7eb5a7d1bc'],
    }),
]

moduleclass = 'devel'
