# Updated to version 6.4.3
# This version is without HDF5 support and Wannier-90. 
# Consequently, of the all test suite the testjobs requiring either will fail.
# All others passed on the L40s which were used.
# Author: J. Sassmannshausen (Imperial College London, UK)

easyblock = 'MakeCp'

name = 'VASP'
version = '6.4.3'

homepage = 'https://www.vasp.at'
description = """The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale
materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics,
from first principles."""

toolchain = {'name': 'nvofbf', 'version': '2024.9'}
toolchainopts = {'openmp': True}

download_instructions = """Vasp is proprietary software, see http://www.vasp.at/index.php/faqs
 on how to get access to the code."""

sources = ['%(namelower)s.%(version)s.tgz']
patches = [
    # This patch replaces -Mscalapack with -lscalapack and adds the NVROOT path via ${EBROOTNVHPC}
    # It also sets the GPU version (hardcoded, needs to be changed!)
    # This is a bit WIP, see below
    '%(namelower)s-%(version)s_nv-flags.patch',
]
checksums = [
    {'vasp.6.4.3.tgz': 'fe30e773f2a3e909b5e0baa9654032dfbdeff7ec157bc348cee7681a7b6c24f4'},
    {'vasp-6.4.3_nv-flags.patch': '0b21ed2e7fbafa03859cc0edca7d4673c08fe64ab930adb62c57ba2cd36123cd'},
]

prebuildopts = 'cp arch/makefile.include.nvhpc_omp_acc ./makefile.include && '

# Makefile uses LIBS as a list of folders
prebuildopts += 'unset LIBS && '
# We would need something to automatically get the CUDA version and cc set, i.e. to
# have -gpu=cc89,cuda12.6 in the patch file automatically populated
# prebuildopts += 'export CUDA=${EBVERSIONCUDA} ; export CUDACOMPILER='$CUDACOMPILER} &&'

local_targets = ['std', 'gam', 'ncl']

buildopts = '%s ' % ' '.join(local_targets)
buildopts += 'FFTW_ROOT="${EBROOTFFTW}" '
buildopts += 'DEPS=1 '  # required for VASP parallel builds

# https://www.vasp.at/wiki/index.php/Validation_tests

# it is recommended to run the testsuite with 4 MPI processes,
# but it's also recommended to use only 1 MPI process per GPU,
# so running the test on GPUs requires 4 GPUs?
local_test_mpiprocs = 4

pretestopts = 'export MPIRUN="mpirun -np %s -x OMP_NUM_THREADS=4 -x OMP_STACKSIZE=512m" && ' % local_test_mpiprocs
pretestopts += 'export LD_LIBRARY_PATH="${EBROOTNVHPC}/Linux_x86_64/${EBVERSIONNVHPC}/compilers/extras/qd/lib:$LD_LIBRARY_PATH" '
pretestopts += 'export VASP_TESTSUITE_EXE_STD="$MPIRUN %(builddir)s/vasp.%(version)s/bin/vasp_std" && '
pretestopts += 'export VASP_TESTSUITE_EXE_NCL="$MPIRUN %(builddir)s/vasp.%(version)s/bin/vasp_ncl" && '
pretestopts += 'export VASP_TESTSUITE_EXE_GAM="$MPIRUN %(builddir)s/vasp.%(version)s/bin/vasp_gam" && '

# skip tests that are too long
local_test_skip = 'HEG_333_LW '

# these tests are failing when using 4 L40S GPUs as the funtionality is not installed (HDF5, Wannier90, LibXC)
# when running *all* tests which takes over 1 day
# bulk_BN_PBExSCANc_Libxc bulk_BN_PBE_Libxc bulk_BN_SCAN_Libxc Co_2E4W_T Co_CRPAR_proj Co_CRPA_proj Co_CRPA_proj_RPR 
# Co_CRPA_weighted Co_CRPA_weighted_RPR FeAl_CRPAR_T mlwf_lif_scdm mlwf_lif_scdm_optics mlwf_mos2_soc_locproj 
# mlwf_mos2_soc_wannier90 mlwf_mos2_wannier90 mlwf_si_scdm NiO_2E4W SrVO3_CRPA_band_removal 
# SrVO3_CRPA_band_removal_RPR SrVO3_CRPA_proj SrVO3_CRPA_proj_RPR SrVO3_CRPA_weighted SrVO3_CRPA_weighted_RPR 
# SrVO3_NLRPA SrVO3_NLRPA_RPR

pretestopts += 'export VASP_TESTSUITE_SKIP_TESTS="%s" && ' % local_test_skip
runtest = 'test'
# Full test suite which takes over 1 day on L40s
# runtest = 'test_all'

files_to_copy = [(['bin/vasp_' + x for x in local_targets], 'bin')]

sanity_check_paths = {
    'files': ['bin/vasp_' + x for x in local_targets],
    'dirs': []
}

# This is needed as else libqdmod and libqd might not be found 
# This can be removed if and when NVHPC has been updated to include this path
modextrapaths = {'LD_LIBRARY_PATH': ':${EBROOTNVHPC}/Linux_x86_64/${EBVERSIONNVHPC}/compilers/extras/qd/lib'}

moduleclass = 'phys'
