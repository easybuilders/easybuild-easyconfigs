From ddbf7ab23ce2e83747ff6a1482ac512e06da82ca Mon Sep 17 00:00:00 2001
From: Viktor Rehnberg <viktor.rehnberg@gmail.com>
Date: Mon, 4 Nov 2024 15:31:55 +0100
Subject: [PATCH] Fix quantization tests

NVME tests didn't always run because the hard-coded nvme_path wasn't
always writable. This commit changed to use tmp_path fixture instead and
disabled distributed test to avoid thread locks hanging.
---
 .../quantization/test_intX_quantization.py    | 43 ++++++++++---------
 1 file changed, 22 insertions(+), 21 deletions(-)

diff --git a/tests/unit/inference/quantization/test_intX_quantization.py b/tests/unit/inference/quantization/test_intX_quantization.py
index 77b51fcd..9e0d7ac0 100644
--- a/tests/unit/inference/quantization/test_intX_quantization.py
+++ b/tests/unit/inference/quantization/test_intX_quantization.py
@@ -17,6 +17,7 @@ from transformers import AutoConfig, OPTConfig, AutoModel
 import pytest
 from collections import OrderedDict
 from typing import Dict
+from pathlib import Path
 
 device = get_accelerator().device_name() if get_accelerator().is_available() else 'cpu'
 
@@ -53,11 +54,11 @@ def quantization_test_helper(pre_quant_type: torch.dtype, num_bits: int):
     assert mean_diff < 0.15 and max_diff < 0.5, f'Numeric error exceed threshold, mean diff {mean_diff} (threshold 0.15), max diff {max_diff} (threshold 0.5)'
 
 
-def zero3_post_init_quantization_test_helper(cpu_offload: bool, nvme_offload: bool, bits: int):
+def zero3_post_init_quantization_test_helper(cpu_offload: bool, nvme_offload: bool, bits: int, tmp_path: Path):
     import deepspeed
     from transformers.integrations.deepspeed import HfDeepSpeedConfig
 
-    def get_zero3_ds_config(hf_config: OPTConfig, cpu_offload: bool, nvme_offload: bool, bits: int) -> Dict:
+    def get_zero3_ds_config(hf_config: OPTConfig, cpu_offload: bool, nvme_offload: bool, bits: int, tmp_path: Path) -> Dict:
         GB = 1 << 30
 
         ds_config = {
@@ -127,7 +128,7 @@ def zero3_post_init_quantization_test_helper(cpu_offload: bool, nvme_offload: bo
             ds_config["zero_optimization"]["offload_param"] = dict(
                 device="nvme",
                 pin_memory=True,
-                nvme_path='~/tmp_offload_dir',
+                nvme_path=str(tmp_path / "tmp_offload_dir"),
                 buffer_count=5,
                 buffer_size=1 * GB,
             )
@@ -142,7 +143,7 @@ def zero3_post_init_quantization_test_helper(cpu_offload: bool, nvme_offload: bo
         return ds_config
 
     hf_config = AutoConfig.from_pretrained('facebook/opt-125m')
-    ds_config = get_zero3_ds_config(hf_config=hf_config, cpu_offload=cpu_offload, nvme_offload=nvme_offload, bits=bits)
+    ds_config = get_zero3_ds_config(hf_config=hf_config, cpu_offload=cpu_offload, nvme_offload=nvme_offload, bits=bits, tmp_path=tmp_path)
 
     input_ids = torch.ones(1, 16, dtype=torch.int32, device=device)
     attention_mask = torch.ones(1, 16, dtype=torch.float32, device=device)
@@ -170,11 +171,11 @@ def zero3_post_init_quantization_test_helper(cpu_offload: bool, nvme_offload: bo
     assert mean_diff < 0.4, f'Numeric error exceed threshold, relative error {mean_diff} (threshold 0.4)'
 
 
-def zero3_quantized_initialization_test_helper(cpu_offload: bool, nvme_offload: bool, bits: int):
+def zero3_quantized_initialization_test_helper(cpu_offload: bool, nvme_offload: bool, bits: int, tmp_path: Path):
     import deepspeed
     from transformers.integrations.deepspeed import HfDeepSpeedConfig
 
-    def get_zero3_ds_config(hf_config: OPTConfig, cpu_offload: bool, nvme_offload: bool, bits: int) -> Dict:
+    def get_zero3_ds_config(hf_config: OPTConfig, cpu_offload: bool, nvme_offload: bool, bits: int, tmp_path: Path) -> Dict:
         GB = 1 << 30
 
         ds_config = {
@@ -206,7 +207,7 @@ def zero3_quantized_initialization_test_helper(cpu_offload: bool, nvme_offload:
             ds_config["zero_optimization"]["offload_param"] = dict(
                 device="nvme",
                 pin_memory=True,
-                nvme_path='~/tmp_offload_dir',
+                nvme_path=str(tmp_path / "tmp_offload_dir"),
                 buffer_count=5,
                 buffer_size=1 * GB,
             )
@@ -221,7 +222,7 @@ def zero3_quantized_initialization_test_helper(cpu_offload: bool, nvme_offload:
         return ds_config
 
     hf_config = AutoConfig.from_pretrained('facebook/opt-125m')
-    ds_config = get_zero3_ds_config(hf_config=hf_config, cpu_offload=cpu_offload, nvme_offload=nvme_offload, bits=bits)
+    ds_config = get_zero3_ds_config(hf_config=hf_config, cpu_offload=cpu_offload, nvme_offload=nvme_offload, bits=bits, tmp_path=tmp_path)
 
     input_ids = torch.ones(1, 16, dtype=torch.int32, device=device)
     attention_mask = torch.ones(1, 16, dtype=torch.float32, device=device)
@@ -376,31 +377,31 @@ class TestQuantizedInt(DistributedTest):
         quantization_test_helper(torch.float16, 8)
 
     @pytest.mark.skipif(device == 'cpu', reason='CPU does support FP16 GEMM')
-    def test_zero3_int4_post_init_quant(self, quantization_bits):
+    def test_zero3_int4_post_init_quant(self, quantization_bits, tmp_path):
         reset_random()
-        zero3_post_init_quantization_test_helper(cpu_offload=False, nvme_offload=False, bits=quantization_bits)
+        zero3_post_init_quantization_test_helper(cpu_offload=False, nvme_offload=False, bits=quantization_bits, tmp_path=tmp_path)
 
     @pytest.mark.skipif(device == 'cpu', reason='CPU does support FP16 GEMM')
-    def test_zero3_int4_post_init_quant_cpu_offload(self, quantization_bits):
+    def test_zero3_int4_post_init_quant_cpu_offload(self, quantization_bits, tmp_path):
         reset_random()
-        zero3_post_init_quantization_test_helper(cpu_offload=True, nvme_offload=False, bits=quantization_bits)
+        zero3_post_init_quantization_test_helper(cpu_offload=True, nvme_offload=False, bits=quantization_bits, tmp_path=tmp_path)
 
     @pytest.mark.skipif(device == 'cpu', reason='CPU does support FP16 GEMM')
-    def test_zero3_int4_post_init_quant_nvme_offload(self):
+    def test_zero3_int4_post_init_quant_nvme_offload(self, tmp_path):
         reset_random()
-        zero3_post_init_quantization_test_helper(cpu_offload=False, nvme_offload=True, bits=4)
+        zero3_post_init_quantization_test_helper(cpu_offload=False, nvme_offload=True, bits=4, tmp_path=tmp_path)
 
     @pytest.mark.skipif(device == 'cpu', reason='CPU does support FP16 GEMM')
-    def test_zero3_int4_quantized_initialization(self, quantization_bits):
+    def test_zero3_int4_quantized_initialization(self, quantization_bits, tmp_path):
         reset_random()
-        zero3_quantized_initialization_test_helper(cpu_offload=False, nvme_offload=False, bits=quantization_bits)
+        zero3_quantized_initialization_test_helper(cpu_offload=False, nvme_offload=False, bits=quantization_bits, tmp_path=tmp_path)
 
     @pytest.mark.skipif(device == 'cpu', reason='CPU does support FP16 GEMM')
-    def test_zero3_int4_quantized_initialization_cpu_offload(self, quantization_bits):
+    def test_zero3_int4_quantized_initialization_cpu_offload(self, quantization_bits, tmp_path):
         reset_random()
-        zero3_quantized_initialization_test_helper(cpu_offload=True, nvme_offload=False, bits=quantization_bits)
+        zero3_quantized_initialization_test_helper(cpu_offload=True, nvme_offload=False, bits=quantization_bits, tmp_path=tmp_path)
 
     @pytest.mark.skipif(device == 'cpu', reason='CPU does support FP16 GEMM')
-    def test_zero3_int4_quantized_initialization_nvme_offload(self):
+    def test_zero3_int4_quantized_initialization_nvme_offload(self, tmp_path):
         reset_random()
-        zero3_quantized_initialization_test_helper(cpu_offload=False, nvme_offload=True, bits=4)
+        zero3_quantized_initialization_test_helper(cpu_offload=False, nvme_offload=True, bits=4, tmp_path=tmp_path)
-- 
2.39.3

