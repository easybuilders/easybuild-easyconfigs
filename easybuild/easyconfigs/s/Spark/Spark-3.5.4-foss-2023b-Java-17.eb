# Author: Denis Krišťák (INUITS)

easyblock = 'Tarball'

name = 'Spark'
version = '3.5.4'
versionsuffix = '-Java-%(javaver)s'
homepage = 'https://spark.apache.org'
description = """Spark is Hadoop MapReduce done in memory"""

toolchain = {'name': 'foss', 'version': '2023b'}

source_urls = [
    'https://archive.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
    'https://downloads.apache.org/%(namelower)s/%(namelower)s-%(version)s/'
]
sources = ['%(namelower)s-%(version)s-bin-hadoop3.tgz']
checksums = ['d8e08877ed428bf9fcd44dbbec8cecadad34785a05513e5020ae74ffdabcbc83']

dependencies = [
    ('Python', '3.11.5'),
    ('Java', '17', '', SYSTEM),  # Requires Java 17: https://spark.apache.org/docs/3.5.4/#downloading
    ('Arrow', '16.1.0'),
]

exts_defaultclass = 'PythonPackage'
exts_default_options = {
    'source_urls': [PYPI_SOURCE],
}

exts_list = [
    ('py4j', '0.10.9.7', {  # pyspark 3.5.4 has requirement py4j==0.10.9.7
        'checksums': ['0b6e5315bb3ada5cf62ac651d107bb2ebc02def3dee9d9548e3baac644ea8dbb'],
    }),
]

sanity_check_paths = {
    'files': ['bin/pyspark', 'bin/spark-shell'],
    'dirs': ['python']
}

sanity_check_commands = [
    "pyspark -h",
    "python -c 'import pyspark'",
]

modextrapaths = {'PYTHONPATH': 'python'}

modextravars = {'SPARK_HOME': '%(installdir)s'}

moduleclass = 'devel'
